<!DOCTYPE html>


  <html class="light page-post">


<head>
  <meta charset="utf-8">
  
  <title>shuffle 详解 | sustcoder</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="spark,RDD," />
  

  <meta name="description" content="shuffle源码解析">
<meta name="keywords" content="spark,RDD,shuffle">
<meta property="og:type" content="article">
<meta property="og:title" content="shuffle 详解">
<meta property="og:url" content="https://sustcoder.github.io/2018/12/17/2018-12-17-shuffle-explain/index.html">
<meta property="og:site_name" content="sustcoder">
<meta property="og:description" content="shuffle源码解析">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/ShuffleMapTask.succeeded.png">
<meta property="og:image" content="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/RDD.iterator.png">
<meta property="og:image" content="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/Reduce.run_.png">
<meta property="og:image" content="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/RDD.saveAsTextFile.png">
<meta property="og:updated_time" content="2019-02-11T07:54:30.241Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="shuffle 详解">
<meta name="twitter:description" content="shuffle源码解析">
<meta name="twitter:image" content="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/ShuffleMapTask.succeeded.png">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cben" rel="stylesheet">


  

  

  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?f1cc8b0edce213e23b90ab65ff3c30ff";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

</head>

<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/blog/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>




<div class="content content-post CENTER">
   <article id="post-2018-12-17-shuffle-explain" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">shuffle 详解</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2018.12.17</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>freeli</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </span>



      
        <span>
          <i class="icon-comment"></i>
          <a href="https://sustcoder.github.io//2018/12/17/2018-12-17-shuffle-explain/#disqus_thread"></a>
        </span>
      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <p><strong>转载：时延军.<a href="http://shiyanjun.cn" target="_blank" rel="noopener">http://shiyanjun.cn</a></strong></p>
<p>Spark在Map阶段调度运行的ShuffleMapTask，最后会生成.data和.index文件，可以通过我的这篇文章 <a href="http://shiyanjun.cn/archives/1655.html" target="_blank" rel="noopener">Spark Shuffle过程分析：Map阶段处理流程</a> 了解具体流程和详情。同时，在Executor上运行一个ShuffleMapTask，返回了一个MapStatus对象，下面是ShuffleMapTask执行后返回结果的相关代码片段：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> writer: <span class="type">ShuffleWriter</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="literal">null</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> manager = <span class="type">SparkEnv</span>.get.shuffleManager</span><br><span class="line">  writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class="line">  writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])</span><br><span class="line">  writer.stop(success = <span class="literal">true</span>).get</span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (writer != <span class="literal">null</span>) &#123;</span><br><span class="line">        writer.stop(success = <span class="literal">false</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">        log.debug(<span class="string">"Could not stop writer"</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> e</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果ShuffleMapTask执行过程没有发生异常，则最后执行的调用为：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">writer.stop(success = <span class="literal">true</span>).get</span><br></pre></td></tr></table></figure>
<p>这里返回了一个MapStatus类型的对象，MapStatus的定义如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">sealed</span> <span class="class"><span class="keyword">trait</span> <span class="title">MapStatus</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">location</span></span>: <span class="type">BlockManagerId</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getSizeForBlock</span></span>(reduceId: <span class="type">Int</span>): <span class="type">Long</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中包含了运行ShuffleMapTask所在的BlockManager的地址，以及后续Reduce阶段每个ResultTask计算需要Map输出的大小（Size）。我们可以看下MapStatus如何创建的，在SortShuffleWriter的write()方法中，可以看到MapStatus的创建，如下代码所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br></pre></td></tr></table></figure>
<p>继续跟踪可以看到，调用了MapStatus的伴生对象的apply()方法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(loc: <span class="type">BlockManagerId</span>, uncompressedSizes: <span class="type">Array</span>[<span class="type">Long</span>]): <span class="type">MapStatus</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (uncompressedSizes.length &gt; <span class="number">2000</span>) &#123;</span><br><span class="line">    <span class="type">HighlyCompressedMapStatus</span>(loc, uncompressedSizes)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CompressedMapStatus</span>(loc, uncompressedSizes)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>uncompressedSizes表示Partition的个数，如果大于2000则创建HighlyCompressedMapStatus对象，否则创建CompressedMapStatus对象，他们具体的实现可以参考源码。</p>
<p><strong>含有Shuffle过程的Spark Application示例</strong></p>
<p>我们先给出一个简单的Spark Application程序代码，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">al rdd = sc.textFile(<span class="string">"/temp/*.h"</span>)</span><br><span class="line"><span class="keyword">val</span> finalRdd = rdd.flatMap(line =&gt; line.split(<span class="string">"\\s+"</span>)).map(w =&gt; (w, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">finalRdd.toDebugString</span><br><span class="line">finalRdd.saveAsTextFile(<span class="string">"/temp/output"</span>)</span><br></pre></td></tr></table></figure>
<p>通过RDD的toDebugString()方法，打印调试信息：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; finalRdd.toDebugString</span><br><span class="line">res0: <span class="type">String</span> = </span><br><span class="line">(<span class="number">133</span>) <span class="type">ShuffledRDD</span>[<span class="number">6</span>] at reduceByKey at &lt;console&gt;:<span class="number">30</span> []</span><br><span class="line">  +-(<span class="number">133</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">5</span>] at map at &lt;console&gt;:<span class="number">30</span> []</span><br><span class="line">      |   <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at flatMap at &lt;console&gt;:<span class="number">30</span> []</span><br><span class="line">      |   /temp<span class="comment">/*.h MapPartitionsRDD[3] at textFile at &lt;console&gt;:29 []</span></span><br><span class="line"><span class="comment">      |   /temp/*.h HadoopRDD[2] at textFile at &lt;console&gt;:29 []</span></span><br></pre></td></tr></table></figure>
<p>可以看到这个过程中，调用了reduceByKey()，创建了一个ShuffledRDD，这在计算过程中会执行Shuffle操作。</p>
<p><strong>ShuffleMapTask执行结果上报处理流程</strong></p>
<p>Spark Application提交以后，会生成ShuffleMapStage和/或ResultStage，而一个ShuffleMapStage对应一组实际需要运行的ShuffleMapTask，ResultStage对应一组实际需要运行ResultTask，每组Task都是有TaskSetManager来管理的，并且只有ShuffleMapStage对应的一组ShuffleMapTask都运行成功结束以后，才会调度ResultStage。<br>所以，我们这里关注的是，当ShuffleMapStage中最后一个ShuffleMapTask运行成功后，如何将Map阶段的信息上报给调度器（Driver上的TaskScheduler和DAGScheduler），了解这个处理流程对理解后续的Reduce阶段处理至关重要，这个过程的详细处理流程，如下图所示：<br><img src="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/ShuffleMapTask.succeeded.png" alt="ShuffleMapTask.succeeded"><br>我们将整个流程按照顺序分为如下几个过程来描述：</p>
<p><strong>ShuffleMapTask完成后处理结果</strong><br>Executor会启动一个TaskRunner线程来运行ShuffleMapTask，ShuffleMapTask完成后，会对结果进行序列化处理，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> directResult = <span class="keyword">new</span> <span class="type">DirectTaskResult</span>(valueBytes, accumUpdates)</span><br><span class="line"><span class="keyword">val</span> serializedDirectResult = ser.serialize(directResult)</span><br><span class="line"><span class="keyword">val</span> resultSize = serializedDirectResult.limit</span><br></pre></td></tr></table></figure>
<p>根据序列化后结果serializedDirectResult的大小resultSize，会进行一些优化，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> serializedResult: <span class="type">ByteBuffer</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (maxResultSize &gt; <span class="number">0</span> &amp;&amp; resultSize &gt; maxResultSize) &#123;</span><br><span class="line">    logWarning(<span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). Result is larger than maxResultSize "</span> +</span><br><span class="line">      <span class="string">s"(<span class="subst">$&#123;Utils.bytesToString(resultSize)&#125;</span> &gt; <span class="subst">$&#123;Utils.bytesToString(maxResultSize)&#125;</span>), "</span> +</span><br><span class="line">      <span class="string">s"dropping it."</span>)</span><br><span class="line">    ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](<span class="type">TaskResultBlockId</span>(taskId), resultSize))</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (resultSize &gt; maxDirectResultSize) &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">TaskResultBlockId</span>(taskId)</span><br><span class="line">    env.blockManager.putBytes(</span><br><span class="line">      blockId,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(serializedDirectResult.duplicate()),</span><br><span class="line">      <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">    logInfo(</span><br><span class="line">      <span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent via BlockManager)"</span>)</span><br><span class="line">    ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](blockId, resultSize))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    logInfo(<span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent to driver"</span>)</span><br><span class="line">    serializedDirectResult</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果结果大小没有超过指定的DirectTaskResult的最大限制值maxDirectResultSize，就直接将上面的DirectTaskResult的序列化结果发送给Driver；如果结果大小超过了Task结果的最大限制值maxResultSize，则直接丢弃结果；否则，当结果大小介于maxDirectResultSize与maxResultSize之间时，会基于Task ID创建一个TaskResultBlockId，然后通过BlockManager将结果暂时保存在Executor上（DiskStore或MemoryStore），以便后续计算直接请求获取该数据。<br>最后，结果会调用CoarseGrainedExecutorBackend的statusUpdate方法，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FINISHED</span>, serializedResult)</span><br></pre></td></tr></table></figure>
<p>将Task对应的运行状态、运行结果发送给Driver。</p>
<p><strong>Driver获取Task运行结果</strong><br>集群模式下，Driver端负责接收Task运行结果的是CoarseGrainedSchedulerBackend，它内部有一个DriverEndpoint来负责实际网络通信，以及接收Task状态及其结果，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</span><br><span class="line">  scheduler.statusUpdate(taskId, state, data.value)</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123;</span><br><span class="line">    executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</span><br><span class="line">        executorInfo.freeCores += scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line">        makeOffers(executorId)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// Ignoring the update since we don't know about the executor.</span></span><br><span class="line">        logWarning(<span class="string">s"Ignored task status update (<span class="subst">$taskId</span> state <span class="subst">$state</span>) "</span> +</span><br><span class="line">          <span class="string">s"from unknown executor with ID <span class="subst">$executorId</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>如果消息类型为StatusUpdate，则首先直接调用了TaskSchedulerImpl的statusUpdate()方法，来获取Task的运行状态及其结果，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</span><br><span class="line">  scheduler.statusUpdate(taskId, state, data.value)</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123;</span><br><span class="line">    executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</span><br><span class="line">        executorInfo.freeCores += scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line">        makeOffers(executorId)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// Ignoring the update since we don't know about the executor.</span></span><br><span class="line">        logWarning(<span class="string">s"Ignored task status update (<span class="subst">$taskId</span> state <span class="subst">$state</span>) "</span> +</span><br><span class="line">          <span class="string">s"from unknown executor with ID <span class="subst">$executorId</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>如果Task状态是TaskState.FINISHED，则通过TaskResultGetter来获取Task运行返回的结果，这里存在DirectTaskResult和IndirectTaskResult两种类型的结果，他们的处理方式不同：对于DirectTaskResult类型的结果，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> directResult: <span class="type">DirectTaskResult</span>[_] =&gt;</span><br><span class="line">  <span class="keyword">if</span> (!taskSetManager.canFetchMoreResults(serializedData.limit())) &#123;</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// deserialize "value" without holding any lock so that it won't block other threads.</span></span><br><span class="line">  directResult.value(taskResultSerializer.get())</span><br></pre></td></tr></table></figure>
<p>直接从DirectTaskResult中就可以通过反序列化得到结果，而对于IndirectTaskResult类型的结果，逻辑相对复杂一些，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> directResult: <span class="type">DirectTaskResult</span>[_] =&gt;</span><br><span class="line">  <span class="keyword">if</span> (!taskSetManager.canFetchMoreResults(serializedData.limit())) &#123;</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// deserialize "value" without holding any lock so that it won't block other threads.</span></span><br><span class="line">  directResult.value(taskResultSerializer.get())</span><br></pre></td></tr></table></figure>
<p>结果大小超过指定的限制值，在ShuffleMapTask运行过程中会直接通过BlockManager存储到Executor的内存/磁盘上，这里就会根据结果Block ID，通过BlockManager来获取到结果对应的Block数据。</p>
<p><strong>更新Driver端Task、Stage状态，并调度Stage运行</strong><br>获取到ShuffleMapTask运行的结果数据后，需要更新TaskSetManager中对应的状态信息，以便为后续调度Task运行提供决策支持，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scheduler.handleSuccessfulTask(taskSetManager, tid, result)</span><br></pre></td></tr></table></figure>
<p>上面代码调用了TaskSetManager的handleSuccessfulTask()方法，更新相关状态，同时继续更新DAGScheduler中对应的状态，代码片段如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sched.dagScheduler.taskEnded(tasks(index), <span class="type">Success</span>, result.value(), result.accumUpdates, info)</span><br><span class="line">maybeFinishTaskSet()</span><br></pre></td></tr></table></figure>
<p>调用DAGScheduler的taskEnded()方法，更新Stage信息。如果一个ShuffleMapTask运行完成后，而且是对应的ShuffleMapStage中最后一个ShuffleMapTask，则该ShuffleMapStage也完成了，则会注册该ShuffleMapStage运行得到的所有Map输出结果，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapOutputTracker.registerMapOutputs(</span><br><span class="line">  shuffleStage.shuffleDep.shuffleId,</span><br><span class="line">  shuffleStage.outputLocInMapOutputTrackerFormat(),</span><br><span class="line">  changeEpoch = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<p>上面MapOutputTracker维护了一个ConcurrentHashMap[Int, Array[MapStatus]]内存结构，用来管理每个ShuffleMapTask运行完成返回的结果数据，其中Key是Shuffle ID，Value使用数组记录每个Map ID对应的输出结果信息。<br>下面代码判断ShuffleMapStage是否可用，从而进行相应的处理：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (!shuffleStage.isAvailable) &#123;</span><br><span class="line">  <span class="comment">// Some tasks had failed; let's resubmit this shuffleStage.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Lower-level scheduler should also deal with this</span></span><br><span class="line">  logInfo(<span class="string">"Resubmitting "</span> + shuffleStage + <span class="string">" ("</span> + shuffleStage.name +</span><br><span class="line">    <span class="string">") because some of its tasks had failed: "</span> +</span><br><span class="line">    shuffleStage.findMissingPartitions().mkString(<span class="string">", "</span>))</span><br><span class="line">  submitStage(shuffleStage)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// Mark any map-stage jobs waiting on this stage as finished</span></span><br><span class="line">  <span class="keyword">if</span> (shuffleStage.mapStageJobs.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> stats = mapOutputTracker.getStatistics(shuffleStage.shuffleDep)</span><br><span class="line">    <span class="keyword">for</span> (job &lt;- shuffleStage.mapStageJobs) &#123;</span><br><span class="line">      markMapStageJobAsFinished(job, stats)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  submitWaitingChildStages(shuffleStage)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果ShuffleMapStage不可用，说明还有某些Partition对应的结果没有计算（或者某些计算失败），Spark会重新提交该ShuffleMapStage；如果可用，则说明当前ShuffleMapStage已经运行完成，更新对应的状态和结果信息：标记ShuffleMapStage已经完成，同时提交Stage依赖关系链中相邻下游的Stage运行。如果后面是ResultStage，则会提交该ResultStage运行。</p>
<p><strong>释放资源、重新调度Task运行</strong><br>一个ShuffleMapTask运行完成，要释放掉对应的Executor占用的资源，在Driver端会增加对应的资源列表，同时调度Task到该释放的Executor上运行，可见CoarseGrainedSchedulerBackend.DriverEndpoint中对应的处理逻，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123;</span><br><span class="line">  executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</span><br><span class="line">      executorInfo.freeCores += scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line">      makeOffers(executorId)</span><br></pre></td></tr></table></figure>
<p>上面makeOffers()方法，会调度一个Task到该executorId标识的Executor上运行。如果ShuffleMapStage已经完成，那么这里可能会调度ResultStage阶段的ResultTask运行。</p>
<p><strong>Reduce阶段处理流程</strong></p>
<p>上面我们给出的例子中，执行reduceByKey后，由于上游的RDD没有按照key执行分区操作，所以必定会创建一个ShuffledRDD，可以在PairRDDFunctions类的源码中看到combineByKeyWithClassTag方法，实现代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">  require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">  <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">    <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Default partitioner cannot partition array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    self.context.clean(createCombiner),</span><br><span class="line">    self.context.clean(mergeValue),</span><br><span class="line">    self.context.clean(mergeCombiners))</span><br><span class="line">  <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">    self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">    &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)</span><br><span class="line">      .setSerializer(serializer)</span><br><span class="line">      .setAggregator(aggregator)</span><br><span class="line">      .setMapSideCombine(mapSideCombine)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里，因为我们给出的例子的上下文中，self.partitioner == Some(partitioner)不成立，所以最终创建了一个ShuffledRDD对象。所以，对于Reduce阶段的处理流程，我们基于ShuffledRDD的处理过程来进行分析。<br>我们从ResultTask类开始，该类中实现了runTask()方法，代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">U</span> = &#123;</span><br><span class="line">  <span class="comment">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class="line">  <span class="keyword">val</span> deserializeStartTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">  <span class="keyword">val</span> ser = <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class="line">  <span class="keyword">val</span> (rdd, func) = ser.deserialize[(<span class="type">RDD</span>[<span class="type">T</span>], (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>)](</span><br><span class="line">    <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">  _executorDeserializeTime = <span class="type">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class="line"> </span><br><span class="line">  func(context, rdd.iterator(partition, context))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，最核心的就是上面的rdd.iterator()调用，具体处理过程，如下图所示：<br><img src="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/RDD.iterator.png" alt="RDD.iterator"><br>最终，它用来计算一个RDD，即对应ShuffledRDD的计算。iterator()方法是在RDD类中给出的，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">    getOrCompute(split, context)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    computeOrReadCheckpoint(split, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>跟踪getOrCompute()方法，最终应该是在ShuffledRDD类的compute()方法中定义。</p>
<p><strong>ShuffledRDD计算</strong><br>ShuffledRDD对应的compute方法的实现代码，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]</span><br><span class="line">  <span class="type">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context)</span><br><span class="line">    .read()</span><br><span class="line">    .asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面主要是通过BlockStoreShuffleReader的read()方法，来实现ShuffledRDD的计算，我们通过下面的序列图来看一下详细的执行流程：<br><img src="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/Reduce.run_.png" alt="Reduce.run"><br>跟踪Map的输出结果，是基于Executor端的MapOutputTracker与Driver端的MapOutputTrackerMaster来实现的，其中MapOutputTrackerMaster作为Server端，MapOutputTracker作为Client端。Driver端管理了一个Spark Application计算程序的ShuffleMapStage中所有ShuffleMapTask的输出，所以在Reduce过程中Executor会通过MapOutputTracker与Driver的MapOutputTrackerMaster进行通信获取。<br>调用BlockStoreShuffleReader的read()方法，最终得到了Reduce过程中需要的输入，即ShuffleMapTask的输出结果所在的位置。通常，为了能够使计算在数据本地进行，每个ResultTask运行所在的Executor节点会存在对应的Map输出，是通过BlockManager来管理这些数据的，通过Block ID来标识。所以，上图中最后返回了一个BlockManager ID及受其管理的一个Block ID列表，然后Executor上的ResultTask就能够根据BlockManager ID来获取到对应的Map输出数据，从而进行数据的计算。<br>ResultTask运行完成后，最终返回一个记录的迭代器，此时计算得到的最终结果数据，是在各个ResultTask运行所在的Executor上的，而数据又是按Block来存储的，是通过BlockManager来管理的。</p>
<p><strong>保存结果RDD</strong><br>根据前面的程序示例，最后调用了RDD的saveAsTextFile()，这会又生成一个ResultStage，进而对应着一组ResultTask。保存结果RDD的处理流程，如下图所示：<br><img src="http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/RDD.saveAsTextFile.png" alt="RDD.saveAsTextFile"><br>上面整个流程，会执行设置RDD输出到HDFS的Writer（一个写文件的函数）、提交ResultStage、构建包含ResultTask的TaskSet、调度ResultTask到指定Executor上执行这几个核心的过程。实际上，在每个Executor上运行的ResultTask的核心处理逻辑，主要是下面这段函数代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> writer = <span class="keyword">new</span> <span class="type">SparkHadoopWriter</span>(hadoopConf)</span><br><span class="line">writer.preSetup()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> writeToFile = (context: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> taskAttemptId = (context.taskAttemptId % <span class="type">Int</span>.<span class="type">MaxValue</span>).toInt</span><br><span class="line">  <span class="keyword">val</span> (outputMetrics, callback) = <span class="type">SparkHadoopWriterUtils</span>.initHadoopOutputMetrics(context)</span><br><span class="line"> </span><br><span class="line">  writer.setup(context.stageId, context.partitionId, taskAttemptId)</span><br><span class="line">  writer.open()</span><br><span class="line">  <span class="keyword">var</span> recordsWritten = <span class="number">0</span>L</span><br><span class="line"> </span><br><span class="line">  <span class="type">Utils</span>.tryWithSafeFinallyAndFailureCallbacks &#123;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> record = iter.next()</span><br><span class="line">      writer.write(record._1.asInstanceOf[<span class="type">AnyRef</span>], record._2.asInstanceOf[<span class="type">AnyRef</span>])</span><br><span class="line"> </span><br><span class="line">      <span class="comment">// Update bytes written metric every few records</span></span><br><span class="line">      <span class="type">SparkHadoopWriterUtils</span>.maybeUpdateOutputMetrics(outputMetrics, callback, recordsWritten)</span><br><span class="line">      recordsWritten += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;(finallyBlock = writer.close())</span><br><span class="line">  writer.commit()</span><br><span class="line">  outputMetrics.setBytesWritten(callback())</span><br><span class="line">  outputMetrics.setRecordsWritten(recordsWritten)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>还记得我们在计算ShuffledRDD的过程中，最终的ResultTask生成了一个结果的迭代器。当调用saveAsTextFile()时，ResultStage对应的一组ResultTask会在Executor上运行，将每个迭代器对应的结果数据保存到HDFS上。</p>
<p>参考：</p>
<p><a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md" target="_blank" rel="noopener">https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md</a></p>
<p><a href="https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-study.md" target="_blank" rel="noopener">https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-study.md</a></p>
<p><a href="https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-hash-sort.md" target="_blank" rel="noopener">https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-hash-sort.md</a></p>
<p><a href="https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html" target="_blank" rel="noopener">https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html</a></p>
<p><a href="http://sharkdtu.com/posts/spark-shuffle.html" target="_blank" rel="noopener">http://sharkdtu.com/posts/spark-shuffle.html</a></p>

    
  </div>

</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持sustcoder</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2018/12/12/2018-12-12-sparkCore-sourceCodeAnalysis_reservoirSampleAndCount/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2018/12/20/2018-12-20-spark-sourceCode-run/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>





   
      <div class="git"></div>
   
</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/blog/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    
  <section class="disqus-comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>
  </section>

  <script>
    var disqus_shortname = 'forsigner';
    
    var disqus_url = 'https://sustcoder.github.io/2018/12/17/2018-12-17-shuffle-explain/';
    
    (function(){
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>

  <script id="dsq-count-scr" src="//forsigner.disqus.com/count.js" async></script>



    

    
    

    

    
    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
