---
title: sparK概念概览
subtitle: 
description: 
keywords: [spark,概念]
date: 2019-03-07
tags: [spark,概念]
category: [spark]
---

#####  什么是RDD

从三方面阐述什么事RDD，即RDD的背景，定义和特点。

rdd的产生是为了提高计算速度，提高比MapReduce更加灵活的操作，以及更好的容错方式。为了实现这三个目标，RDD设置成了一个弹性分布式数据集，但是其并不记录数据，而是记录了数据的转换过程，并且将整个转换过程序列化后，在多个机器上对RDD的不同partition进行并行计算。这种方式相比传统的记录数据检查点或者更新情况而言，在集群中有更高效的数据传输。通过缓存机制可以减少转换操作的计算次数，通过血缘关系来解决数据丢失问题。因此RDD具备了：只读，分片，分区，并行计算，持久化和容错的特点。

RDD还具备惰性求值，即在Action操作触发后才开始真正的执行转换操作，从惰性求值的角度看RDD，RDD就是一组spark计算的指令列表。

##### 缓存策略

优先使用内存，以减少磁盘IO带来的性能损耗，一般情况下选择的顺序是：

- 默认选择`MEMORY_ONLY `
- 如果内存不足，选择`MEMORY_ONLY_SER`
- 如果需要做容错,选择`MEMORY_ONLY_SER_2`
- 如果中间计算RDD的代价比较大时，选择`MEMORY_AND_DISK `

在缓存操作中有两种方法，cache和persist,cache调用了persist(memory_only),persist可以自定义缓存级别，还有另一种持久化的方式checkpoint机制，checkpoint机制和cache不同的是，checkpoint会将数据持久化到hdfs中去，并且**切掉与父RDD的依赖关系**

同时在缓存确定使用完毕后，可通过unpersist操作来释放掉内存的占用，如果未手动释放，spark会通过线程定时去检测内存情况，并通过LRU策略进行淘汰。

##### 水塘抽样

概念：在不知道总数N的值的情况下，从N个数中随机取出一个数，且这个操作的概率是1/N;类比，从N个数中取出M个数，且每个数被选中的概率是M/N。

实现：先取出前M个数，然后从M后面的数K开始遍历，生成一个在区间[0,k]中的随机数j，如果j小于M，则用K替换j位置上的数,[详情](https://my.oschina.net/freelili/blog/2987667)

```
从S中抽取首k项放入「水塘」中
对于每一个S[j]项（j ≥ k）：
   随机产生一个范围0到j的整数r
   若 r < k 则把水塘中的第r项换成S[j]项
```

##### RangePartitioner

计算总体的数据抽样大小sampleSize，计算规则是：至少每个分区抽取20个数据或者最多1M的数据量。

1. 根据sampleSize和分区数量计算每个分区的数据抽样样本数量最大值sampleSizePrePartition
2. 根据以上两个值进行水塘抽样，返回RDD的总数据量，分区ID和每个分区的采样数据。
3. 计算出数据量较大的分区通过RDD.sample进行重新抽样。
4. 通过抽样数组 candidates: ArrayBuffer[(K, wiegth)]
5. 计算出分区边界的数组BoundsArray。计算出步长：权重总数相当于预计数据总量，除以分区数就是每个分区的数量，得到的值即是按区间分割的区间步长
6. 在取数据时，如果分区数小于128则直接获取，如果大于128则通过二分法，获取当前Key属于那个区间，返回对应的BoundsArray下标即为partitionsID，[详情](https://my.oschina.net/freelili/blog/2987568)

##### 统一内存管理模型

在1.5之前使用的是静态内存管理模型，内存的分配是固定的。[详情](https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html)

默认内存分配策略：预留300M内存，剩余的百分之60内存计算和存储各占一半，默认堆外内存不进行使用

- 预留内存reservedMemory=300M
- 管理内存maxHeapMemory = (systemMemory - reservedMemory) * 0.6
- storageMemory=excutionMemory=maxHeapMemory*0.5
- 非堆内存默认值0，可通过spark.memory.offHeap.size参数调整，其中storage和excution的内存占比也均为50%
- 未被管理内存：剩余的40%用来存储用户自定义的数据结构或者spark内部元数据，如rdd的partition信息等

计算和存储内存分配区别：存储内存在向计算内存借用时，如果借用失败则放弃借用，计算内存在向存储内存借用时如果借用不到则继续等待，直到借用成功为止

释放内存的区别：存储内存在释放时，如果要求释放内存大于已使用内存则将已使用内存直接清零，在计算内存释放时如果需要释放内存大于task内存，则释放掉task的所有内存，当task执行完毕后会通知回收内存。

内存淘汰策略：

- 如果存储级别有落盘，则会将内存中淘汰的数据写入磁盘
- 如果没有罗盘，则直接删除
- 淘汰规则为LinkedHahsMap的最近最少使用策略

Spark与MapReduce区别

- 设计上都分为Map和reduce两个阶段
- MR在map阶段会排序，spark默认在Map阶段默认使用hash进行聚合，但不排序
- 实现方式上mapreduce将整个处理流程细化为map，shufffle，sort，reduce等阶段，需要用户去实现每个节点，但是spark是由一系列算子组成的。

##### 什么是Job

站在不同的角度看job
1. transaction: Job是由一组RDD上转换和动作组成。
2. stage: Job是由ResultStage和多个ShuffleMapState组成
3. init:由action操作触发提交执行的一个函数

##### SparkStreaming

作为Spark平台的流式实现，Spark Streaming 是有单独一套抽象和API.



![11](E:\data\oss\spark\srccode\1.webp)

##### StructStreaming

spark1.0与spark2.0 中RDD

- Spark 1.x 的 RDD 更多意义上是一个一维、只有行概念的数据集，比如 RDD[Person]，那么一行就是一个 Person，存在内存里也是把 Person 作为一个整体（序列化前的 java object，或序列化后的 bytes）。
- Spark 2.x 里，一个 Person 的 Dataset 或 DataFrame，是二维行+列的数据集，比如一行一个 Person，有 name:String, age:Int, height:Double 三列；在内存里的物理结构，也会显式区分列边界。
    Dataset/DataFrame 在 API 使用上有区别：Dataset 相比 DataFrame 而言是 type-safe 的，能够在编译时对 AnalysisExecption 报错

![11](E:\data\oss\spark\srccode\110.png)

一次执行的过程如上图；这里有 6 个关键步骤：

1. 从source获取offset，并将offset写入offsetlog中
2. 根据offset调用source.getBatch(offset)方法获取数据并和DAG构成逻辑执行计划
3. 对逻辑执行计划进行优化
4. 调用sink.add保存数据，并记录本次提交的ID到batchCommitLog。

[详细步骤](https://github.com/lw-lin/CoolplaySpark/blob/master/Structured%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/1.1%20Structured%20Streaming%20%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF%E4%B8%8E%E5%AE%9E%E7%8E%B0%E6%A6%82%E8%BF%B0.md)：

1. StreamExecution 通过 Source.getOffset() 获取最新的 offsets，即最新的数据进度；
2. StreamExecution 将 offsets 等写入到 `offsetLog` 里
   - 这里的 offsetLog 是一个持久化的 WAL (Write-Ahead-Log)，是将来可用作故障恢复用
3. StreamExecution 构造本次执行的 LogicalPlan
   - (3a) 将预先定义好的逻辑（即 StreamExecution 里的 logicalPlan 成员变量）制作一个副本出来
   - (3b) 给定刚刚取到的 offsets，通过 `Source.getBatch(offsets) `获取本执行新收到的数据的 Dataset/DataFrame 表示，并替换到 (3a) 中的副本里
   - 经过 (3a), (3b) 两步，构造完成的 `LogicalPlan` 就是针对本执行新收到的数据的 Dataset/DataFrame 变换（即整个处理逻辑）了
4. 触发对本次执行的 `LogicalPlan 的优化`，得到 IncrementalExecution
   - 逻辑计划的优化：通过 Catalyst 优化器完成
   - 物理计划的生成与选择：结果是可以直接用于执行的 RDD DAG
   - 逻辑计划、优化的逻辑计划、物理计划、及最后结果 RDD DAG，合并起来就是 IncrementalExecution
5. 将表示计算结果的 Dataset/DataFrame (包含 IncrementalExecution) 交给 Sink，即调用 `Sink.add(ds/df)`
6. 计算完成后的 commit
   - (6a) 通过 `Source.commit() `告知 Source 数据已经完整处理结束；Source 可按需完成数据的 garbage-collection
   - (6b) 将本次执行的批次 id 写入到 `batchCommitLog` 里

##### 增量查询

Structured Streaming 在编程模型上暴露给用户的是，**每次持续查询看做面对全量数据**（而不仅仅是本次执行信收到的数据），所以每次执行的结果是针对全量数据进行计算的结果。

但是在实际执行过程中，由于全量数据会越攒越多，那么每次对全量数据进行计算的代价和消耗会越来越大。

Structured Streaming 的做法是：

- 引入全局范围、高可用的 **StateStore**
- 转全量为增量，即在每次执行时：
  - **先从 StateStore 里 restore 出上次执行后的状态**
  - **然后加入本执行的新数据，再进行计算**
  - **如果有状态改变，将把改变的状态重新 save 到 StateStore 里**
- 为了在 Dataset/DataFrame 框架里完成对 StateStore 的 restore 和 save 操作，引入两个新的物理计划节点 —— StateStoreRestoreExec 和 StateStoreSaveExec

所以 Structured Streaming 在编程模型上暴露给用户的是，每次持续查询看做面对全量数据，但在具体实现上转换为增量的持续查询。

 StateStore 在写出状态的更新时，是写出的修改流水 log。

StateStore 本身也带了 maintainess 即维护模块，会周期性的在后台将过去的状态和最近若干版本的流水 log 进行合并，并把合并后的结果重新写回到 HDFS：`old_snapshot + delta_a + delta_b + … => lastest_snapshot`。

这个过程跟 HBase 的 major/minor compact 差不多，但还没有区别到 major/minor 的粒度

##### [输出类型](https://github.com/lw-lin/CoolplaySpark/blob/master/Structured%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/4.1%20Structured%20Streaming%20%E4%B9%8B%20Event%20Time%20%E8%A7%A3%E6%9E%90.md)

- **Complete mode**: Complete 的输出是和 State 是完全一致的
- **Append mode (default)**: Append 的语义将保证，一旦输出了某条 key，未来就不会再输出同一个 key，适用于filter类型和历史数据无关的算子，也可以通过watermark在确认数据不会再写入后再输出。
- **Update mode**: 只有本执行批次 State 中被更新了的条目会被输出，相当于 Append mode 的加强版。

##### Struct & Streaming

1. 1.0中sparkStreaming有单独的一套API，本质上是编写DAG，不同人编写出来的执行效率差距较大，到了2.0StructStreaming将流看作了一张无限大的表，复用了sparkSql的执行引擎，在执行计划内存管理等方面做了优化。
2. 1.0不支持端到端的exactly-once。Structured Streaming 非常显式地提出了输入(Source)、执行(StreamExecution)、输出(Sink)的 3 个组件，并且在每个组件显式地做到 fault-tolerant，由此得到整个 streaming 程序的 end-to-end exactly-once guarantees.