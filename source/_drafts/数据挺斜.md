# 数据挺斜

## 数据倾斜现象

你的大部分的task，都执行的特别特别快，刷刷刷，就执行完了（你要用client模式，standalone client，yarn client，本地机器主要一执行spark-submit脚本，就会开始打印log），task175 finished；剩下几个task，执行的特别特别慢，前面的task，一般1s可以执行完5个；最后发现1000个task，998，999 task，要执行1个小时，2个小时才能执行完一个task。

运行的时候，其他task都刷刷刷执行完了，也没什么特别的问题；但是有的task，就是会突然间，啪，报了一个OOM，JVM Out Of Memory，内存溢出了，task failed，task lost，resubmitting task。反复执行几次都到了某个task就是跑不通，最后就挂掉。

某个task就直接OOM，那么基本上也是因为数据倾斜了，task分配的数量实在是太大了！！！所以内存放不下，然后你的task每处理一条数据，还要创建大量的对象。内存爆掉了。

## 数据挺斜原因

出现数据倾斜的原因，基本只可能是因为发生了shuffle操作，在shuffle的过程中，出现了数据倾斜的问题。因为某个，或者某些key对应的数据，远远的高于其他的key

## 数据挺斜定位

- 看Log
- 看stage划分
- 找shuffle操作

## 数据挺斜解决方案

### 聚合源数据

1. 在hive等数据源处进行聚合操作，减少在spark中使用groupByke+map的这种操作。
2. 改变聚合粒度，按照key直接聚合数据量过大，可在key 上加时间标签等，分散每个key对应value的大小。

### 过滤挺斜Key

1. 如果是个别key特别大的话，可以在处理前将其过滤掉，如果不影响最终结果可以直接干掉，如果影响可对其进行单独处理。

### 提高reduce task并行度

1. 增加reduce task数量，就可以让每个reduce task分配到更少的数据量，就可以缓解每个task的压力。默认并行度大小可通过`spark.default.parallelism`配置，对于具体的算子,可通过传入paralleism参数修改并行度,例如：`groupByKey(parallesimSize)`,`countByKey(parallesimSize)`,`reduceByKey(parallesimSize)`

2. 缺陷：治标不治本的意思，因为它没有从根本上改变数据倾斜的本质和问题。

### 随机key实现双重聚合

1. 第一轮聚合的时候，对key进行打散，将原先一样的key，变成不一样的key，相当于是将每个key分为多组；
   先针对多个组，进行key的局部聚合；接着，再去除掉每个key的前缀，然后对所有的key，进行全局的聚合。

### 将reduce jion转换为map jion

普通jion操作，如果有key交叉，就会走shuffle，如果两个RDD的大小差异较大，可以将较小的RDD广播出去，另一个RDD做map操作

### sample采样

通过sample函数对RDD进行随机采样，找出样本中占比最多的Key,将其看作原RDD中导致数据挺斜的KEY，将原始RDD分成没有导致挺斜的RDD1和有导致挺斜的RDD2，分别去jion RDD3,最后再将Jion的结果union。但是此方法不适用于有特别多的Key发生挺斜的情况。

### 随机数和扩容表

主要解决sample采样解决不了的，有大量导致挺斜的Key的情况，

将其中一个RDD进行扩容，对另一个RDD进行普通映射，最后再进行jion操作，其目的就是为了将一个RDD拆分成多个RDD进行处理。

局限性： 因为两个RDD都比较大，所以不能对RDD进行无限放大。







