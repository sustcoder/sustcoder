##### spark从kafka读数据并发问题

默认情况下一个kafka分区对应一个RDD分区，但是有些场景下存在部分kafka分区处理较慢的场景，首先应该考虑是否是数据倾斜导致，以及增加kafka分区数量是否可以解决，如果解决不了可通过以下思路解决：

- 重写kafkaRDD的getPartition方法，让一个spark分区对应多个kafka分区，缺点是破坏了一个kafka分区内数据有序性问题。
- 在处理数据前重新分区，通过repartition或者coalease方法对数据重新分区。缺点是需要额外的时间和IO的浪费
- 在rdd.mapPartitions方法里面创建多个线程来处理同一个RDD分区里面的数据

[查看详情](https://www.iteblog.com/archives/2419.html)

##### 将RDBMS的数据实时传送到hive中

将binglog通过kafka发送到broker上，通过flume代理作为消费者消费数据并写入fluem

![kafka9](E:\data\oss\kafka\kafka9.png)

[查看详情](https://www.iteblog.com/archives/1771.html)

##### spark streaming kafka实现数据零丢失的几种方式

1. kafka高级API

使用Receiver接收数据，并开启WAL，将存储级别修改为`StorageLevel.MEMORY_AND_DISK_SER`。开启WAL后能够保证receiver在正常工作的情况下能够把接收到的消息存入WAL，但是如果receiver被强制中断的话依旧存在问题，因此可在streaming程序最后添加代码，确定wal执行完毕后再终止程序。通过receiver的方式只能实现at least once。

2. kafka Direct API

kafka direct api的方式是指自己维护offsets进行checkpoint操作，在receiver重启时重新从checkpoint开始消费，但是会存在checkpoint里保存的offset已经处理过的情况，需要手动去重新制定checkpoint。也可以存入zookeeper中。

```scala
messages.foreachRDD(rdd=>{
   val message = rdd.map(_._2)  
   //对数据进行一些操作
   message.map(method)
   //更新zk上的offset (自己实现)
   updateZKOffsets(rdd)
})
```



1



.请说说hadoop1的HA如何实现？

2.列举出hadoop中定义的最常用的InputFormats。那个是默认的？

3.TextInputFormat和KeyValueInputFormat类之间的不同之处在于哪里？

4.hadoop中的InputSplit是什么？

5.hadoop框架中文件拆分是如何被触发的？

6.hadoop中的RecordReader的目的是什么？

7.如果hadoop中没有定义定制分区，那么如何在输出到reducer前执行数据分区？

8.什么是jobtracker？jobtracker有哪些特别的函数？

9.hadoop中job和task之间是什么关系？

10.假设hadoop一个job产生了100个task，其中一个task失败了，hadoop会如何处理？

11.hadoop推测执行是如何实现的？

12.关系型数据库有什么弱点？
很难进行分布式部署，I/O瓶颈显著，依赖于强大的服务器，需要花更大的代价才能突破性能极限
很难处理非结构化数据

13.什么情况下使用hbase？
适合海量的，但同时也是简单的操作（例如：key-value）
成熟的数据分析主题，查询模式已经确定并且不会轻易改变。
传统的关系型数据库已经无法承受负荷，高速插入，大量读取

1.hbase怎么预分区？

2.hbase怎么给web前台提供接口来访问？

3.htable API有没有线程安全问题，在程序中是单例还是多例？

4.hbase有没有并发问题？

5.metaq消息队列，zookeeper集群，storm集群，就可以完成对商城推荐系统功能吗？还有没有其他的中间件？

6.storm 怎么完成对单词的计数？

7.hdfs的client端，复制到第三个副本时宕机， hdfs怎么恢复保证下次写第三副本?

8.block块信息是先写dataNode还是先写nameNode?

9.[Hadoop](https://www.iteblog.com/archives/tag/hadoop/)生态圈中各种框架的运用场景？

10.hive中的压缩格式RCFile、TextFile、SequenceFile各有什么区别？

11.说明 [Hadoop](https://www.iteblog.com/archives/tag/hadoop/) 2.0 的基本构成。

12.相比于 HDFS1.0, HDFS 2.0最主要的改进在哪几方面？

13.试使用"步骤 1，步骤 2，步骤 3…" 说明YARN中运行应用程序的基本流程。

14.MapReduce2.0中，MRAppMaster主要作用是什么？MRAppMaster如何实现任务容错的？

15.为什么会产生yarn, 它解决了什么问题，有什么优势？

\1. 集群多少台, 数据量多大, 吞吐量是多大, 每天处理多少G的数据？

\2. 我们的日志是不是除了apache的访问日志是不是还有其他的日志？

\3. 假设我们有其他的日志是不是可以对这个日志有其他的业务分析？这些业务分析都有什么？

\4. 你们的服务器有多少台？服务器的内存多大？

\5. 你们的服务器怎么分布的？（这里说地理位置分布，最好也从机架方面也谈谈）

\6. 你平常在公司都干些什么（一些建议）

\7. 你们的集群规模？

537台。

\8. 你们的数据是用什么导入到数据库的？导入到什么数据库？
处理之前的导入：通过 hadoop 命令导入到 hdfs 文件系统
处理完成之后的导出：利用 hive 处理完成之后的数据，通过 sqoop 导出到 mysql 数据库中，以供报表层使用。

\9. 你们业务数据量多大？有多少行数据？
开发时使用的是部分数据，不是全量数据，有将近一亿行（ 8、 9 千万，具体不详，一般开发中也没人会特别关心这个问题）

\10. 你们处理数据是直接读数据库的数据还是读文本数据？
将日志数据导入到 hdfs 之后进行处理

\11. 你们写 hive 的 hql 语句，大概有多少条？
不清楚，我自己写的时候也没有做过统计

\12. 你们提交的 job 任务大概有多少个？这些 job 执行完大概用多少时间？
没统计过，加上测试的，会有很多

\13. 你在项目中主要的工作任务是？
利用 hive 分析数据

\14. 你在项目中遇到了哪些难题，是怎么解决的？
某些任务执行时间过长，且失败率过高，检查日志后发现没有执行完就失败，原因出在hadoop 的 job 的 timeout 过短（相对于集群的能力来说），设置长一点即可

\15. 你自己写过 udf 函数么？写了哪些？

\16. 你的项目提交到 job 的时候数据量有多大？

\17. 数据备份, 你们是多少份, 如果数据超过存储容量, 你们怎么处理？

\18. 怎么提升多个 JOB 同时执行带来的压力, 如何优化, 说说思路？

\19. 你们用 HBASE 存储什么数据？

\20. 你们的 hive 处理数据能达到的指标是多少？

\21. 你们的 hbase 大概在公司业务中（主要是网上商城）大概都几个表，几个表簇，都存什么样的数据？

https://www.iteblog.com/archives/1758.html

## 二. 分析题

1.有一千万条短信，有重复，以文本文件的形式保存，一行一条，有重复。请用5分钟时间，找出重复出现最多的前10条。
分析：
常规方法是先排序，在遍历一次，找出重复最多的前10条。但是排序的算法复杂度最低为nlgn。可以设计一个 hash_table, hash_map ，依次读取一千万条短信，加载到hash_table表中，并且统计重复的次数，与此同时维护一张最多10条的短信表。这样遍历一次就能找出最多的前10条，算法复杂度为 O(n)。



24.时间窗口函数如何做到这个时间的窗口值依赖上个时间的窗口值
25.flume配置需要关注哪些点?
29.jvm有哪几种配置模式,假如JVM配置了G1模式,会有年轻代吗

hbase的基本使用，rowkey设计，region设计，数据读写的过程。

32.在资源有限的情况下,一万维的矩阵乘以一万维的矩阵应该怎么处理?

1.网易大数据面试题

- 说说项目
- Spark哪部分用得好，如何调优
- Java哪部分了解比较好
- 聊聊并发，并发实现方法，volatile关键字说说
- HashMap的底层原理
- 为什么要重写hashcode和equals
- 说说jvm
- 各个垃圾收集器运用在什么情形
- jvm调优
- 说说io
- 为什么考虑转行呢？是因为原专业不好就业吗？

2.蚂蚁金服面试题

- 小文件的合并
- MR与Spark的区别
- 关注哪些名人的博客
- 对大数据领域有什么自己的见解
- 平常怎么学习大数据的
- StringBuilder与StringBuffer的区别
- HashMap与Hashtable的区别
- 谈谈你对树的理解
- 数据库索引的实现
- jvm的内存模型
- jvm的垃圾收集器
- jvm的垃圾收集算法
- HDFS架构
- HDFS读写流程
- Hadoop3.0做了哪些改进
- 谈谈YARN
- 为什么项目选择使用Spark，你觉得Spark的优点在哪里
- 了解Flink与Storm嘛，他们与Spark Streaming的区别在哪里
- 1TB文件，取重复的词，top5指定的资源的场景下，如何快速统计出来

 

3.美图面试题

- 为什么选择美图，你知道美图地点在哪里嘛
- 介绍下你做的项目吧
- 数据统一管理平台，我挺感兴趣的，你说说吧
- 我大概知道是怎么回事了，java web这块你参与开发了吗
- 你刚刚项目提到了元数据，你能说说hive的元数据管理嘛，对它了解嘛
- 还是hive，你对hive有哪些原理性了解呢
- 知道AST、operator tree这些长什么样吗
- 那你的hive转mr过程是怎么了解的呢？
- 除了谓词下推，还能说说其它的优化嘛？别说数据倾斜的调优
- jvm了解不，说下垃圾收集算法
- 平常用java和scala语言哪个多点
- 如果我现在要使用map集合，你觉得哪种适合多线程情况下进行访问
- 如何去监控线程
- Spark 出现OOM，你觉得该怎么进行调优呢？不去动jvm的参数
- 你觉得join该怎么优化
- 你对未来的规划是什么？(五年内)
- 你也就是走技术路线咯

4.美图二面

- ThriftServer的HA如何去实现，能说下实现的思路嘛
- 说下Zookeeper的watch机制是如何实现的嘛？
- 场景题：

​     现在有1个client，2个server，当我动态加入一台机器，或者删除一台机器，或者某台机器宕机了，client该如何去感知到，说下实现思路(不使用Zookeeper),如何通信，说说具体实现?

 

5.七牛云面试题

- 快排
- hive和hdfs之间的联系
- inode和文件描述符
- linux指令如何创建文件
- http中header中放入key value 有什么变化
- 系统调用和库函数区别
- http缓冲实现机智
- session cookie  区别
- 进程间通信方式
- jsp本质
- http请求状状态
- get post put remove
- 数据库join 
- 数据库引擎
- hibernate和mybiters区别
- jvm垃圾回收
- hive和关系型数据库区别
- hive实现原理
- spark与mr的区别

数据倾斜的产生和解决办法？

spark 性能优化有哪些？

spark 实际工作中，是怎么来根据任务量，判定需要多少资源的？

Spark 提交的 job 的工作流程

Spark 的 Shuffle 原理及调优

updateStateByKey解析

海量日志数据 top1 问题

有限内存的 topN 统计

搜索引擎热点词汇 topN

多文件排序

> **答案**: [**博客1**](https://blog.csdn.net/fenglei0415/article/details/86309207) [**博客2**](https://blog.csdn.net/fenglei0415/article/details/86313135)



