[{"title":"sparkStreaming源码解析之数据的产生与导入","url":"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_DataInputOutput/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_dataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis__faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps8B7D.tmp.jpg\" alt=\"img\"></p>\n<p>数据的产生与导入主要分为以下五个部分</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps231.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"1-分发receivers\"><a href=\"#1-分发receivers\" class=\"headerlink\" title=\"1. 分发receivers\"></a>1. 分发receivers</h1><p>由 Receiver 的总指挥 ReceiverTracker 分发多个 job（每个 job 有 1 个 task），到多个 executor 上分别启动 ReceiverSupervisor 实例</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps232.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>从ReceiverInputDStreams中获取Receivers，并把他们发送到所有的worker nodes:</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> endpoint:<span class=\"type\">RpcEndpointRef</span>=</span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">**launchReceivers**</span></span>()&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// DStreamGraph的属性inputStreams</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> receivers=inputStreams.map&#123;nis=&gt;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">val</span> rcvr=nis.getReceiver()</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// rcvr是对kafka,socket等接受数据的定义</span></span><br><span class=\"line\">\t\t\trcvr</span><br><span class=\"line\">\t\t&#125;\t</span><br><span class=\"line\">\t\t<span class=\"comment\">// 发送到worker</span></span><br><span class=\"line\">\t\t endpoint.send(<span class=\"type\">StartAllReceivers</span>(receivers))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-1-选择receivers位置\"><a href=\"#1-1-选择receivers位置\" class=\"headerlink\" title=\"1.1. 选择receivers位置\"></a>1.1. <strong>选择receivers位置</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps234.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>目的地选择分两种情况：初始化选择和失败重启选择</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 分发目的地的计算</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> schedulingPolicy=</span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSchedulingPolicy</span>() </span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 首次启动</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span>(receivers) =&gt;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-1-首次启动\"><a href=\"#1-1-1-首次启动\" class=\"headerlink\" title=\"1.1.1. 首次启动\"></a>1.1.1. <strong>首次启动</strong></h3><p><strong>1. 选择最优executors位置</strong></p>\n<p><strong>2. 遍历构造最终分发的excutor</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> schedulingPolicy=</span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSchedulingPolicy</span>() </span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 首次启动</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span>(receivers) =&gt;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 1. 选择最优executors位置</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">val</span> locations=</span><br><span class=\"line\">\t\t\t\tschedulingPolicy.scheduleReceivers(</span><br><span class=\"line\">\t\t\t\t\treceivers,getExecutors</span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 2. 遍历构造最终分发的excutor</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">for</span>(receiver&lt;- receivers)&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> executors = scheduledLocations(</span><br><span class=\"line\">\t\t\t\t\treceiver.streamId)</span><br><span class=\"line\">\t\t\t\tstartReceiver(receiver, executors)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-2-失败重启\"><a href=\"#1-1-2-失败重启\" class=\"headerlink\" title=\"1.1.2. 失败重启\"></a>1.1.2. <strong>失败重启</strong></h3><p><strong>1.获取之前的executors</strong></p>\n<p><strong>2. 计算新的excutor位置</strong></p>\n<p>​    <strong>2.1 之前excutors可用，则使用之前的</strong></p>\n<p>​    <strong>2.2 之前的不可用则重新计算位置</strong></p>\n<p><strong>3. 发送给worker重启receiver</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> schedulingPolicy=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSchedulingPolicy</span>() </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 首次启动</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span>(receivers) =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1.获取之前的executors</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> oldScheduledExecutors =getStoredScheduledExecutors(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.streamId</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 计算新的excutor位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t <span class=\"keyword\">val</span> scheduledLocations = <span class=\"keyword\">if</span> (oldScheduledExecutors.nonEmpty) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// 2.1 之前excutors可用，则使用之前的</span></span><br><span class=\"line\"></span><br><span class=\"line\">            oldScheduledExecutors</span><br><span class=\"line\"></span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 2.2 之前的不可用则重新计算位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tschedulingPolicy.rescheduleReceiver(）</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 发送给worker重启receiver</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t   startReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver, scheduledLocations)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-3-选择策略\"><a href=\"#1-1-3-选择策略\" class=\"headerlink\" title=\"1.1.3. 选择策略\"></a>1.1.3. <strong>选择策略</strong></h3><p><strong>策略选择由ReceiverSchedulingPolicy实现，默认策略是轮训（round-robin），在1.5版本之前是使用依赖 Spark Core 的 TaskScheduler 进行通用分发，</strong></p>\n<p><strong>在1.5之前存在executor分发不均衡问题导致Job执行失败：</strong></p>\n<p>如果某个 Task 失败超过 spark.task.maxFailures(默认=4) 次的话，整个 Job 就会失败。这个在长时运行的 Spark Streaming 程序里，Executor 多失效几次就有可能导致 Task 失败达到上限次数了，如果某个 Task 失效一下，Spark Core 的 TaskScheduler 会将其重新部署到另一个 executor 上去重跑。但这里的问题在于，负责重跑的 executor 可能是在下发重跑的那一刻是正在执行 Task 数较少的，但不一定能够将 Receiver 分布的最均衡的。</p>\n<p>策略代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> scheduledLocations =<span class=\"type\">ReceiverSchedulingPolicy</span>.scheduleReceivers(receivers,xecutors)</span><br><span class=\"line\"><span class=\"keyword\">val</span> scheduledLocations =<span class=\"type\">ReceiverSchedulingPolicy</span>.rescheduleReceiver(receiver, ...)</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-2-构造Receiver-RDD\"><a href=\"#1-2-构造Receiver-RDD\" class=\"headerlink\" title=\"1.2. 构造Receiver RDD\"></a>1.2. <strong>构造Receiver RDD</strong></h2><p><strong>将receiver列表转换为RDD</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstartReceiver(receiver, executors)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\">\t\t\treceiver: <span class=\"type\">Receiver</span>[_],</span><br><span class=\"line\"></span><br><span class=\"line\">        \tscheduledLocations: <span class=\"type\">Seq</span>[<span class=\"type\">TaskLocation</span>])&#123;\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> receiverRDD: <span class=\"type\">RDD</span>[<span class=\"type\">Receiver</span>] =</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (scheduledLocations.isEmpty) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          \t**ssc.sc.makeRDD(<span class=\"type\">Seq</span>(receiver), <span class=\"number\">1</span>)**</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          <span class=\"keyword\">val</span> preferredLocations = scheduledLocations.map(_.toString).distinct</span><br><span class=\"line\"></span><br><span class=\"line\">          ssc.sc.makeRDD(<span class=\"type\">Seq</span>(receiver -&gt; preferredLocations))</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      receiverRDD.setName(<span class=\"string\">s\" <span class=\"subst\">$receiverId</span>\"</span>)\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-3-定义启动reciever的Func\"><a href=\"#1-3-定义启动reciever的Func\" class=\"headerlink\" title=\"1.3. 定义启动reciever的Func\"></a>1.3. <strong>定义启动reciever的Func</strong></h2><p><strong>将每个receiver,spark环境变量，hadoop配置文件，检查点路径等信息传送给excutor的接收对象ReceiverSupervisorImpl</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">  ...</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> startReceiverFunc:</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"type\">Iterator</span>[<span class=\"type\">Receiver</span>[_]]=&gt;<span class=\"type\">Unit</span>=</span><br><span class=\"line\"></span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t(iterator:<span class=\"type\">Iterator</span>)=&gt;&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> receiver=iterator.next()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> supervisor=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSupervisoImpl</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">SparkEnv</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">HadoopConf</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcheckpointDir,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.start(),</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.awaitTermination()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  ...</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-4-分发RDD-Receiver-和Func到具体的excutor\"><a href=\"#1-4-分发RDD-Receiver-和Func到具体的excutor\" class=\"headerlink\" title=\"1.4. 分发RDD(Receiver)和Func到具体的excutor\"></a>1.4. <strong>分发RDD(Receiver)和Func到具体的excutor</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps247.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>将前两部定义的rdd和fun从driver提交到excutor</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> future=ssc.sparkContext.submitJob(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiverRDD,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstartReceverFunc,</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t  ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-1-启动Executor\"><a href=\"#1-4-1-启动Executor\" class=\"headerlink\" title=\"1.4.1. 启动Executor\"></a>1.4.1. <strong>启动Executor</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps248.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>Executor的启动在Receiver类中定义，在ReceiverSupervisor类中调用，在Receiver的子类中实现</strong></p>\n<p>excutor中共需要启动两个线程</p>\n<p>​    -1. 启动Receiver接收数据</p>\n<p>​    - 2. 启动pushingThread定时推送数据到driver</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisor</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\treceiver: <span class=\"type\">Receiver</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tconf: sparkConf</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">    startReceiver()</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-4-1-1-启动Receiver\"><a href=\"#1-4-1-1-启动Receiver\" class=\"headerlink\" title=\"1.4.1.1. 启动Receiver\"></a>1.4.1.1. <strong>启动Receiver</strong></h4><p><strong>启动Receiver，开始接收数据</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisor</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\treceiver: <span class=\"type\">Receiver</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tconf: sparkConf</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">    startReceiver()</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 1. 启动Receiver，开始接收数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiverState=<span class=\"type\">Started</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiver.onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-4-1-2-启动blockPushingThread\"><a href=\"#1-4-1-2-启动blockPushingThread\" class=\"headerlink\" title=\"1.4.1.2. 启动blockPushingThread\"></a>1.4.1.2. <strong>启动blockPushingThread</strong></h4><p><strong>启动pushTread，定时推送信息到driver</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisor</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\treceiver: <span class=\"type\">Receiver</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tconf: sparkConf</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">    startReceiver()</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 1. 启动Receiver，开始接收数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiverState=<span class=\"type\">Started</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiver.onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 2. 启动pushTread，定时推送信息到driver</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    registeredBlockGenerators.asScala.foreach &#123; \t\t_.start() </span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// _.start() 的实现</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockIntervalTimer.start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockPushingThread.start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-5-监控excutor\"><a href=\"#1-5-监控excutor\" class=\"headerlink\" title=\"1.5. 监控excutor\"></a>1.5. <strong>监控excutor</strong></h2><p>启动 Receiver 实例，并一直 block 住当前线程</p>\n<p>在1.5版本之前，一个job包含多个task，一个task失败次数失败超过4次后，整个Job都会失败，<strong>1.5版本之后一个job只包含一个task,并且添加了可重试机制</strong>，大大增加了job的活性</p>\n<p> Spark Core 的 Task 下发时只会参考并大部分时候尊重 Spark Streaming 设置的 preferredLocation 目的地信息，还是有一定可能该分发 Receiver 的 Job 并没有在我们想要调度的 executor 上运行。此时，在第 1 次执行 Task 时，会首先向 ReceiverTracker 发送 RegisterReceiver 消息，只有得到肯定的答复时，才真正启动 Receiver，否则就继续做一个空操作，导致本 Job 的状态是成功执行已完成。当然，ReceiverTracker 也会另外调起一个 Job，来继续尝试 Receiver 分发……如此直到成功为止。</p>\n<p>一个 Receiver 的分发 Job 是有可能没有完成分发 Receiver 的目的的，所以 ReceiverTracker 会继续再起一个 Job 来尝试 Receiver 分发。这个机制保证了，如果一次 Receiver 如果没有抵达预先计算好的 executor，就有机会再次进行分发，从而实现在 Spark Streaming 层面对 Receiver 所在位置更好的控制。</p>\n<p> 对 Receiver 的监控重启机制</p>\n<p>上面分析了每个 Receiver 都有专门的 Job 来保证分发后，我们发现这样一来，Receiver 的失效重启就不受 spark.task.maxFailures(默认=4) 次的限制了。</p>\n<p>因为现在的 Receiver 重试不是在 Task 级别，而是在 Job 级别；并且 Receiver 失效后并不会导致前一次 Job 失败，而是前一次 Job 成功、并新起一个 Job 再次进行分发。这样一来，不管 Spark Streaming 运行多长时间，Receiver 总是保持活性的，不会随着 executor 的丢失而导致 Receiver 死去。</p>\n<p>// todo 阻塞，知道executor返回发送结果</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\"> \t\tfuture.onComplete &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">Success</span>(_)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">Failure</span>())=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tonReceiverJobFinish(receiverId)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;&#125;(<span class=\"type\">ThreadUtils</span>.sameThread)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"2-数据接收\"><a href=\"#2-数据接收\" class=\"headerlink\" title=\"2. 数据接收\"></a>2. <strong>数据接收</strong></h1><p>每个 ReceiverSupervisor 启动后将马上生成一个用户提供的 Receiver 实现的实例 —— 该 Receiver 实现可以持续产生或者持续接收系统外数据，比如 TwitterReceiver 可以实时爬取 twitter 数据 —— 并在 Receiver 实例生成后调用 Receiver.onStart()。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps25A.tmp.jpg\" alt=\"img\"> </p>\n<p> <strong>数据的接收由Executor端的Receiver实现，启动和停止需要子类实现，存储基类实现，供子类调用</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Receiver</span>[<span class=\"type\">T</span>](<span class=\"params\">val storageLevel: <span class=\"type\">StorageLevel</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 启动和停止需要子类实现</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStop</span></span>()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储单条小数据】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataItem: <span class=\"type\">T</span>) &#123;...&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储数组形式的块数据】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataBuffer: <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">T</span>]) &#123;...&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储 iterator 形式的块数据】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataIterator: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) &#123;...&#125;   </span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储 ByteBuffer 形式的块数据】 </span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(bytes: <span class=\"type\">ByteBuffer</span>) &#123;...&#125;         </span><br><span class=\"line\"></span><br><span class=\"line\">   ...</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps25B.tmp.jpg\" alt=\"img\"> </p>\n<p>通过kafka去接收数据，</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span> <span class=\"title\">**extends</span> <span class=\"title\">Receiver**</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\t_ssc : <span class=\"type\">StreamingContext</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tkafkaParams : <span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\ttopics : <span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tuseReliableReceiver : <span class=\"type\">Boolean</span> </span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tstorageLevel : <span class=\"type\">StorageLevel</span> </span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-1-获取kafka参数\"><a href=\"#2-3-1-获取kafka参数\" class=\"headerlink\" title=\"2.3.1. 获取kafka参数\"></a>2.3.1. 获取kafka参数</h3><p>拼接kafka consumer所需参数</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t **<span class=\"comment\">// 1. 获取kafka参数**\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-2-链接到kafka\"><a href=\"#2-3-2-链接到kafka\" class=\"headerlink\" title=\"2.3.2. 链接到kafka\"></a>2.3.2. <strong>链接到kafka</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// kafka链接器</span></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> consumerConnector:<span class=\"type\">ConsumerConnector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 获取kafka参数\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 链接到kafka</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> consumerConf=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ConsumerConfig</span>(props)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconsumerConnector=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Consumer</span>.create(consumerConf)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-3-监听所有topic\"><a href=\"#2-3-3-监听所有topic\" class=\"headerlink\" title=\"2.3.3. 监听所有topic\"></a>2.3.3. 监听所有topic</h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// kafka链接器</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> consumerConnector:<span class=\"type\">ConsumerConnector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 获取kafka参数\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 链接到kafka</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> consumerConf=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ConsumerConfig</span>(props)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconsumerConnector=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Consumer</span>.create(consumerConf)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 3. 监听所有topic</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> topicMessageStreams=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tconsumerConnector.createMessage()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> executorPool=<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tnewDaemonFixedTreadPool(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttopics.values.sum,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"kafkaMessageHandler\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttopicMessageStreams.values.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreams=&gt;streams.foreach&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstream=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\texecutorPool.submit(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">MessageHandler</span>(stream)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-4-异步保存数据\"><a href=\"#2-3-4-异步保存数据\" class=\"headerlink\" title=\"2.3.4. 异步保存数据\"></a>2.3.4. 异步保存数据</h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// kafka链接器</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> consumerConnector:<span class=\"type\">ConsumerConnector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 获取kafka参数\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 链接到kafka</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> consumerConf=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ConsumerConfig</span>(props)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconsumerConnector=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Consumer</span>.create(consumerConf)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 3. 监听所有topic</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> topicMessageStreams=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tconsumerConnector.createMessage()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> executorPool=<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tnewDaemonFixedTreadPool(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttopics.values.sum,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"kafkaMessageHandler\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttopicMessageStreams.values.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreams=&gt;streams.foreach&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstream=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\texecutorPool.submit(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">MessageHandler</span>(stream)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 4. 异步保存数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MessageHandler</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\t stream:<span class=\"type\">KafkaStream</span>[<span class=\"type\">K</span>,<span class=\"type\">V</span>]</span>) <span class=\"keyword\">extends</span> <span class=\"title\">Runable</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t \t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">val</span> streamIterator=stream.iterator()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span>(streamIterator.hasNext())&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> msgAndMetadata=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstreamIterator.next()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**store(**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t**msgAndMetadata.key,**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t**msgAndMetadata.message**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**)**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>自定义的Receiver只需要继承Receiver类，并实现onStart方法里新拉起数据接收线程，并在接收到数据时 store() 到 Spark Streamimg 框架就可以了。</p>\n<h1 id=\"3-数据转存\"><a href=\"#3-数据转存\" class=\"headerlink\" title=\"3. 数据转存\"></a>3. <strong>数据转存</strong></h1><p>Receiver 在 onStart() 启动后，就将持续不断地接收外界数据，并持续交给 ReceiverSupervisor 进行数据转储</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps260.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"3-1-存储格式\"><a href=\"#3-1-存储格式\" class=\"headerlink\" title=\"3.1. 存储格式\"></a>3.1. <strong>存储格式</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps261.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>Receiver在调用store方法后，根据不同的入参会调用ReceiverSupervisor的不同方法。ReceiverSupervisor的方法由ReceiverSupervisorImpl实现</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Receiver</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> supervisor:<span class=\"type\">ReceiverSupervisor</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 1.单条数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">strore</span></span>(dataItem: <span class=\"type\">T</span> )&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pushSigle(dataItem)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 2. byte数组</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(bytes : <span class=\"type\">ByteBuffer</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pushBytes(bytes,<span class=\"type\">None</span>,<span class=\"type\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 3. 迭代器格式</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataIterator : <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>])&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pusthIteratro(dataIterator)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 4. ByteBuffer格式</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataBuffer:<span class=\"type\">ArrayBuffer</span>[<span class=\"type\">T</span>])&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pushArrayBuffer(dataBuffer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-1-单条数据\"><a href=\"#3-1-1-单条数据\" class=\"headerlink\" title=\"3.1.1. 单条数据\"></a>3.1.1. <strong>单条数据</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps272.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>调用ReceiverSupervisorImpl的pushSigle方法保存单条数据</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> defaultBlockGenerator=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">BlockGenerator</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockGeneratorListener, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\tenv.conf</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushSinge</span></span>(data:<span class=\"type\">Any</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tdefaultBlockGenerator.addData(data)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-1-控制batch大小\"><a href=\"#3-1-1-1-控制batch大小\" class=\"headerlink\" title=\"3.1.1.1. 控制batch大小\"></a>3.1.1.1. <strong>控制batch大小</strong></h4><p><strong>先检查接收数据的频率，控制住频率就控制了每个batch需要处理的最大数据量</strong></p>\n<p>就是在加入 currentBuffer 数组时会先由 rateLimiter 检查一下速率，是否加入的频率已经太高。如果太高的话，就需要 block 住，等到下一秒再开始添加。这里的最高频率是由 spark.streaming.receiver.maxRate (default = Long.MaxValue) 控制的，是单个 Receiver 每秒钟允许添加的条数。控制了这个速率，就控制了整个 Spark Streaming 系统每个 batch 需要处理的最大数据量。</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RateLimiter</span>(<span class=\"params\">conf:<span class=\"type\">SparkConf</span></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> maxRateLimit=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconf.getLong(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"string\">\"spark.streaming.receiver.maxRate\"</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">Long</span>.<span class=\"type\">MaxValue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> rateLimiter=<span class=\"type\">GuavaRateLimiter</span>.create(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tmaxRateLimit.toDouble</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">waitToPush</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\trateLimiter.acquire()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-2-添加数据到arrayBuffer\"><a href=\"#3-1-1-2-添加数据到arrayBuffer\" class=\"headerlink\" title=\"3.1.1.2. 添加数据到arrayBuffer\"></a>3.1.1.2. <strong>添加数据到arrayBuffer</strong></h4><p><strong>如果频率正常，则把数据添加到数组中，否则抛异常</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> currentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 添加数据到currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(state==<span class=\"type\">Active</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer+=data</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">SparkException</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"string\">\"connot add data ...\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-3-定时放入blocakQueue\"><a href=\"#3-1-1-3-定时放入blocakQueue\" class=\"headerlink\" title=\"3.1.1.3. 定时放入blocakQueue\"></a>3.1.1.3. <strong>定时放入blocakQueue</strong></h4><p>3.1 清空currentBuffer</p>\n<p>3.2 将block块放入blocakQueue</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> currentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 定时器：定时更新currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blockIntervalTimer=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tclock,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockIntervalMs,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tupdateCurrentBuffer,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"string\">\"BlockGenerator\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 保存block的数组大小，默认是10</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> queueSize=conf.getInt(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"string\">\"spark.streaming.blockQueueSize\"</span>,<span class=\"number\">10</span>)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blocksForPushing=</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">ArrayBlockingQueue</span>[<span class=\"type\">Block</span>](queueSize)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 添加数据到currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer+=data</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateCurrentBuffer</span></span>(timer:<span class=\"type\">Long</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">var</span> newBlock:<span class=\"type\">Block</span>=<span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3.1 清空currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> newBlockBuffer=currentBuffer</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 2 将block块放入blocakQueue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnewBlock=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">Block</span>(id,newBlockBuffer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblocksForPushing.put(newBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-4-保存并推送blocks\"><a href=\"#3-1-1-4-保存并推送blocks\" class=\"headerlink\" title=\"3.1.1.4. 保存并推送blocks\"></a>3.1.1.4. <strong>保存并推送blocks</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps286.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>在初始化BlockGenerator时，启动一个线程去持续的执行pushBlocks方法。如果还没有生成blocks，则阻塞调用queue.poll去获取数据，如果已经存在blocks块，则直接queue.take(10)</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> currentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 定时器：定时更新currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blockIntervalTimer=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tclock,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockIntervalMs,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tupdateCurrentBuffer,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"string\">\"BlockGenerator\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 保存block的数组大小，默认是10</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> queueSize=conf.getInt(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"string\">\"spark.streaming.blockQueueSize\"</span>,<span class=\"number\">10</span>)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blocksForPushing=</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">ArrayBlockingQueue</span>[<span class=\"type\">Block</span>](queueSize)</span><br><span class=\"line\"></span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 推送block块</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blockPushingThread=<span class=\"keyword\">new</span> <span class=\"type\">Thread</span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>()&#123;keepPushingBlocks()&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 添加数据到currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer+=data</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateCurrentBuffer</span></span>(timer:<span class=\"type\">Long</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">var</span> newBlock:<span class=\"type\">Block</span>=<span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3.1 清空currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> newBlockBuffer=currentBuffer</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]</span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 2 将block块放入blocakQueue</span></span><br><span class=\"line\">\t\t\t\tnewBlock=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">Block</span>(id,newBlockBuffer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblocksForPushing.put(newBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">keepPushingBlocks</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// **4.1 当block正在产时，等待其生成**</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span>(areBlocksBeingGenerated)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Option</span>(blocksForPushing.poll(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\twaitingTime</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t) <span class=\"keyword\">match</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">Some</span>(block)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\tpushBLock(block)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 4.2 block块已经生成</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span>(!blocksForPushing.isEmpty)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> block=blocksForPushing.take()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tpushBlock(block)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-1-1-4-1-保存\"><a href=\"#3-1-1-4-1-保存\" class=\"headerlink\" title=\"3.1.1.4.1. 保存\"></a>3.1.1.4.1. <strong>保存</strong></h5><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushAndReportBlock</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> blockStoreResult =</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t**receivedBlockHandler.storeBlock**(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlock</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-1-1-4-2-推送\"><a href=\"#3-1-1-4-2-推送\" class=\"headerlink\" title=\"3.1.1.4.2. 推送\"></a>3.1.1.4.2. <strong>推送</strong></h5><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushAndReportBlock</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> blockStoreResult =</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceivedBlockHandler.**storeBlock**(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlock</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t    <span class=\"keyword\">val</span> blockInfo = <span class=\"type\">ReceivedBlockInfo</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tmetadataOption, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockStoreResult</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    trackerEndpoint.askSync[<span class=\"type\">Boolean</span>](<span class=\"type\">AddBlock</span>(blockInfo))</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-2-bytes格式数据\"><a href=\"#3-1-2-bytes格式数据\" class=\"headerlink\" title=\"3.1.2. bytes格式数据\"></a>3.1.2. <strong>bytes格式数据</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>   <span class=\"title\">ReceiverSupervisorImpl</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushBytes</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">      bytes: <span class=\"type\">ByteBuffer</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">      metadataOption: <span class=\"type\">Option</span>[<span class=\"type\">Any</span>],</span><br><span class=\"line\"></span><br><span class=\"line\">      blockIdOption: <span class=\"type\">Option</span>[<span class=\"type\">StreamBlockId</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    pushAndReportBlock(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"type\">ByteBufferBlock</span>(bytes), </span><br><span class=\"line\"></span><br><span class=\"line\">\t\tmetadataOption,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockIdOption</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-3-iterator格式数据\"><a href=\"#3-1-3-iterator格式数据\" class=\"headerlink\" title=\"3.1.3. iterator格式数据\"></a>3.1.3. <strong>iterator格式数据</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>   <span class=\"title\">ReceiverSupervisorImpl</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushIterator</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">      iterator: <span class=\"type\">Iterator</span>[_],</span><br><span class=\"line\"></span><br><span class=\"line\">      metadataOption: <span class=\"type\">Option</span>[<span class=\"type\">Any</span>],</span><br><span class=\"line\"></span><br><span class=\"line\">      blockIdOption: <span class=\"type\">Option</span>[<span class=\"type\">StreamBlockId</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    pushAndReportBlock(<span class=\"type\">IteratorBlock</span>(iterator), metadataOption, blockIdOption)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-4-ByteBuffer格式数据\"><a href=\"#3-1-4-ByteBuffer格式数据\" class=\"headerlink\" title=\"3.1.4. ByteBuffer格式数据\"></a>3.1.4. <strong>ByteBuffer格式数据</strong></h3> <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>   <span class=\"title\">ReceiverSupervisorImpl</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushArrayBuffer</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">      arrayBuffer: <span class=\"type\">ArrayBuffer</span>[_],</span><br><span class=\"line\"></span><br><span class=\"line\">      metadataOption: <span class=\"type\">Option</span>[<span class=\"type\">Any</span>],</span><br><span class=\"line\"></span><br><span class=\"line\">      blockIdOption: <span class=\"type\">Option</span>[<span class=\"type\">StreamBlockId</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    pushAndReportBlock(<span class=\"type\">ArrayBufferBlock</span>(arrayBuffer), metadataOption, blockIdOption)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-2-存储方式\"><a href=\"#3-2-存储方式\" class=\"headerlink\" title=\"3.2. 存储方式\"></a>3.2. <strong>存储方式</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps297.tmp.jpg\" alt=\"img\"> </p>\n<p>ReceivedBlockHandler 有两个具体的存储策略的实现：</p>\n<p>(a) BlockManagerBasedBlockHandler，是直接存到 executor 的内存或硬盘</p>\n<p>(b) WriteAheadLogBasedBlockHandler，是先写 WAL，再存储到 executor 的内存或硬盘</p>\n<h3 id=\"3-2-1-BlockManager\"><a href=\"#3-2-1-BlockManager\" class=\"headerlink\" title=\"3.2.1. BlockManager\"></a>3.2.1. <strong>BlockManager</strong></h3><p><strong>将数据存储交给blockManager进行管理，调用blockmanager的putIterator方法，由其实现在不同excutor上的复制以及缓存策略。</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockManagerBasedBlockHandler</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tblockManager:<span class=\"type\">BlockManager</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tstorageLevel:<span class=\"type\">StorageLevel</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)<span class=\"keyword\">extends</span> <span class=\"title\">ReceivedBlockHandler</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">storeBlock</span></span>(blockId,block)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">var</span> numRecords:<span class=\"type\">Option</span>[<span class=\"type\">Long</span>]=<span class=\"type\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> putSucceeded:<span class=\"type\">Boolean</span> = block <span class=\"keyword\">match</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">ArrayBufferBlock</span>(arrayBuffer)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords=<span class=\"type\">Some</span>(arrayBuffer.size)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockManager.putIterator(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tarrayBuffer.iterator,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstorageLevel,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\ttellMaster=<span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">IteratorBlock</span>(iterator)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> countIterator=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">CountingIterator</span>(iterator)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> putResult=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t**blockManager.putIterato**r(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tarrayBuffer.iterator,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tstorageLevel,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\ttellMaster=<span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords=countIterator.count</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tputResult</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">ByteBufferBlock</span>(byteBuffer)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockManager.putBytes(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ChunkedBytedBuffer</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tbyteBuffer.duplicate(),</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tstorageLevel,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\ttellMaster=<span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 报告给driver的信息：id和num</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">BlockManagerBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ChunkedBytedBuffer: 将byte数组分片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// byteBuffer.duplicate(): 复制</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-2-WAL\"><a href=\"#3-2-2-WAL\" class=\"headerlink\" title=\"3.2.2. WAL\"></a>3.2.2. <strong>WAL</strong></h3><p>WriteAheadLogBasedBlockHandler 的实现则是同时写到可靠存储的 WAL 中和 executor 的 BlockManager 中；在<strong>两者都写完成后，再上报块数据的 meta 信息</strong>。</p>\n<p><strong>BlockManager 中的块数据是计算时首选使用的，只有在 executor 失效时，才去 WAL 中读取写入过的数据</strong>。</p>\n<p>同其它系统的 WAL 一样，数据是完全顺序地写入 WAL 的；在稍后上报块数据的 meta 信息，就额外包含了块数据所在的 WAL 的路径，及在 WAL 文件内的偏移地址和长度。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WriteAheadLogBasedBlockHandler</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tblockManager: <span class=\"type\">BlockManager</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    serializerManager: <span class=\"type\">SerializerManager</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    streamId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    storageLevel: <span class=\"type\">StorageLevel</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    conf: <span class=\"type\">SparkConf</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    hadoopConf: <span class=\"type\">Configuration</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    checkpointDir: <span class=\"type\">String</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    clock: <span class=\"type\">Clock</span> = new <span class=\"type\">SystemClock</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)<span class=\"keyword\">extends</span> <span class=\"title\">ReceivedBlockHandler</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 保存超时时间</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tblockStoreTimeout = conf.getInt(    \t\t</span><br><span class=\"line\">    </span><br><span class=\"line\">    \t\t<span class=\"string\">\"spark.streaming.receiver.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\t\t\tblockStoreTimeout\"</span>,<span class=\"number\">30</span>).seconds</span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 写log类</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> writeAheadLog=<span class=\"type\">WriteAheadLogUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcreatLogForReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tconf,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcheckpointDirToLogDir(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tcheckpointDir,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstreamId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\thadoopConf</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">storeBlock</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 执行blockManager</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> serializedBlock = block <span class=\"keyword\">match</span> &#123;...&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 执行保存到log</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 用future异步执行</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> storeInBlockManagerFuture=<span class=\"type\">Future</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockManger.putBytes(...serializedBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> storeInWriteAheadLogFuture=<span class=\"type\">Future</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\twriteAheadLog.write(...serializedBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> combineFuture=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstoreInBlockManagerFuture.zip(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstoreInWriteAHeadLogFuture\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t).map(_._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> walRecordHandle=<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tawaitUtils.awaitResult(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcombineFuture,blockStoreTimeout</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"type\">WriteAheandLogBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tnumRecords,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\twalRecordHandle</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// future1.zip(future2): 合并future,返回tuple(future)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 两个future中有一个失败，则失败</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"4-数据上报\"><a href=\"#4-数据上报\" class=\"headerlink\" title=\"4. 数据上报\"></a>4. <strong>数据上报</strong></h1><p>每次成块在 executor 存储完毕后，ReceiverSupervisor 就会及时上报块数据的 meta 信息给 driver 端的 ReceiverTracker；这里的 meta 信息包括数据的标识 id，数据的位置，数据的条数，数据的大小等信息</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps298.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>ReceiverSupervisor会将数据的标识ID，数据的位置，数据的条数，数据的大小等信息上报给driver</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushAndReportBlock</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> blockStoreResult =</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceivedBlockHandler.storeBlock(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlock</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t    <span class=\"keyword\">val</span> blockInfo = <span class=\"type\">ReceivedBlockInfo</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**streamId,** </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**numRecords,** </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**metadataOption,** </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**blockStoreResult**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    trackerEndpoint.askSync[<span class=\"type\">Boolean</span>](<span class=\"type\">AddBlock</span>(blockInfo))</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-1-wal上报\"><a href=\"#4-1-wal上报\" class=\"headerlink\" title=\"4.1. wal上报\"></a>4.1. <strong>wal上报</strong></h2><p>// 报告给driver的信息：blockId，block数量，<strong>walRecordHandle</strong>        </p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">WriteAheandLogBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\tnumRecords,</span><br><span class=\"line\"></span><br><span class=\"line\">\t**walRecordHandle**</span><br><span class=\"line\"></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-2-BlockManager上报\"><a href=\"#4-2-BlockManager上报\" class=\"headerlink\" title=\"4.2. BlockManager上报\"></a>4.2. <strong>BlockManager上报</strong></h2><p>// <strong>报告给driver的信息：id和num</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">BlockManagerBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\tnumRecords</span><br><span class=\"line\"></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<h1 id=\"5-receiver管理\"><a href=\"#5-receiver管理\" class=\"headerlink\" title=\"5. receiver管理\"></a>5. <strong>receiver管理</strong></h1><ol>\n<li>分发和监控receiver</li>\n<li>作为RpcEndpoint和reciever通信，接收和发送消息</li>\n<li>管理上报的meta信息</li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps299.tmp.jpg\" alt=\"img\"> </p>\n<p>一方面 Receiver 将通过 AddBlock 消息上报 meta 信息给 ReceiverTracker，另一方面 JobGenerator 将在每个 batch 开始时要求 ReceiverTracker 将已上报的块信息进行 batch 划分，ReceiverTracker 完成了块数据的 meta 信息管理工作。</p>\n<p>具体的，ReceiverTracker 有一个成员 ReceivedBlockTracker，专门负责已上报的块数据 meta 信息管理。</p>\n<h2 id=\"5-1-分发\"><a href=\"#5-1-分发\" class=\"headerlink\" title=\"5.1. 分发\"></a>5.1. <strong>分发</strong></h2><p>在 ssc.start() 时，将隐含地调用 ReceiverTracker.start()；而 ReceiverTracker.start() 最重要的任务就是调用自己的 launchReceivers() 方法将 Receiver 分发到多个 executor 上去。然后在每个 executor 上，由 <strong>ReceiverSupervisor</strong> 来分别启动一个 Receiver 接收数据</p>\n<p>而且在 1.5.0 版本以来引入了 <strong>ReceiverSchedulingPolicy</strong>，是在 Spark Streaming 层面添加对 Receiver 的分发目的地的计算，相对于之前版本依赖 Spark Core 的 TaskScheduler 进行通用分发，新的 ReceiverSchedulingPolicy 会对 Streaming 应用的更好的语义理解，也能计算出更好的分发策略。</p>\n<p>并且还通过每个 <strong>Receiver</strong> 对应 1 个 Job 的方式，保证了 Receiver 的多次分发，和失效后的重启、永活</p>\n<h2 id=\"5-2-监控\"><a href=\"#5-2-监控\" class=\"headerlink\" title=\"5.2. 监控\"></a>5.2. <strong>监控</strong></h2><h2 id=\"5-3-消息类型\"><a href=\"#5-3-消息类型\" class=\"headerlink\" title=\"5.3. 消息类型\"></a>5.3. <strong>消息类型</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2A9.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>ReceiverTracker</strong>：</p>\n<p>RpcEndPoint 可以理解为 RPC 的 server 端,底层由netty提供通信支持，供 client 调用。</p>\n<p>ReceiverTracker 作为 RpcEndPoint 的地址 —— 即 driver 的地址 —— 是公开的，可供 Receiver 连接；如果某个 Receiver 连接成功，那么 ReceiverTracker 也就持有了这个 Receiver 的 RpcEndPoint。这样一来，通过发送消息，就可以实现双向通信。</p>\n<h3 id=\"5-3-1-只接收不回复\"><a href=\"#5-3-1-只接收不回复\" class=\"headerlink\" title=\"5.3.1. 只接收不回复\"></a>5.3.1. <strong>只接收不回复</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2AA.tmp.jpg\" alt=\"img\"> </p>\n<p>只接收消息不回复，除了错误上报消息是excutor发送的以外，其余都是driver的tracker自己给自己发送的命令,接收消息均在ReceiverTracker.receive方法中实现</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-1-首次启动消息\"><a href=\"#5-3-1-1-首次启动消息\" class=\"headerlink\" title=\"5.3.1.1. 首次启动消息\"></a>5.3.1.1. <strong>首次启动消息</strong></h4><p>在 ReceiverTracker 刚启动时，发给自己这个消息，触发具体的 schedulingPolicy 计算，和后续分发</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> scheduledLocations = schedulingPolicy.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tscheduleReceivers(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceivers, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tgetExecutors</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (receiver &lt;- receivers) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          <span class=\"keyword\">val</span> executors = scheduledLocations(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">          updateReceiverScheduledExecutors(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\texecutors</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">          receiverPreferredLocations(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.streamId) = </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceiver.preferredLocation</span><br><span class=\"line\"></span><br><span class=\"line\">          \tstartReceiver(receiver, executors)</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-2-重新启动消息\"><a href=\"#5-3-1-2-重新启动消息\" class=\"headerlink\" title=\"5.3.1.2. 重新启动消息\"></a>5.3.1.2. <strong>重新启动消息</strong></h4><p>当初始分发的 executor 不对，或者 Receiver 失效等情况出现，发给自己这个消息，触发 Receiver 重新分发</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1.获取之前的executors</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> oldScheduledExecutors = \t\t\tgetStoredScheduledExecutors(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.streamId</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 计算新的excutor位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t <span class=\"keyword\">val</span> scheduledLocations = <span class=\"keyword\">if</span> \t\t\t(oldScheduledExecutors.nonEmpty) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// 2.1 之前excutors可用，则使用之前的</span></span><br><span class=\"line\"></span><br><span class=\"line\">            oldScheduledExecutors</span><br><span class=\"line\"></span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 2.2 之前的不可用则重新计算位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tschedulingPolicy.rescheduleReceiver(）</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 发送给worker重启receiver</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t   startReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver, scheduledLocations)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-3-清除blocks消息\"><a href=\"#5-3-1-3-清除blocks消息\" class=\"headerlink\" title=\"5.3.1.3. 清除blocks消息\"></a>5.3.1.3. <strong>清除blocks消息</strong></h4><p>当块数据已完成计算不再需要时，发给自己这个消息，将给所有的 Receiver 转发此 CleanupOldBlocks 消息</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceiverTrackingInfos.values.flatMap(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t_.endpoint</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t).foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t_.send(c)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-4-更新接收频率\"><a href=\"#5-3-1-4-更新接收频率\" class=\"headerlink\" title=\"5.3.1.4. 更新接收频率\"></a>5.3.1.4. <strong>更新接收频率</strong></h4><p>ReceiverTracker 动态计算出某个 Receiver 新的 rate limit，将给具体的 Receiver 发送 UpdateRateLimit 消息</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t <span class=\"keyword\">for</span> (info &lt;- \t\t\treceiverTrackingInfos.get(streamUID);</span><br><span class=\"line\"></span><br><span class=\"line\"> \t\t\teP &lt;- info.endpoint) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          \t\teP.send(<span class=\"type\">UpdateRateLimit</span>(newRate))</span><br><span class=\"line\"></span><br><span class=\"line\">        \t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-5-错误上报消息\"><a href=\"#5-3-1-5-错误上报消息\" class=\"headerlink\" title=\"5.3.1.5. 错误上报消息\"></a>5.3.1.5. <strong>错误上报消息</strong></h4> <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\treportError(streamId, message, error)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-3-2-接收并回复\"><a href=\"#5-3-2-接收并回复\" class=\"headerlink\" title=\"5.3.2. 接收并回复\"></a>5.3.2. <strong>接收并回复</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2AB.tmp.jpg\" alt=\"img\"> </p>\n<p>接收executor的消息，处理完毕后并回复给executor</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-1-注册Receiver消息\"><a href=\"#5-3-2-1-注册Receiver消息\" class=\"headerlink\" title=\"5.3.2.1. 注册Receiver消息\"></a>5.3.2.1. <strong>注册Receiver消息</strong></h4><p>由 Receiver 在试图启动的过程中发来，将回复允许启动，或不允许启动</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> successful=registerReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreamId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"class\"><span class=\"keyword\">type</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">host</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">executorId</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">receiverEndpoint</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">context</span>.<span class=\"title\">senderAddress</span>)</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">context</span>.<span class=\"title\">reply</span>(<span class=\"params\">successful</span>)</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t</span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">AddBlock</span>(<span class=\"params\"></span>) <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">DeregisterReceiver</span>(<span class=\"params\"></span>) <span class=\"title\">=&gt;</span> ... </span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">AllReceiverIds</span> <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">GetAllReceiverInfo</span> <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">StopAllReceivers</span> <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t&#125;</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">&#125;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-2-meta上报消息\"><a href=\"#5-3-2-2-meta上报消息\" class=\"headerlink\" title=\"5.3.2.2. meta上报消息\"></a>5.3.2.2. <strong>meta上报消息</strong></h4><p>具体的块数据 meta 上报消息，由 Receiver 发来，将返回成功或失败</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcontext.reply(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\taddBlock(receivedBlockInfo)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-3-查询ReceiverIds消息\"><a href=\"#5-3-2-3-查询ReceiverIds消息\" class=\"headerlink\" title=\"5.3.2.3. 查询ReceiverIds消息\"></a>5.3.2.3. <strong>查询ReceiverIds消息</strong></h4><p>executor发送的本地消息。在 ReceiverTracker stop() 的过程中，查询是否还有活跃的 Receiver，返回所有或者的receiverId</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcontext.reply(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiverTrackingInfos.filter(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t_._2.state != \t\t\t\t\t<span class=\"type\">ReceiverState</span>.<span class=\"type\">INACTIVE</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t).keys.toSeq</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-4-查询ReceiversInfo消息\"><a href=\"#5-3-2-4-查询ReceiversInfo消息\" class=\"headerlink\" title=\"5.3.2.4. 查询ReceiversInfo消息\"></a>5.3.2.4. <strong>查询ReceiversInfo消息</strong></h4><p>查询所有excutors的信息给receiver</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcontext.reply(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiverTrackingInfos.toMap</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-5-停止所有Receiver消息\"><a href=\"#5-3-2-5-停止所有Receiver消息\" class=\"headerlink\" title=\"5.3.2.5. 停止所有Receiver消息\"></a>5.3.2.5. <strong>停止所有Receiver消息</strong></h4><p>在 ReceiverTracker stop() 的过程刚开始时，要求 stop 所有的 Receiver；将向所有的 Receiver 发送 stop 信息,并返回true</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t assert(isTrackerStopping || \t\t\t\t\tisTrackerStopped)</span><br><span class=\"line\"></span><br><span class=\"line\">        \t\t\treceiverTrackingInfos.values.flatMap(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t_.endpoint</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t).foreach &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t _.send(<span class=\"type\">StopReceiver</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        \tcontext.reply(<span class=\"literal\">true</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-6-注销Receiver消息\"><a href=\"#5-3-2-6-注销Receiver消息\" class=\"headerlink\" title=\"5.3.2.6. 注销Receiver消息\"></a>5.3.2.6. <strong>注销Receiver消息</strong></h4><p>由 Receiver 发来，停止receiver，处理后，无论如何都返回 true</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tderegisterReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tmessage, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\terror</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">       \t\t context.reply(<span class=\"literal\">true</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-4-meta信息管理\"><a href=\"#5-4-meta信息管理\" class=\"headerlink\" title=\"5.4. meta信息管理\"></a>5.4. <strong>meta信息管理</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2BC.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"5-4-1-接收meta信息\"><a href=\"#5-4-1-接收meta信息\" class=\"headerlink\" title=\"5.4.1. 接收meta信息\"></a>5.4.1. <strong>接收meta信息</strong></h3><p>addBlock(receivedBlockInfo: ReceivedBlockInfo)方法接收到某个 Receiver 上报上来的块数据 meta 信息，将其加入到 streamIdToUnallocatedBlockQueues 里</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 上报上来的、但尚未分配入 batch 的 Block 块数据的 meta</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> streamIdToUnallocatedBlockQueues = </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ReceivedBlockQueue</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// WAL</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> writeResult=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\twriteToLog(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">BlockAdditionEvent</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceivedBlockInfo</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span>(writeResult)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreamIdToUnallocatedBlockQueues.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tgetOrElseUpdate(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceivedBlockQueue</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)+=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlockInfo</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-4-2-batch分配\"><a href=\"#5-4-2-batch分配\" class=\"headerlink\" title=\"5.4.2. batch分配\"></a>5.4.2. <strong>batch分配</strong></h3><p>JobGenerator 在发起新 batch 的计算时，将 streamIdToUnallocatedBlockQueues 的内容，以传入的 batchTime 参数为 key，<strong>添加到 timeToAllocatedBlocks 里，并更新 lastAllocatedBatchTime</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 上报上来的、已分配入 batch 的 Block 块数据的 meta,按照 batch 进行一级索引、再按照 receiverId 进行二级索引的 queue，所以是一个 HashMap: time → HashMap</span></span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> timeToAllocatedBlocks = </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">Time</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">AllocatedBlocks</span>:<span class=\"type\">Map</span>[</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"type\">Int</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"type\">Seq</span>[<span class=\"type\">ReceivedBlockInfo</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 记录了最近一个分配完成的 batch 是哪个</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> lastAllocatedBatchTime: <span class=\"type\">Time</span> = <span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 收集所有未分配的blocks</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allocateBlocksToBatch</span></span>(batchTime: <span class=\"type\">Time</span>): \t<span class=\"type\">Unit</span> = synchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断时间是否合法：大于最近收集的时间</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (lastAllocatedBatchTime == <span class=\"literal\">null</span> || batchTime &gt; lastAllocatedBatchTime) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t <span class=\"comment\">// 从未分配队列中取出blocks</span></span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> streamIdToBlocks = streamIds.map &#123; \t\t\t</span><br><span class=\"line\">    \t\tstreamId =&gt;(streamId,getReceivedBlockQueue(streamId)\t\t\t\t\t</span><br><span class=\"line\">\t\t\t\t.dequeueAll(x =&gt; <span class=\"literal\">true</span>))</span><br><span class=\"line\">      \t\t&#125;.toMap</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> allocatedBlocks =<span class=\"type\">AllocatedBlocks</span>(streamIdToBlocks)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (writeToLog(<span class=\"type\">BatchAllocationEvent</span>(batchTime, allocatedBlocks))) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 放入已分配队列</span></span><br><span class=\"line\"></span><br><span class=\"line\">        timeToAllocatedBlocks.put(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tbatchTime, allocatedBlocks)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 更新最近分配的时间戳</span></span><br><span class=\"line\"></span><br><span class=\"line\">        lastAllocatedBatchTime = batchTime</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">     logInfo(<span class=\"string\">s\"Possibly processed batch <span class=\"subst\">$batchTime</span> needs to be processed again in WAL recovery\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-4-3-计算DAG生成\"><a href=\"#5-4-3-计算DAG生成\" class=\"headerlink\" title=\"5.4.3. 计算DAG生成\"></a>5.4.3. <strong>计算DAG生成</strong></h3><p>JobGenerator 在发起新 batch 的计算时，由 DStreamGraph 生成 RDD DAG 实例时，调用getBlocksOfBatch(batchTime: Time)查 timeToAllocatedBlocks，获得划入本 batch 的块数据元信息，由此生成处理对应块数据的 RDD</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getBlocksOfBatch</span></span>(batchTime: <span class=\"type\">Time</span>): \t<span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">ReceivedBlockInfo</span>]] = \tsynchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    timeToAllocatedBlocks.get(batchTime).map &#123; \t\t_.streamIdToAllocatedBlocks \t&#125;.getOrElse(<span class=\"type\">Map</span>.empty)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-4-4-清除meta信息\"><a href=\"#5-4-4-清除meta信息\" class=\"headerlink\" title=\"5.4.4. 清除meta信息\"></a>5.4.4. <strong>清除meta信息</strong></h3><p>当一个 batch 已经计算完成、可以把已追踪的块数据的 meta 信息清理掉时调用，将通过job清理 timeToAllocatedBlocks 表里对应 cleanupThreshTime 之前的所有 batch 块数据 meta 信息</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cleanupOldBatches</span></span>(cleanupThreshTime: <span class=\"type\">Time</span>, waitForCompletion: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = synchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> timesToCleanup = \ttimeToAllocatedBlocks.keys.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfilter &#123; _ &lt; cleanupThreshTime &#125;.toSeq&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (writeToLog(</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">BatchCleanupEvent</span>(timesToCleanup))) &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t <span class=\"comment\">// 清除已分配batch队列 </span></span><br><span class=\"line\"></span><br><span class=\"line\">\ttimeToAllocatedBlocks --= timesToCleanup</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 清除WAL</span></span><br><span class=\"line\"></span><br><span class=\"line\">\twriteAheadLogOption.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t_.clean(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcleanupThreshTime.milliseconds, \t\twaitForCompletion)</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> 脑图制作参考：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p>完整脑图链接地址：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkStreaming源码解析之容错","url":"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_faultTolerance/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_dataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis__faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps753E.tmp.jpg\" alt=\"img\"></p>\n<p>​    策略        优点            缺点</p>\n<p>(1) 热备        无 recover time        需要占用双倍资源</p>\n<p>(2) 冷备        十分可靠                存在 recover time</p>\n<p>(3) 重放        不占用额外资源        存在 recover time</p>\n<p>(4) 忽略        无 recover time        准确性有损失</p>\n<h1 id=\"1-driver端容错\"><a href=\"#1-driver端容错\" class=\"headerlink\" title=\"1. driver端容错\"></a>1. <strong>driver端容错</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D6.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"2-executor端容错\"><a href=\"#2-executor端容错\" class=\"headerlink\" title=\"2. executor端容错\"></a>2. <strong>executor端容错</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D7.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"2-1-热备\"><a href=\"#2-1-热备\" class=\"headerlink\" title=\"2.1. 热备\"></a>2.1. <strong>热备</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D8.tmp.jpg\" alt=\"img\"> </p>\n<p>Receiver 收到的数据，通过 ReceiverSupervisorImpl，将数据交给 BlockManager 存储；而 BlockManager 本身支持将数据 replicate() 到另外的 executor 上，这样就完成了 Receiver 源头数据的热备过程。</p>\n<p>而在计算时，计算任务首先将获取需要的块数据，这时如果一个 executor 失效导致一份数据丢失，那么计算任务将转而向另一个 executor 上的同一份数据获取数据。因为另一份块数据是现成的、不需要像冷备那样重新读取的，所以这里不会有 recovery time。</p>\n<h3 id=\"2-1-1-备份\"><a href=\"#2-1-1-备份\" class=\"headerlink\" title=\"2.1.1. 备份\"></a>2.1.1. <strong>备份</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D9.tmp.jpg\" alt=\"img\"> </p>\n<p>备份流程：</p>\n<p>​    先保存此block块，如果保存失败则不再进行备份，如果保存成功则获取保存的block块，执行复制操作。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockManager</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doPutIterator</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tdoPut(blockId,level,tellMaster)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 存储数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(level)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tmemoryStore.putIteratorAsBytes（）</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(level.useDisk)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tdiskStore.put()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 当前block已经存储成功则继续：\t\t</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(blockWasSuccessfullyStored)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 报告结果给master</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(tellMaster)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\treportBlockStatus(blockid,status)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 备份</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(level.replication&gt;<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"comment\">// 从上面保存成功的位置获取block</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t <span class=\"keyword\">val</span> bytesToReplicate =doGetLocalBytes(blockId, info)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"comment\">// 正式备份</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\treplicate(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tbytesToReplicate, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tlevel</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-1-2-恢复\"><a href=\"#2-1-2-恢复\" class=\"headerlink\" title=\"2.1.2. 恢复\"></a>2.1.2. <strong>恢复</strong></h3><p>计算任务首先将获取需要的块数据，这时如果一个 executor 失效导致一份数据丢失，那么计算任务将转而向另一个 executor 上的同一份数据获取数据。因为另一份块数据是现成的、不需要像冷备那样重新读取的，所以这里不会有 recovery time。</p>\n<h2 id=\"2-2-冷备\"><a href=\"#2-2-冷备\" class=\"headerlink\" title=\"2.2. 冷备\"></a>2.2. <strong>冷备</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DA.tmp.jpg\" alt=\"img\"> </p>\n<p>冷备是每次存储块数据时，除了存储到本 executor，还会把块数据作为 log 写出到 WriteAheadLog 里作为冷备。这样当 executor 失效时，就由另外的 executor 去读 WAL，再重做 log 来恢复块数据。WAL 通常写到可靠存储如 HDFS 上，所以恢复时可能需要一段 recover time</p>\n<h3 id=\"2-2-1-WriteAheadLog\"><a href=\"#2-2-1-WriteAheadLog\" class=\"headerlink\" title=\"2.2.1. WriteAheadLog\"></a>2.2.1. <strong>WriteAheadLog</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DB.tmp.jpg\" alt=\"img\"> </p>\n<p>WriteAheadLog 的特点是顺序写入，所以在做数据备份时效率较高，但在需要恢复数据时又需要顺序读取，所以需要一定 recovery time。</p>\n<p>不过对于 Spark Streaming 的块数据冷备来讲，在恢复时也非常方便。这是因为，对某个块数据的操作只有一次（即新增块数据），而没有后续对块数据的追加、修改、删除操作，这就使得在 WAL 里只会有一条此块数据的 log entry。所以，我们在恢复时只要 seek 到这条 log entry 并读取就可以了，而不需要顺序读取整个 WAL。</p>\n<p><strong>也就是，Spark Streaming 基于 WAL 冷备进行恢复，需要的 recovery time 只是 seek 到并读一条 log entry 的时间，而不是读取整个 WAL 的时间</strong>，这个是个非常大的节省</p>\n<h4 id=\"2-2-1-1-配置\"><a href=\"#2-2-1-1-配置\" class=\"headerlink\" title=\"2.2.1.1. 配置\"></a>2.2.1.1. <strong>配置</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DC.tmp.jpg\" alt=\"img\"> </p>\n<h5 id=\"2-2-1-1-1-存放目录配置\"><a href=\"#2-2-1-1-1-存放目录配置\" class=\"headerlink\" title=\"2.2.1.1.1. 存放目录配置\"></a>2.2.1.1.1. <strong>存放目录配置</strong></h5><p>WAL 存放的目录：<code>{checkpointDir}/receivedData/{receiverId}</code></p>\n<p>{checkpointDir} ：在 <code>ssc.checkpoint(checkpointDir)</code>指定的​    </p>\n<p>{receiverId} ：是 Receiver 的 id</p>\n<p>文件名：不同的 rolling log 文件的命名规则是     <code>log-{startTime}-{stopTime}</code></p>\n<h5 id=\"2-2-1-1-2-rolling配置\"><a href=\"#2-2-1-1-2-rolling配置\" class=\"headerlink\" title=\"2.2.1.1.2. rolling配置\"></a>2.2.1.1.2. <strong>rolling配置</strong></h5><p>FileBasedWriteAheadLog 的实现把 log 写到一个文件里（一般是 HDFS 等可靠存储上的文件），然后每隔一段时间就关闭已有文件，产生一些新文件继续写，也就是 rolling 写的方式</p>\n<p>rolling 写的好处是单个文件不会太大，而且删除不用的旧数据特别方便</p>\n<p>这里 rolling 的间隔是由参数 <strong>spark.streaming.receiver.writeAheadLog.rollingIntervalSecs</strong>（默认 = 60 秒） 控制的</p>\n<h4 id=\"2-2-1-2-读写对象管理\"><a href=\"#2-2-1-2-读写对象管理\" class=\"headerlink\" title=\"2.2.1.2. 读写对象管理\"></a>2.2.1.2. <strong>读写对象管理</strong></h4><p>WAL将读写对象和读写实现分离，由FileBasedWriterAheadLog管理读写对象，LogWriter和LogReader根据不同输出源实现其读写操作</p>\n<p>class FileBasedWriteAheadLog:</p>\n<p>write(byteBuffer:ByteBuffer,time:Long):</p>\n<p>​    1.  先调用getCurrentWriter(),获取当前currentWriter.</p>\n<p>​    2. 如果log file 需要rolling成新的，则currentWriter也需要更新为新的currentWriter</p>\n<p>​    3. 调用writer.write(byteBuffer)进行写操作</p>\n<p>​    4. 保存成功后返回： </p>\n<p>​        path:保存路径</p>\n<p>​        offset:偏移量</p>\n<p>​        length:长度</p>\n<p>read(segment:WriteAheadRecordHandle):</p>\n<p>​    ByteBuffer {}:</p>\n<p>​    1. 直接调用reader.read(fileSegment)</p>\n<p>read实现：</p>\n<p>// 来自 FileBasedWriteAheadLogRandomReader</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\tsegment: <span class=\"type\">FileBasedWriteAheadLogSegment</span>): <span class=\"type\">ByteBuffer</span> = synchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  assertOpen()</span><br><span class=\"line\"></span><br><span class=\"line\">  \t<span class=\"comment\">// 【seek 到这条 log 所在的 offset】</span></span><br><span class=\"line\"></span><br><span class=\"line\"> \t instream.seek(segment.offset)</span><br><span class=\"line\"></span><br><span class=\"line\"> \t <span class=\"comment\">// 【读一下 length】</span></span><br><span class=\"line\"></span><br><span class=\"line\"> \t <span class=\"keyword\">val</span> nextLength = instream.readInt()</span><br><span class=\"line\"></span><br><span class=\"line\">  \t <span class=\"keyword\">val</span> buffer = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Byte</span>](nextLength)</span><br><span class=\"line\"></span><br><span class=\"line\">  \t <span class=\"comment\">// 【读一下具体的内容】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  \t instream.readFully(buffer)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 【以 ByteBuffer 的形式，返回具体的内容】</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">ByteBuffer</span>.wrap(buffer)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-3-重放\"><a href=\"#2-3-重放\" class=\"headerlink\" title=\"2.3. 重放\"></a>2.3. <strong>重放</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DD.tmp.jpg\" alt=\"img\"> </p>\n<p>如果上游支持重放，比如 Apache Kafka，那么就可以选择不用热备或者冷备来另外存储数据了，而是在失效时换一个 executor 进行数据重放即可。</p>\n<h3 id=\"2-3-1-基于Receiver\"><a href=\"#2-3-1-基于Receiver\" class=\"headerlink\" title=\"2.3.1. 基于Receiver\"></a>2.3.1. <strong>基于Receiver</strong></h3><p><strong>偏移量又kafka负责，有可能导致重复消费</strong></p>\n<p>这种是将 Kafka Consumer 的偏移管理交给 Kafka —— 将存在 ZooKeeper 里，失效后由 Kafka 去基于 offset 进行重放</p>\n<p>这样可能的问题是，Kafka 将同一个 offset 的数据，重放给两个 batch 实例 —— 从而只能保证 at least once 的语义</p>\n<h3 id=\"2-3-2-Direct方式\"><a href=\"#2-3-2-Direct方式\" class=\"headerlink\" title=\"2.3.2. Direct方式\"></a>2.3.2. <strong>Direct方式</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DE.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>偏移量由spark自己管理，可以保证exactly-once</strong></p>\n<p>由 Spark Streaming 直接管理 offset —— 可以给定 offset 范围，直接去 Kafka 的硬盘上读数据，使用 Spark Streaming 自身的均衡来代替 Kafka 做的均衡</p>\n<p>这样可以保证，每个 offset 范围属于且只属于一个 batch，从而保证 exactly-once</p>\n<p>所以看 Direct 的方式，<strong>归根结底是由 Spark Streaming 框架来负责整个 offset 的侦测、batch 分配、实际读取数据</strong>；并且这些分 batch 的信息都是 checkpoint 到可靠存储（一般是 HDFS）了。这就没有用到 Kafka 使用 ZooKeeper 来均衡 consumer 和记录 offset 的功能，而是把 Kafka 直接当成一个底层的文件系统来使用了。</p>\n<h4 id=\"2-3-2-1-DirectKafkaInputDStream\"><a href=\"#2-3-2-1-DirectKafkaInputDStream\" class=\"headerlink\" title=\"2.3.2.1. DirectKafkaInputDStream\"></a>2.3.2.1. <strong>DirectKafkaInputDStream</strong></h4><p>负责侦测最新 offset，并将 offset 分配至唯一个 batch</p>\n<h4 id=\"2-3-2-2-KafkaRDD\"><a href=\"#2-3-2-2-KafkaRDD\" class=\"headerlink\" title=\"2.3.2.2. KafkaRDD\"></a>2.3.2.2. <strong>KafkaRDD</strong></h4><p>负责去读指定 offset 范围内的数据，并基于此数据进行计算</p>\n<h2 id=\"2-4-忽略\"><a href=\"#2-4-忽略\" class=\"headerlink\" title=\"2.4. 忽略\"></a>2.4. <strong>忽略</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9EF.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"2-4-1-粗粒度忽略\"><a href=\"#2-4-1-粗粒度忽略\" class=\"headerlink\" title=\"2.4.1. 粗粒度忽略\"></a>2.4.1. <strong>粗粒度忽略</strong></h3><p>在driver端捕获job抛出的异常，防止当前job失败，这样做会忽略掉整个batch里面的数据</p>\n<h3 id=\"2-4-2-细粒度忽略\"><a href=\"#2-4-2-细粒度忽略\" class=\"headerlink\" title=\"2.4.2. 细粒度忽略\"></a>2.4.2. <strong>细粒度忽略</strong></h3><p>细粒度忽略是在excutor端进行的，如果接收的block失效后，将失败的Block忽略掉，只发送没有问题的block块到driver</p>\n<p><strong>脑图制作参考</strong>：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p><strong>完整脑图链接地址</strong>：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkStreaming源码解析之Job动态生成","url":"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_dataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis__faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps1C70.tmp.jpg\" alt=\"img\"></p>\n<p>在 Spark Streaming 程序的入口，我们都会定义一个 batchDuration，就是需要每隔多长时间就比照静态的 DStreamGraph 来动态生成一个 RDD DAG 实例。在 Spark Streaming 里，总体负责动态作业调度的具体类是 JobScheduler。</p>\n<p> JobScheduler 有两个非常重要的成员：JobGenerator 和 ReceiverTracker。JobScheduler 将每个 batch 的 RDD DAG 具体生成工作委托给 JobGenerator，而将源头输入数据的记录工作委托给 ReceiverTracker。</p>\n<h1 id=\"1-启动\"><a href=\"#1-启动\" class=\"headerlink\" title=\"1. 启动\"></a>1. <strong>启动</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2045.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"1-1-JobScheduler\"><a href=\"#1-1-JobScheduler\" class=\"headerlink\" title=\"1.1. JobScheduler\"></a>1.1. <strong>JobScheduler</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2046.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>job运行的总指挥是JobScheduler.start()，</strong></p>\n<p> JobScheduler 有两个非常重要的成员：JobGenerator 和 ReceiverTracker。JobScheduler 将每个 batch 的 RDD DAG 具体生成工作委托给 JobGenerator，而将源头输入数据的记录工作委托给 ReceiverTracker。</p>\n<p><strong>在StreamingContext中启动scheduler</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamingContext</span>(<span class=\"params\">sc,cp,batchDur</span>)</span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> scheduler = <span class=\"keyword\">new</span> <span class=\"type\">JobScheduler</span>(<span class=\"keyword\">this</span>)</span><br><span class=\"line\">\tstart()&#123;</span><br><span class=\"line\">\t\tscheduler.start()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>在JobScheduler中启动recieverTracker和JobGenerator</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobScheduler</span>(<span class=\"params\">ssc</span>) </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> receiverTracker:<span class=\"type\">ReceiverTracker</span>=<span class=\"literal\">null</span></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> jobGenerator=<span class=\"keyword\">new</span> <span class=\"type\">JobGenerator</span>(<span class=\"keyword\">this</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobExecutor=<span class=\"type\">ThreadUtils</span>.newDaemonFixedThreadPool()</span><br><span class=\"line\">\t<span class=\"keyword\">if</span>(stared) <span class=\"keyword\">return</span> <span class=\"comment\">// 只启动一次</span></span><br><span class=\"line\">\treceiverTracker.start()</span><br><span class=\"line\">    jobGenerator.start()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-1-启动ReceiverTracker\"><a href=\"#1-1-1-启动ReceiverTracker\" class=\"headerlink\" title=\"1.1.1. 启动ReceiverTracker\"></a>1.1.1. <strong>启动ReceiverTracker</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2057.tmp.jpg\" alt=\"img\"> </p>\n<ol>\n<li><p>在JobScheduler的start中启动ReceiverTraker:<code>receiverTracker.start()：</code></p>\n</li>\n<li><p>RecieverTracker 调用launchReceivers方法</p>\n</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> endpoint:<span class=\"type\">RpcEndpointRef</span>=<span class=\"literal\">null</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>()=synchronized&#123;</span><br><span class=\"line\">\t\tendpoint=ssc.env.rpcEnv.setEndpoint(</span><br><span class=\"line\">\t\t\t<span class=\"string\">\"receiverTracker\"</span>,</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverTrackerEndpoint</span>() </span><br><span class=\"line\">\t\t)</span><br><span class=\"line\">\t\tlaunchReceivers()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-1-1-1-ReceiverSupervisor\"><a href=\"#1-1-1-1-ReceiverSupervisor\" class=\"headerlink\" title=\"1.1.1.1. ReceiverSupervisor\"></a>1.1.1.1. <strong>ReceiverSupervisor</strong></h4><p> ReceiverTracker将RDD DAG和启动receiver的Func包装成ReceiverSupervisor发送到最优的Excutor节点上</p>\n<h4 id=\"1-1-1-2-拉起receivers\"><a href=\"#1-1-1-2-拉起receivers\" class=\"headerlink\" title=\"1.1.1.2. 拉起receivers\"></a>1.1.1.2. <strong>拉起receivers</strong></h4><p> 从ReceiverInputDStreams中获取Receivers，并把他们发送到所有的worker nodes:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class  ReceiverTracker &#123;</span><br><span class=\"line\">\tvar endpoint:RpcEndpointRef=</span><br><span class=\"line\">\tprivate def launchReceivers()&#123;</span><br><span class=\"line\">\t\t// DStreamGraph的属性inputStreams</span><br><span class=\"line\">\t\tval receivers=inputStreams.map&#123;nis=&gt;</span><br><span class=\"line\">\t\t\tval rcvr=nis.getReceiver()</span><br><span class=\"line\">\t\t\t// rcvr是对kafka,socket等接受数据的定义</span><br><span class=\"line\">\t\t\trcvr</span><br><span class=\"line\">\t\t&#125;\t</span><br><span class=\"line\">\t\t// 发送到worker</span><br><span class=\"line\">\t\t endpoint.send(StartAllReceivers(receivers))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-2-启动DAG生成\"><a href=\"#1-1-2-启动DAG生成\" class=\"headerlink\" title=\"1.1.2. 启动DAG生成\"></a>1.1.2. <strong>启动DAG生成</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2058.tmp.jpg\" alt=\"img\"> </p>\n<p> 在JobScheduler的start中启动JobGenerator:<code>JobGenerator.start()</code></p>\n<h4 id=\"1-1-2-1-startFirstTime\"><a href=\"#1-1-2-1-startFirstTime\" class=\"headerlink\" title=\"1.1.2.1. startFirstTime\"></a>1.1.2.1. <strong>startFirstTime</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2059.tmp.jpg\" alt=\"img\"> </p>\n<p>  <strong>首次启动</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startFirstTime</span></span>() &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 定义定时器</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> startTime = </span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(timer.getStartTime())</span><br><span class=\"line\">\t<span class=\"comment\">// 启动DStreamGraph</span></span><br><span class=\"line\">    graph.start(startTime - graph.batchDuration)</span><br><span class=\"line\">    <span class=\"comment\">//  启动定时器</span></span><br><span class=\"line\">\t timer.start(startTime.milliseconds)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-1-2-1-1-启动DAG\"><a href=\"#1-1-2-1-1-启动DAG\" class=\"headerlink\" title=\"1.1.2.1.1. 启动DAG\"></a>1.1.2.1.1. <strong>启动DAG</strong></h5><p><strong>graph的生成是在StreamingContext中</strong>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> graph: <span class=\"type\">DStreamGraph</span>=&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 重启服务时</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span>（isCheckpointPresent）&#123;</span><br><span class=\"line\">\t\tcheckPoint.graph.setContext(<span class=\"keyword\">this</span>)</span><br><span class=\"line\">\t\tcheckPoint.graph.restoreCheckPointData()</span><br><span class=\"line\">\t\tcheckPoint.graph</span><br><span class=\"line\">\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 首次初始化时</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> newGraph=<span class=\"keyword\">new</span> <span class=\"type\">DStreamGraph</span>()</span><br><span class=\"line\">\t\tnewGraph.setBatchDuration(_batchDur)</span><br><span class=\"line\">\t\tnewGraph</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>在GenerateJobs中启动graph</strong>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">graph.start(nowTime-batchDuration)</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-1-2-1-2-启动timer\"><a href=\"#1-1-2-1-2-启动timer\" class=\"headerlink\" title=\"1.1.2.1.2. 启动timer\"></a>1.1.2.1.2. <strong>启动timer</strong></h5><p><strong>JobGenerator中定义了一个定时器：</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> timer=<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(colck,batchDuaraion,</span><br><span class=\"line\">\t\tlongTime=&gt;eventLoop.post(</span><br><span class=\"line\">            <span class=\"type\">GenerateJobs</span>(</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(longTime)</span><br><span class=\"line\">            )</span><br><span class=\"line\">         )</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p><strong>在JobGenerator启动时会开始执行这个调度器：</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">timer.start(startTime.milliseconds)</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-2-RecurringTimer：定时器\"><a href=\"#1-2-RecurringTimer：定时器\" class=\"headerlink\" title=\"1.2. RecurringTimer：定时器\"></a>1.2. <strong>RecurringTimer：定时器</strong></h2><p>// 来自 JobGenerator</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[streaming]</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobGenerator</span>(<span class=\"params\">jobScheduler: <span class=\"type\">JobScheduler</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> timer = <span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(clock, ssc.graph.batchDuration.milliseconds,</span><br><span class=\"line\">      longTime =&gt; eventLoop.post(<span class=\"type\">GenerateJobs</span>(<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(longTime))), <span class=\"string\">\"JobGenerator\"</span>)</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过代码也可以看到，整个 timer 的调度周期就是 batchDuration，每次调度起来就是做一个非常简单的工作：往 eventLoop 里发送一个消息 —— 该为当前 batch (new Time(longTime)) GenerateJobs 了！</p>\n<h1 id=\"2-生成\"><a href=\"#2-生成\" class=\"headerlink\" title=\"2. 生成\"></a>2. <strong>生成</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps205A.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>JobGenerator中定义了一个定时器，在定时器中启动生成job操作</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobGenerator</span></span>:</span><br><span class=\"line\"><span class=\"comment\">// 定义定时器</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> timer=</span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(colck,batchDuaraion,</span><br><span class=\"line\">\tlongTime=&gt;eventLoop.post(<span class=\"type\">GenerateJobs</span>(</span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(longTime))))</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJobs</span></span>(time: <span class=\"type\">Time</span>) &#123;</span><br><span class=\"line\">  <span class=\"type\">Try</span> &#123;</span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 1. 将已收到的数据进行一次 allocate</span></span><br><span class=\"line\">  receiverTracker.allocateBlocksToBatch(time)  </span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">//   2. 复制一份新的DAG实例</span></span><br><span class=\"line\">  graph.generateJobs(time)                                                 </span><br><span class=\"line\">   &#125; <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">Success</span>(jobs) =&gt;</span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 3. 获取 meta 信息</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> streamIdToInputInfos = jobScheduler.inputInfoTracker.getInfo(time)  </span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 4. 提交job     </span></span><br><span class=\"line\"> jobScheduler.submitJobSet(<span class=\"type\">JobSet</span>(time, jobs, streamIdToInputInfos))   </span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Failure</span>(e) =&gt;</span><br><span class=\"line\">      jobScheduler.reportError(<span class=\"string\">\"Error generating jobs for time \"</span> + time, e)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 5. checkpoint</span></span><br><span class=\"line\">  eventLoop.post(<span class=\"type\">DoCheckpoint</span>(time, clearCheckpointDataLater = <span class=\"literal\">false</span>))      </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-2-获取DAG实例\"><a href=\"#2-2-获取DAG实例\" class=\"headerlink\" title=\"2.2. 获取DAG实例\"></a>2.2. <strong>获取DAG实例</strong></h2><p>在生成Job并提交到excutor的第二步，</p>\n<p>JobGenerator-&gt;DStreamGraph-&gt;OutputStreams-&gt;ForEachDStream-&gt;TransformationDStream-&gt;InputDStream</p>\n<p>具体流程是：</p>\n<p>- 1. JobGenerator调用了DStreamGraph里面的gererateJobs(time)方法</p>\n<p>- 2. DStreamGraph里的generateJobs方法遍历了outputStreams</p>\n<p>- 3. OutputStreams调用了其generateJob(time)方法</p>\n<p>- 4. ForEachDStream实现了generateJob方法，调用了：</p>\n<p>​    parent.getOrCompute(time)</p>\n<p>递归的调用父类的getOrCompute方法去动态生成物理DAG图</p>\n<h1 id=\"3-运行\"><a href=\"#3-运行\" class=\"headerlink\" title=\"3. 运行\"></a>3. <strong>运行</strong></h1><h2 id=\"3-1-异步处理-JobScheduler\"><a href=\"#3-1-异步处理-JobScheduler\" class=\"headerlink\" title=\"3.1. 异步处理:JobScheduler\"></a>3.1. <strong>异步处理:JobScheduler</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2081.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>JobScheduler通过线程池执行从JobGenerator提交过来的Job，jobExecutor异步的去处理提交的job</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobScheduler</span></span>&#123;</span><br><span class=\"line\">  numConcurrentJobs = ssc.conf.getInt(<span class=\"string\">\"spark.streaming.concurrentJobs\"</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> jobExecutor =<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\">\tnewDaemonFixedThreadPool(numConcurrentJobs, <span class=\"string\">\"streaming-job-executor\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitJobSet</span></span>(jobSet: <span class=\"type\">JobSet</span>) &#123;</span><br><span class=\"line\">\t\tjobSet.jobs.foreach(job =&gt;</span><br><span class=\"line\">                            jobExecutor.execute(<span class=\"keyword\">new</span> <span class=\"type\">JobHandler</span>(job)))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-1-Job-类比Thread\"><a href=\"#3-1-1-Job-类比Thread\" class=\"headerlink\" title=\"3.1.1. Job:类比Thread\"></a>3.1.1. <strong>Job:类比Thread</strong></h3><h3 id=\"3-1-2-JobHandler：真正执行job\"><a href=\"#3-1-2-JobHandler：真正执行job\" class=\"headerlink\" title=\"3.1.2. JobHandler：真正执行job\"></a>3.1.2. <strong>JobHandler：真正执行job</strong></h3><p> JobHandler 除了做一些状态记录外，最主要的就是调用 job.run()，</p>\n<p> 在 ForEachDStream.generateJob(time) 时，是定义了 Job 的运行逻辑，即定义了 Job.func。而在 <strong>JobHandler 这里，是真正调用了 Job.run()、将触发 Job.func 的真正执行</strong>！</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 来自 JobHandler</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  <span class=\"comment\">// 【发布 JobStarted 消息】</span></span><br><span class=\"line\">  _eventLoop.post(<span class=\"type\">JobStarted</span>(job))</span><br><span class=\"line\">  <span class=\"type\">PairRDDFunctions</span>.disableOutputSpecValidation.withValue(<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 【主要逻辑，直接调用了 job.run()】</span></span><br><span class=\"line\">    job.run()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _eventLoop = eventLoop</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (_eventLoop != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">  <span class=\"comment\">// 【发布 JobCompleted 消息】</span></span><br><span class=\"line\">    _eventLoop.post(<span class=\"type\">JobCompleted</span>(job))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-3-concurrentJobs-job并行度\"><a href=\"#3-1-3-concurrentJobs-job并行度\" class=\"headerlink\" title=\"3.1.3. concurrentJobs : job并行度\"></a>3.1.3. <strong>concurrentJobs : job并行度</strong></h3><p><strong>spark.streaming.concurrentJobs job并行度</strong></p>\n<p>这里 jobExecutor 的线程池大小，是由 spark.streaming.concurrentJobs 参数来控制的，当没有显式设置时，其取值为 1。</p>\n<p>进一步说，这里 jobExecutor 的线程池大小，就是能够并行执行的 Job 数。而回想前文讲解的 DStreamGraph.generateJobs(time) 过程，一次 batch 产生一个 Seq[Job}，里面可能包含多个 Job —— 所以，确切的，有几个 output 操作，就调用几次 ForEachDStream.generatorJob(time)，就产生出几个 Job</p>\n<p><strong>脑图制作参考</strong>：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p><strong>完整脑图链接地址</strong>：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkStreaming源码解析之DAG定义","url":"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_dataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis__faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/DAG.jpg\" alt=\"img\"></p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82B.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"1-DStream\"><a href=\"#1-DStream\" class=\"headerlink\" title=\"1. DStream\"></a>1. <strong>DStream</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82C.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"1-1-RDD\"><a href=\"#1-1-RDD\" class=\"headerlink\" title=\"1.1. RDD\"></a>1.1. <strong>RDD</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA83D.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>DStream和RDD关系：</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">DStream is a continuous sequence of RDDs：</span><br><span class=\"line\">generatedRDDs=new HashMap[Time,RDD[T]]()</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-1-存储\"><a href=\"#1-1-1-存储\" class=\"headerlink\" title=\"1.1.1. 存储\"></a>1.1.1. <strong>存储</strong></h3><p> <strong>存储格式</strong></p>\n<p>DStream内部通过一个HashMap的变量generatedRDD来记录生成的RDD:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">private[streaming] var generatedRDDs = new HashMap[Time, RDD[T]] ()</span><br></pre></td></tr></table></figure>\n<p><em>其中 ：</em></p>\n<p>​    <em>- key: time是生成当前batch的时间戳</em></p>\n<p>​    <em>- value: 生成的RDD实例</em></p>\n<p> <strong>每一个不同的 DStream 实例，都有一个自己的 generatedRDD，即每个转换操作的结果都会保留</strong></p>\n<h3 id=\"1-1-2-获取\"><a href=\"#1-1-2-获取\" class=\"headerlink\" title=\"1.1.2. 获取\"></a>1.1.2. <strong>获取</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA83E.tmp.jpg\" alt=\"img\"> </p>\n<h4 id=\"1-1-2-1-getOrCompute\"><a href=\"#1-1-2-1-getOrCompute\" class=\"headerlink\" title=\"1.1.2.1. getOrCompute\"></a>1.1.2.1. <strong>getOrCompute</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA83F.tmp.jpg\" alt=\"img\"> </p>\n<ol>\n<li><p>从rdd的map中获取：generatedRDDs.get(time).orElse</p>\n</li>\n<li><p>map中没有则计算：val newRDD=compute(time)</p>\n</li>\n<li><p>将计算的newRDD放入map中：generatedRDDs.put(time, newRDD)</p>\n<p>其中compute方法有以下特点：</p>\n</li>\n</ol>\n<ul>\n<li><p>不同DStream的计算方式不同</p>\n</li>\n<li><p>inputStream会对接对应数据源的API</p>\n</li>\n<li><p>transformStream会从父依赖中去获取RDD并进行转换得新的DStream</p>\n</li>\n</ul>\n<p>compute方法实现：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverInputDStream</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(validTime: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">RDD</span>[<span class=\"type\">T</span>]] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockRDD = &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">      <span class=\"keyword\">if</span> (validTime &lt; graph.startTime) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// If this is called for any time before the start time of the context,</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// then this returns an empty RDD. This may happen when recovering from a</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// driver failure without any write ahead log to recover pre-failure data.</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"type\">BlockRDD</span>[<span class=\"type\">T</span>](ssc.sc, <span class=\"type\">Array</span>.empty)</span><br><span class=\"line\"></span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Otherwise, ask the tracker for all the blocks that have been allocated to this stream</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// for this batch</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">val</span> receiverTracker = ssc.scheduler.receiverTracker</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">val</span> blockInfos = receiverTracker.getBlocksOfBatch(validTime).getOrElse(id, <span class=\"type\">Seq</span>.empty)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// Register the input blocks information into InputInfoTracker</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">val</span> inputInfo = <span class=\"type\">StreamInputInfo</span>(id, blockInfos.flatMap(_.numRecords).sum)</span><br><span class=\"line\"></span><br><span class=\"line\">        ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// Create the BlockRDD</span></span><br><span class=\"line\"></span><br><span class=\"line\">        createBlockRDD(validTime, blockInfos)</span><br><span class=\"line\"></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">Some</span>(blockRDD)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-3-生成\"><a href=\"#1-1-3-生成\" class=\"headerlink\" title=\"1.1.3. 生成\"></a>1.1.3. <strong>生成</strong></h3><p>RDD主要分为以下三个过程：InputStream -&gt; TransFormationStream -&gt; OutputStream</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA864.tmp.jpg\" alt=\"img\"> </p>\n<h4 id=\"1-1-3-1-InputStream\"><a href=\"#1-1-3-1-InputStream\" class=\"headerlink\" title=\"1.1.3.1. InputStream\"></a>1.1.3.1. <strong>InputStream</strong></h4><p>inputstream包括FileInputStream，KafkaInputStream等等</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA865.tmp.jpg\" alt=\"img\"> </p>\n<h5 id=\"1-1-3-1-1-FileInputStream\"><a href=\"#1-1-3-1-1-FileInputStream\" class=\"headerlink\" title=\"1.1.3.1.1. FileInputStream\"></a>1.1.3.1.1. <strong>FileInputStream</strong></h5><p>FileInputStream的生成步骤：</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA866.tmp.jpg\" alt=\"img\"> </p>\n<ol>\n<li><p>找到新产生的文件：val newFiles = findNewFiles(validTime.milliseconds)</p>\n</li>\n<li><p>将newFiles转换为RDDs：val rdds=filesToRDD(newFiles)</p>\n<p>2.1.  遍历文件列表获取生成RDD: val fileRDDs=files.map(file=&gt;newAPIHadoop(file))</p>\n<p>2.2.  将每个文件的RDD进行合并并返回：return new UnionRDD(fileRDDs)</p>\n</li>\n<li><p>返回生成的rdds</p>\n</li>\n</ol>\n<h4 id=\"1-1-3-2-TransformationStream\"><a href=\"#1-1-3-2-TransformationStream\" class=\"headerlink\" title=\"1.1.3.2. TransformationStream\"></a>1.1.3.2. <strong>TransformationStream</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA87D.tmp.jpg\" alt=\"img\"> </p>\n<p>RDD的转换实现：</p>\n<ol>\n<li>获取parent DStream：val parentDs=parent.getOrCompute(validTime)</li>\n<li>执行转换函数并返回转换结果：return parentDs.map(mapFunc)</li>\n</ol>\n<p><strong>转换类的DStream实现特点</strong>：</p>\n<ul>\n<li><p>传入parent DStream和转换函数</p>\n</li>\n<li><p>compute方法中从parent DStream中获取DStream并对其作用转换函数</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[streaming]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MappedDStream</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>] (<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    parent: <span class=\"type\">DStream</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    mapFunc: <span class=\"type\">T</span> =&gt; <span class=\"type\">U</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">DStream</span>[<span class=\"type\">U</span>](<span class=\"params\">parent.ssc</span>) </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dependencies</span></span>: <span class=\"type\">List</span>[<span class=\"type\">DStream</span>[_]] = <span class=\"type\">List</span>(parent)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">slideDuration</span></span>: <span class=\"type\">Duration</span> = parent.slideDuration</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(validTime: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">RDD</span>[<span class=\"type\">U</span>]] = &#123;</span><br><span class=\"line\">    parent.getOrCompute(validTime).map(_.map[<span class=\"type\">U</span>](mapFunc))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>不同DStream的getOrCompute方法实现：</p>\n<ul>\n<li>FilteredDStream：<code>parent.getOrCompute(validTime).map(_.filter(filterFunc)</code></li>\n<li>FlatMapValuedDStream:<code>parent.getOrCompute(validTime).map(_.flatMapValues[U](flatMapValueFunc)</code></li>\n<li>MappedDStream:<code>parent.getOrCompute(validTime).map(_.map[U](mapFunc))</code></li>\n</ul>\n<p>在最开始， <strong>DStream 的 transformation 的 API 设计与 RDD 的 transformation 设计保持了一致，就使得，每一个 dStreamA.transformation() 得到的新 dStreamB 能将 dStreamA.transformation() 操作完美复制为每个 batch 的 rddA.transformation() 操作</strong>。这也就是 DStream 能够作为 RDD 模板，在每个 batch 里实例化 RDD 的根本原因。</p>\n<h4 id=\"1-1-3-3-OutputDStream\"><a href=\"#1-1-3-3-OutputDStream\" class=\"headerlink\" title=\"1.1.3.3. OutputDStream\"></a>1.1.3.3. <strong>OutputDStream</strong></h4><p>OutputDStream的操作最后都转换到ForEachDStream(),ForeachDStream中会生成Job并返回。</p>\n<p> <strong>伪代码</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJob</span></span>(time:<span class=\"type\">Time</span>)&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobFunc=()=&gt;crateRDD&#123;</span><br><span class=\"line\">\t\tforeachFunc(rdd,time)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"type\">Some</span>(<span class=\"keyword\">new</span> <span class=\"type\">Job</span>(time,jobFunc))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>源码</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[streaming]</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ForEachDStream</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>] (<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    parent: <span class=\"type\">DStream</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    foreachFunc: (<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], <span class=\"type\">Time</span></span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Unit</span></span></span><br><span class=\"line\"><span class=\"class\">  ) <span class=\"keyword\">extends</span> <span class=\"title\">DStream</span>[<span class=\"type\">Unit</span>](<span class=\"params\">parent.ssc</span>) </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dependencies</span></span>: <span class=\"type\">List</span>[<span class=\"type\">DStream</span>[_]] = <span class=\"type\">List</span>(parent)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">slideDuration</span></span>: <span class=\"type\">Duration</span> = parent.slideDuration</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(validTime: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">RDD</span>[<span class=\"type\">Unit</span>]] = <span class=\"type\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJob</span></span>(time: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">Job</span>] = &#123;</span><br><span class=\"line\">    parent.getOrCompute(time) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(rdd) =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> jobFunc = () =&gt; createRDDWithLocalProperties(time) &#123;</span><br><span class=\"line\">          ssc.sparkContext.setCallSite(creationSite)</span><br><span class=\"line\">          foreachFunc(rdd, time)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"type\">Some</span>(<span class=\"keyword\">new</span> <span class=\"type\">Job</span>(time, jobFunc))</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; <span class=\"type\">None</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过对output stream节点进行遍历，就可以得到所有上游依赖的DStream,直至找到没有父依赖的inputStream。</p>\n<h2 id=\"1-2-特征\"><a href=\"#1-2-特征\" class=\"headerlink\" title=\"1.2. 特征\"></a>1.2. <strong>特征</strong></h2><p> DStream基本属性:</p>\n<ul>\n<li><p>父依赖： dependencies: List[DStream[_]]</p>\n</li>\n<li><p>时间间隔：slideDuration:Duration</p>\n</li>\n<li><p>生成RDD的函数：compute</p>\n</li>\n</ul>\n<h2 id=\"1-3-实现类\"><a href=\"#1-3-实现类\" class=\"headerlink\" title=\"1.3. 实现类\"></a>1.3. <strong>实现类</strong></h2><p>DStream的实现类可分为三种：输入，转换和输出</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA880.tmp.jpg\" alt=\"img\"> </p>\n<p>DStream之间的转换类似于RDD之间的转换，对于wordCount的例子，实现代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> lines=ssc.socketTextStream(ip,port)</span><br><span class=\"line\"><span class=\"keyword\">val</span> worlds=lines.flatMap(_.split(<span class=\"string\">\"_\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> pairs=words.map(word=&gt;(word,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> wordCounts=pairs.reduceByKey(_+_)</span><br><span class=\"line\">wordCounts.print()</span><br></pre></td></tr></table></figure>\n<p>每个函数的返回对象用具体实现代替：</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> lines=<span class=\"keyword\">new</span> <span class=\"type\">SocketInputDStream</span>(ip,port)</span><br><span class=\"line\"><span class=\"keyword\">val</span> words=<span class=\"keyword\">new</span> <span class=\"type\">FlatMappedDStream</span>(lines,_.split(<span class=\"string\">\"_\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> pairs=<span class=\"keyword\">new</span> <span class=\"type\">MappedDStream</span>(words,word=&gt;(word,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> wordCounts=<span class=\"keyword\">new</span> <span class=\"type\">ShuffledDStream</span>(pairs,_+_)</span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">ForeachDStream</span>(wordCounts,cnt=&gt;cnt.print())</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-1-ForeachDStream\"><a href=\"#1-3-1-ForeachDStream\" class=\"headerlink\" title=\"1.3.1. ForeachDStream\"></a>1.3.1. <strong>ForeachDStream</strong></h3><p> DStream的实现分为两种，transformation和output</p>\n<p>不同的转换操作有其对应的DStream实现，所有的output操作只对应于ForeachDStream</p>\n<h3 id=\"1-3-2-Transformed-DStream\"><a href=\"#1-3-2-Transformed-DStream\" class=\"headerlink\" title=\"1.3.2. Transformed DStream\"></a>1.3.2. <strong>Transformed DStream</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA890.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"1-3-3-InputDStream\"><a href=\"#1-3-3-InputDStream\" class=\"headerlink\" title=\"1.3.3. InputDStream\"></a>1.3.3. <strong>InputDStream</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA891.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"2-DStreamGraph\"><a href=\"#2-DStreamGraph\" class=\"headerlink\" title=\"2. DStreamGraph\"></a>2. <strong>DStreamGraph</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA892.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"2-1-DAG分类\"><a href=\"#2-1-DAG分类\" class=\"headerlink\" title=\"2.1 DAG分类\"></a>2.1 <strong>DAG分类</strong></h2><ul>\n<li><p>逻辑DAG: 通过transformation操作正向生成</p>\n</li>\n<li><p>物理DAG: 惰性求值的原因，在遇到output操作时根据dependency逆向宽度优先遍历求值。</p>\n</li>\n</ul>\n<h2 id=\"2-2-DAG生成\"><a href=\"#2-2-DAG生成\" class=\"headerlink\" title=\"2.2 DAG生成\"></a>2.2 DAG生成</h2><p> <strong>DStreamGraph属性</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">inputStreams=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">InputDStream</span>[_]]()</span><br><span class=\"line\">outputStreams=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">DStream</span>[_]]()</span><br></pre></td></tr></table></figure>\n<p><strong>DAG实现过程</strong></p>\n<p>​    通过对output stream节点进行遍历，就可以得到所有上游依赖的DStream,直至找到没有父依赖的inputStream。</p>\n<p>​    sparkStreaming 记录整个DStream DAG的方式就是通过一个DStreamGraph 实例记录了到所有output stream节点的引用</p>\n<p><strong>generateJobs</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJobs</span></span>(time: <span class=\"type\">Time</span>): <span class=\"type\">Seq</span>[<span class=\"type\">Job</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> jobs = <span class=\"keyword\">this</span>.synchronized &#123;</span><br><span class=\"line\">      outputStreams.flatMap &#123; </span><br><span class=\"line\">\t\toutputStream =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> jobOption = </span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 调用了foreachDStream来生成每个job</span></span><br><span class=\"line\">\t\t\toutputStream.generateJob(time)   jobOption.foreach(_.setCallSite(outputStream.creationSite))</span><br><span class=\"line\">        jobOption</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t<span class=\"comment\">// 返回生成的Job列表</span></span><br><span class=\"line\">    jobs</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p> <strong>脑图制作参考</strong>：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p><strong>完整脑图链接地址</strong>：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"微服务架构中的数据一致性","url":"https://sustcoder.github.io/2018/11/14/Data consistency  in microservices/","content":"<p>Data <strong>consistency</strong><sub>一致性</sub> in <strong>microservices</strong><sub>微服务</sub><strong>architecture</strong><sub>架构</sub></p>\n<p>In microservices, one logically <strong>atomic operation</strong><sub>原子操作</sub> can frequently <strong>span</strong> <sub>跨越</sub>multiple microservices. Even a <strong>monolithic</strong><sub>单片的</sub> system might use multiple databases or messaging solutions. With several independent <strong>data storage solutions</strong><sub>数据存储方案</sub>, we risk inconsistent data if one of the distributed process participants fails — such as <strong>charging</strong><sub>收费</sub> a customer without placing the order or not <strong>notifying</strong> <sub>通知</sub>the customer that the order succeeded. In this article, I’d like to share some of the techniques I’ve learned for making data between microservices <strong>eventually consistent</strong><sub>最终一致性</sub></p>\n<p>在微服务中，逻辑上的原子操作经常跨越多个微服务。即使是单体应用架构下，也可能使用多个数据库或消息队列解决方案。使用几种独立的数据存储解决方案，如果分布式流程参与者之一失败，我们将面临数据不一致的风险，例如，向客户收费而不下订单，或者不通知客户订单成功。在本文中，我想分享一些在微服务体系架构下确保数据最终一致的技术。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/1.jpg\" alt=\"Distributed process failures\"></p>\n<p>Why is it so <strong>challenging</strong> <sub>挑战性</sub>to achieve this? <strong>As long as</strong> <sub>只要</sub>we have multiple places where the data is stored (which are not in a single database), consistency is not solved <strong>automatically</strong><sub>自动的</sub> and engineers need to take care of consistency while designing the system. For now, in my opinion, the <strong>industry</strong> <sub>行业</sub>doesn’t yet have a widely known solution for updating data atomically in multiple different data sources — and we probably shouldn’t wait for one to be available soon.</p>\n<p>为什么要做到这一点如此具有挑战性？只要我们有多个存储数据的地方(而不是在一个数据库中)，一致性就不会自动解决，工程师在设计系统时就需要考虑一致性。就目前而言，在我看来，业界还没有一个广为人知的解决方案来原子化地更新多个不同数据源中的数据，而且在可预见的将来，也不会有一个很快就可以使用的解决方案。</p>\n<p>One attempt to solve this problem in an automated and <strong>hassle-free</strong> <sub>无忧</sub><strong>manner</strong> <sub>方式</sub>is the XA <strong>protocol</strong> <sub>协议</sub><strong>implementing</strong><sub>实现</sub> the two-phase commit (<strong>2PC</strong><sub>两阶段提交</sub>) pattern. But in <strong>modern</strong><sub>现代的</sub> <strong>high-scale applications</strong><sub>高可用服务</sub> (especially in <strong>a cloud environment</strong><sub>云环境</sub>), 2PC doesn’t seem to perform so well. To <strong>eliminate</strong><sub>消除</sub> the disadvantages of 2PC, we have to <strong>trade</strong><sub>交易，交换</sub> ACID for BASE and <strong>cover</strong><sub>覆盖</sub> consistency concerns ourselves in different ways depending on the requirements.</p>\n<p>以自动化和省力的方式解决这个问题的一个尝试是以<a href=\"http://t.cn/EzBmvFS\" target=\"_blank\" rel=\"noopener\">XA协议</a>实现<a href=\"http://t.cn/RckficO\" target=\"_blank\" rel=\"noopener\">两阶段提交(2PC)模式</a>但是在现代大规模应用程序中(特别是在云环境中)，2PC的性能似乎不太好。为了消除2PC的缺点，我们必须牺牲ACID来遵循BASE原则，并根据需要采用不同的方式来满足数据一致性的要求</p>\n<h2 id=\"SAGA模式\"><a href=\"#SAGA模式\" class=\"headerlink\" title=\"SAGA模式\"></a>SAGA模式</h2><p>The most well-known way of <strong>handling</strong><sub>处理</sub> consistency <strong>concerns</strong><sub>忧虑，问题</sub> in multiple microservices is the Saga Pattern. You may <strong>treat</strong> <sub>看待</sub>Sagas as application-level distributed <strong>coordination</strong> <sub>协调</sub>of multiple transactions. Depending on the use-case and requirements, you <strong>optimize</strong> <sub>优化</sub>your own Saga <strong>implementation</strong>.<sub>实现</sub> <strong>In contrast</strong><sub>相反</sub>, the XA <strong>protocol</strong><sub>协议</sub> tries to cover all the <strong>scenarios</strong><sub>场景</sub>. The Saga Pattern is also not new. It was known and used in ESB and SOA architectures in the past. Finally, it successfully transitioned to the microservices world. Each <strong>atomic business operation</strong> <sub>原子操作</sub>that spans multiple services might <strong>consist of</strong><sub>由…组成</sub> multiple transactions on a technical level. The key idea of the Saga Pattern is to be able to roll back one of the individual transactions. As we know, rollback is not possible for already committed individual transactions out of the box. But this is achieved by invoking a <strong>compensation action</strong><sub>补偿措施</sub> — by introducing a “Cancel” operation.</p>\n<p>在多个微服务中处理一致性问题的最著名方法是<a href=\"http://t.cn/EzB3uQA\" target=\"_blank\" rel=\"noopener\">SAGA模式</a>可以将SAGA视为多个事务的应用程序级分布式协调机制。根据用例和需求，可以优化自己的SAGA实现，XA协议正相反，试图以通用方案涵盖所有的场景。SAGA模式也并非什么新概念。它过去在ESB和SOA体系结构中就得到认知和使用，并最终成功地向微服务世界过渡。跨多个服务的每个原子业务操作可能由一个技术级别上的多个事务组成。Saga模式的关键思想是能够回滚单个事务。正如我们所知道的，对于已经提交的单个事务来说，回滚是不可能的。但通过调用补偿行动，即通过引入“Cancel”操作可以变相的实现这一点。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/2.jpg\" alt=\"Compensating operations\"></p>\n<p>In addition to <strong>cancelation</strong><sub>取消</sub>, you should consider making your service <strong>idempotent</strong><sub>幂等</sub>, so you can retry or restart certain operations in case of failures. Failures should be monitored and reaction to failures should be <strong>proactive</strong><sub>主动的</sub>.</p>\n<p>除了取消操作之外，还需要考虑使服务的幂等性，以便在发生故障时可以重新尝试或重新启动某些操作。应该对失败进行监测，对失败的反应应该积极主动。</p>\n<h3 id=\"Reconciliation-对账\"><a href=\"#Reconciliation-对账\" class=\"headerlink\" title=\"Reconciliation 对账\"></a>Reconciliation 对账</h3><p><strong>What if</strong><sub>如果</sub> in the middle of the process the system responsible for calling a compensation action crashes or restarts. In this case, the user may receive an error message and the compensation logic should be triggered or — when processing <strong>asynchronous</strong> <sub>异步</sub>user requests, the <strong>execution logic</strong><sub>执行逻辑</sub> should be <strong>resumed</strong><sub>恢复，重新开始</sub>.</p>\n<p>如果在进程中间，负责调用补偿操作的系统崩溃或重新启动怎么办？在这种情况下，用户可能会收到错误消息，触发补偿逻辑，在处理异步用户请求时，重试执行逻辑。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/3.jpg\" alt=\"main process failure\"></p>\n<p>To find <strong>crashed</strong><sub>崩溃</sub> transactions and resume operation or apply compensation, we need to <strong>reconcile</strong> <sub>协调</sub>data from multiple services. Reconciliation is a technique familiar to engineers who have worked in the financial domain. Did you ever wonder how banks make sure your money transfer didn’t get lost or how money transfer happens between two different banks in general? The quick answer is reconciliation.</p>\n<p>要查找崩溃的事务并恢复操作或应用补偿，我们需要协调来自多个服务的数据。对从事金融领域工作的工程师来说，对账是一种熟悉的技术。你有没有想过，银行如何确保你的汇款不会丢失，或者在两家不同的银行之间是如何发生转账的？快速的答案是对账。</p>\n<p>In <strong>accounting</strong><sub>会计</sub>, reconciliation is the process of ensuring that two sets of records (usually the <strong>balances</strong><sub>余额</sub> of two accounts) are in <strong>agreement</strong><sub>一致</sub>. Reconciliation is used to ensure that the money leaving an account matches the actual money spent. This is done by making sure the balances match at the end of a particular accounting period </p>\n<p>在会计领域，对账是确保两套记录(通常是两个账户的余额)一致的过程。对账手段确保离开帐户的钱与实际花费的钱相符。这是通过确保在特定会计期间结束时的余额匹配来实现的。</p>\n<p>Coming back to microservices, using the same principle we can reconcile data from multiple services on some action trigger. Actions could be triggered on a scheduled basis or by a monitoring system when failure is <strong>detected</strong><sub>检测到</sub>. The simplest <strong>approach</strong><sub>途径方法</sub> is to run a <strong>record-by-record comparison</strong><sub>逐条比较</sub>. This process could be optimized by comparing aggregated values. In this case, one of the systems will be a <em>source of truth</em> for each record.</p>\n<p>回到微服务方面，使用相同的理念，我们可以在某些操作触发器上协调来自多个服务的数据。可以按计划执行对账操作，也可以在检测到出状况时，由监视系统触发相关操作。最简单的方法是按记录逐条进行比较，当然，也可以通过比较汇总值来优化此过程。在这种情况下，某个系统的数据将成为基准数据来对每条数据进行比对。</p>\n<h2 id=\"Event-log-日志事件\"><a href=\"#Event-log-日志事件\" class=\"headerlink\" title=\"Event log 日志事件\"></a>Event log 日志事件</h2><p><strong>Imagine</strong><sub>设想</sub> <strong>multistep transactions</strong><sub>多重事物</sub>. How to determine during reconciliation which transactions might have failed and which steps have failed? One solution is to check the status of each transaction. In some cases, this functionality is not <strong>available</strong><sub>可达的，可用的</sub> (imagine a stateless mail service that sends email or produces other kinds of messages). In some other <strong>cases</strong><sub>场景案例</sub>, you might want to get immediate visibility on the transaction state, especially in <strong>complex scenarios</strong><sub>复杂场景</sub> with many steps. For example, a multistep order with booking flights, hotels, and transfers.</p>\n<p>再来讨论多步事务的情况。如何确定在对账过程中哪些事务在哪些环节上失败了？一种解决方案是检查每个事务的状态。在某些情况下，这个方法并不适用（比如无状态的邮件服务发送电子邮件或生成其他类型的消息）。在其他一些情况下，我们可能希望获得事务状态的即时可见性（也就是实时知晓其当前的状态），特别是在具有多个步骤的复杂场景中。例如，一个多步骤的订单，包括预订航班、酒店和转乘。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/5.jpg\" alt=\"main process failure\"></p>\n<p>In these situations, an event log can help. Logging is a simple but powerful technique. Many distributed systems rely on logs. “Write-ahead logging” is how databases achieve transactional behavior or maintain consistency between replicas <strong>internally</strong><sub>内部的</sub>. The same technique could be applied to microservices design. Before making an actual data change, the service writes a log entry about its <strong>intent</strong><sub>意图</sub> to make a change. In practice, the event log could be a table or a collection inside a database owned by the <strong>coordinating</strong><sub>协调</sub> service.</p>\n<p>在这种情况下，事件日志可能会有所帮助。日志记录是一种简单但功能强大的技术。许多分布式系统依赖日志。“预写日志“就是在数据库内部实现事务行为或保持副本之间的一致性的方法。同样的技术也可以应用于微服务设计。在进行实际的数据更改之前，服务会编写一个日志条目，对即将实施的更改操作进行说明。实现方式上，事件日志是从属于协调服务的数据库中的表或集合。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/6.jpg\" alt=\"sample event log\"></p>\n<p>The event log could be used not only to resume transaction processing but also to provide visibility to system users, customers, or to the support team. However, in simple scenarios a service log might be <strong>redundant</strong><sub>多余的</sub> and status <strong>endpoints</strong> <sub>状态节点</sub>or status fields be enough.</p>\n<p>事件日志不仅可用于恢复事务处理，还可用于向系统用户、客户或支持团队提供可见性。但是，在简单的场景中，服务日志可能是多余的，状态端点或状态字段就足够了。</p>\n<h3 id=\"Orchestration-vs-choreography-编曲和编舞\"><a href=\"#Orchestration-vs-choreography-编曲和编舞\" class=\"headerlink\" title=\"Orchestration vs. choreography 编曲和编舞\"></a>Orchestration vs. choreography 编曲和编舞</h3><p>By this point, you might think sagas are only a part of orchestration scenarios. But sagas can be used in choreography as well, where each microservice knows only a part of the process. Sagas include the knowledge on handling both <strong>positive</strong><sub>正向积极</sub> and <strong>negative</strong><sub>负向，否定</sub> flows of distributed transaction. In choreography, each of the distributed transaction participants has this kind of knowledge.</p>\n<p>至此，您可能会认为SAGA只适用于编曲场景的一部分。但是SAGA也可以用于编舞场景，每个微服务只知道其中的一部分。SAGA内置了处理分布式事务的正向流和负向流的相关机制。在编舞场景中，每个分布式事务参与者都有这样的知识。</p>\n<h3 id=\"Single-write-with-events\"><a href=\"#Single-write-with-events\" class=\"headerlink\" title=\"Single-write with events\"></a>Single-write with events</h3><p>单一写入事件</p>\n<p>The consistency solutions described <strong>so far</strong><sub>到目前为止</sub> are not easy. They are <strong>indeed</strong><sub>确实</sub> complex. But there is a simpler way: modifying a single datasource at a time.Instead of changing the state of the service and <strong>emitting</strong><sub>触发</sub> the event in one process, we could separate those two steps.</p>\n<p>到目前为止，上述的一致性解决方案并不容易。它们确实很复杂。但有一种更简单的方法：每次只修改一个数据源。我们可以将更改服务的状态并发出事件这两个步骤分开，而不是在一个进程中处理。</p>\n<p><strong>Change-first “变更优先”原则</strong></p>\n<p>In a main business operation, we modify our own state of the service while a separate process <strong>reliably</strong> <sub>可靠地</sub>captures the change and <strong>produces</strong> <sub>生成</sub>the event. This technique is known as <em>Change Data Capture (CDC)</em>. Some of the technologies implementing this approach are Kafka Connect or Debezium.</p>\n<p>在主要业务操作中，我们可以修改自己服务的状态，同时由一个单独的进程捕获相关变更并生成事件。这种技术被称为变更数据捕获(CDC)。实现此方法的一些技术包括<a href=\"http://t.cn/EzrZOpE\" target=\"_blank\" rel=\"noopener\"> Kafka Connect</a>或 <a href=\"https://debezium.io/\" target=\"_blank\" rel=\"noopener\">Debezium</a>。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/7.jpg\" alt=\"Change Data Capture with Debezium and Kafka Connect\"></p>\n<p>However, sometimes no specific framework is required. Some databases offer a friendly way to <strong>tail</strong> <sub>跟踪</sub>their operations log, such as MongoDB Oplog. If there is no such functionality in the database, changes can be polled by timestamp or queried with the last processed ID for immutable records. The key to avoiding inconsistency is making the data change notification a separate process. The database record is in this case the <em>single source of truth</em>. A change is only captured if it happened in the first place.</p>\n<p>然而，有时不需要特定的框架来进行处理。一些数据库提供了一种跟踪操作日志的友好方法，如<a href=\"http://t.cn/Ezrw6xj\" target=\"_blank\" rel=\"noopener\">MongoDB Oplog</a>。如果数据库中没有这样的功能，则可以使用时间戳轮询更改，或者使用最后处理的ID查询不可变记录。避免不一致的关键是使数据更改通知成为一个单独的进程。数据库记录在这种情况下为基准数据。一旦发生数据变更，相关数据即被捕获和记录。</p>\n<p>  <img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/8.jpg\" alt=\"Change Data Capture without specific tools\"></p>\n<p>The biggest <strong>drawback</strong><sub>退税，缺陷</sub> of change data capture is the separation of business logic. Change capture <strong>procedures</strong><sub>程序</sub> will most likely live in your <strong>codebase</strong><sub>代码库</sub> separate from the change logic itself — which is <strong>inconvenient</strong><sub>不方便</sub>. The most well-known application of change data capture is domain-agnostic change replication such as sharing data with a <strong>data warehouse</strong><sub>数仓</sub>. For domain events, it’s better to employ a different <strong>mechanism</strong><sub>机制</sub> such as sending events <strong>explicitly</strong><sub>显式地，明确的</sub>.</p>\n<p>变更数据捕获的最大缺点是业务逻辑的分离。更改捕获过程很可能存在于您的代码库中，与更改逻辑本身分离，这是不方便的（所谓的不方便，我的理解，不是指更改捕获过程与业务逻辑的分离，而是指用户需要为每个业务逻辑单独的实现更改捕获逻辑）。最广为人知的更改数据捕获应用场景是领域无关的更改复制，例如通过数据仓库进行数据共享。对于域内的事件，最好使用不同的机制，比如显式地发送事件。</p>\n<h3 id=\"Event-first-“事件优先”原则\"><a href=\"#Event-first-“事件优先”原则\" class=\"headerlink\" title=\"Event-first “事件优先”原则\"></a>Event-first “事件优先”原则</h3><p>Let’s look at <strong>the <em>single source of truth</em><sub>唯一来源，基础数据</sub></strong> <strong>upside down</strong><sub>逆向</sub>. What if <strong>instead of</strong><sub>而不是</sub> writing to the database first we trigger an event instead and share it with ourselves and with other services. In this case, the event <strong>becomes</strong><sub>成为</sub> the single source of truth. This would be a form of event-sourcing where the state of our own service effectively becomes a read model and each event is a write model.</p>\n<p>让我们对“基准数据”做一个逆向思考。如果我们不是首先写入数据库，而是先触发一个事件并与我们自己和其他服务共享这个事件呢？在这种情况下，事件成为基准数据。这将是一种 event-sourcing的形态，在这种情况下，服务状态实际上变成了一个读模型，而每个事件都是一个写模型。</p>\n<p>  <img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/9.jpg\" alt=\"Event-first approach\"></p>\n<p>On the one hand, it’s a <strong>command query responsibility segregation</strong><sub>查询责任分离</sub> (CQRS) <strong>pattern</strong><sub>模式</sub> where we separate the read and write models, but CQRS by itself doesn’t focus on the most important part of the solution — consuming the events with multiple services.</p>\n<p>所以，这也是一种命令查询责任分离(CQRS)模式，将读写模型分离开来，但是CQRS本身并没有关注解决方案中最重要的部分，即如何由多个服务来对事件进行处理。</p>\n<p>In contrast, event-driven architectures focus on events consumed by multiple systems but don’t <strong>emphasize</strong> <sub>强调</sub>the fact that events are the only atomic pieces of data update. So I’d like to introduce “e<em>vent-first”</em> as a name to this approach: updating the internal state of the microservice by emitting a single event — both to our own service and any other interested microservices.</p>\n<p>相反，事件驱动体系结构关注多个系统对事件的处理，但不突出强调事件是数据更新的基准数据。所以我想引入 “事件优先”原则作为此方法的名称：通过发出单个事件来更新微服务的内部状态-包括对我们自己的服务和任何其他感兴趣的微服务。</p>\n<p>The challenges with an “event-first” approach are also the challenges of CQRS itself. Imagine that before making an order we want to check item availability. What if two instances concurrently receive an order of the same item? Both will concurrently check the inventory in a read model and <strong>emit</strong><sub>发出，触发</sub> an order event. Without some sort of <strong>covering</strong> <sub>覆盖</sub>scenario we could run into troubles.</p>\n<p>采用“事件优先”方法的挑战也是CQRS本身的挑战。想象一下，在下订单之前，我们要检查商品的可用性。如果两个实例同时接收同一项的订单怎么办？两个实例将以读取模型同时检查库存，并触发一个订单事件。如果不解决这个问题，我们可能会遇到麻烦。</p>\n<p>The usual way to handle these cases is <strong>optimistic</strong><sub>乐观</sub> <strong>concurrency</strong><sub>并发</sub>: to <strong>place</strong><sub>放置</sub> a read model version into the event and ignore it on the consumer side if the read model was already updated on consumer side. The other solution would be using <strong>pessimistic</strong><sub>悲观</sub> concurrency control, such as creating a lock for an item while we check its availability.</p>\n<p>处理这些情况的通常方法是乐观并发：在事件中放置一个读取模型版本，如果已在使用者端更新读取模型，则忽略这个读取操作。另一种解决方案是使用悲观的并发控制，例如在查询项目可用性时为其创建锁。</p>\n<p>The other challenge of the “event-first” approach is a challenge of any event-driven architecture — <strong>the order of events</strong><sub>时间顺序</sub>. Processing events in the wrong order by multiple concurrent consumers might give us another kind of consistency issue, for example processing an order of a customer who hasn’t been created yet.</p>\n<p>“事件优先”方法的另一个挑战是对任何事件驱动的体系结构的挑战，即事件的顺序。多个并发消费者以错误的顺序处理事件可能会给我们带来另一种一致性问题，例如，处理尚未创建的客户的订单。</p>\n<p>Data streaming solutions such as Kafka or AWS Kinesis can <strong>guarantee</strong><sub>保证</sub> that events related to a single entity will be processed sequentially (such as creating an order for a customer only after the user is created). In Kafka for example, you can partition topics by user ID so that all events related to a single user will be processed by a single consumer assigned to the partition, thus allowing them to be processed sequentially. In contrast, in Message Brokers, message queues have an order but multiple concurrent consumers make message processing in a given order hard, <strong>if not impossible</strong><sub>甚至不可能</sub>. In this case, your could run into concurrency issues.</p>\n<p>数据流解决方案(如Kafka或AWS Kinesis)可以保证与单个实体相关的事件将按顺序处理(例如，只在创建用户之后才为客户创建订单)。例如，在Kafka中，您可以通过用户ID对主题进行分区，这样与单个用户相关的所有事件都将由分配给该分区的单个使用者处理，从而允许按顺序处理这些事件。相反，在使用消息代理机制时，消息队列虽然有其固有的执行顺序，但多个并发使用者使得按给定顺序进行消息处理非常困难，甚至不可能。这样就可能会遇到并发问题。</p>\n<p>In practice, an “event-first” approach is hard to implement in scenarios when <strong>linearizability</strong><sub>线性化</sub> is required or in scenarios with many data <strong>constraints</strong><sub>约束</sub> such as uniqueness checks. But it really <strong>shines</strong> <sub>发光，大放异彩</sub>in other scenarios. However, due to its asynchronous nature, challenges with concurrency and race conditions still need to be <strong>addressed</strong><sub>解决</sub>.</p>\n<p>实际上，当<strong>线性一致性</strong>是必需的，或者在有许多数据约束(如唯一性检查)的情况下，“事件优先”方法很难实现。但在其他场景中但它确实可以大放异彩。然而，由于它的异步性质，并发和竞争条件的挑战仍然需要解决。</p>\n<h2 id=\"Consistency-by-design\"><a href=\"#Consistency-by-design\" class=\"headerlink\" title=\"Consistency by design\"></a>Consistency by design</h2><p>设计一致性</p>\n<p>There many ways to split the system into multiple services. We <strong>strive</strong> <sub>努力</sub>to match separate microservices with separate domains. But how <strong>granular</strong><sub>粒度</sub> are the domains? Sometimes it’s hard to <strong>differentiate</strong> <sub>区分</sub>domains from subdomains or aggregation roots. There is no simple rule to define your microservices split.</p>\n<p>有许多方法可以将系统分成多个服务。我们努力将不同的微服务与不同的域相匹配。但是这些域有多细呢？有时很难将域与子域或聚合根区分开来。没有简单的规则来定义您的微服务拆分。</p>\n<p><strong>Rather than</strong><sub>与其</sub> focusing only on domain-driven design, I suggest to be <strong>pragmatic</strong><sub>务实</sub> and consider all the implications of the design options. One of those implications is how well microservices <strong>isolation</strong> <sub>独立</sub><strong>aligns with</strong><sub>对齐</sub> the transaction boundaries. A system where transactions only reside within microservices doesn’t require any of the solutions above. We should definitely consider the <strong>transaction boundaries</strong><sub>事物边界</sub> while designing the system. In practice, it might be hard to design the whole system in this manner, but I think we should aim to <strong>minimize</strong><sub>最小化</sub> data consistency challenges.</p>\n<p>与其只关注领域驱动的设计，我建议采取务实的态度，并考虑所有设计选项的含义。其中一个含义是微服务隔离与事务边界的匹配程度。事务只驻留在微服务中的系统不需要上述任何解决方案。在设计系统时，一定要考虑事务边界。在实践中，可能很难以这种方式设计整个系统，但我认为我们的目标应该是尽量减少数据一致性的挑战。</p>\n<h4 id=\"Accepting-inconsistency-接受不一致\"><a href=\"#Accepting-inconsistency-接受不一致\" class=\"headerlink\" title=\"Accepting inconsistency 接受不一致\"></a>Accepting inconsistency 接受不一致</h4><p>While it’s <strong>crucial</strong><sub>重要</sub> to match the account balance, there are many use cases where consistency is much less important. Imagine <strong>gathering</strong><sub>收集</sub> data for analytics or statistics purposes. Even if we lose 10% of data from the system randomly, most likely the business value from analytics won’t be affected.</p>\n<p>虽然与帐户余额匹配是至关重要的，但在许多用例中，一致性的重要性要小得多。比如，为分析或统计目的收集数据。即使我们随机丢失了10%的系统数据，从分析中获得的业务价值也很可能不会受到影响。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/10.jpg\" alt=\"Sharing data with events\"></p>\n<h2 id=\"Which-solution-to-choose\"><a href=\"#Which-solution-to-choose\" class=\"headerlink\" title=\"Which solution to choose\"></a>Which solution to choose</h2><p><strong>选择哪种解决方案</strong></p>\n<p>Atomic update of data requires a <strong>consensus</strong><sub>共识</sub> between two different systems, an <strong>agreement</strong> <sub>协议</sub>if a single value is 0 or 1. When it comes to microservices, it <strong>comes down</strong><sub>归结</sub> to problem of consistency between two <strong>participants</strong><sub>参与者</sub> and all practical solutions follow a single <strong>rule of thumb</strong><sub>经验法则</sub>:</p>\n<p>数据的原子更新需要两个不同系统之间的协商一致，形成对某值为0或者为1的共识。当涉及到微服务时，它归结为两个参与者之间的一致性问题，所有实际的解决方案都遵循一个经验法则：</p>\n<blockquote>\n<p>In a given moment, for each data record, you need to find which data source is trusted by your system</p>\n<p>在给定的时刻，对于每个数据记录，需要找到可信的基准数据</p>\n</blockquote>\n<p>The source of truth could be events, the database or one of the services. Achieving consistency in microservice systems is developers’ responsibility. My approach is the following:</p>\n<ol>\n<li><p>Try to design a system that doesn’t require distributed consistency. Unfortunately, that’s <strong>barely possible</strong><sub>几乎不可能</sub> for complex systems.</p>\n</li>\n<li><p>Try to reduce the number of <strong>inconsistencies</strong> <sub>不一致性</sub>by modifying one data source at a time.</p>\n</li>\n<li><p>Consider event-driven architecture. A big strength of event-driven architecture in addition to <strong>loose coupling</strong><sub>松耦合</sub> is a natural way of achieving data consistency by having events as a single source of truth or producing events as a result of change data capture.</p>\n</li>\n<li><p>More complex scenarios might still require synchronous calls between services, failure handling and compensations. Know that sometimes you may have to reconcile afterwards.</p>\n</li>\n<li><p>Design your service capabilities to be <strong>reversible</strong>可逆的, decide how you will handle failure scenarios and achieve consistency early in the design phase.</p>\n</li>\n</ol>\n<p>基准数据可以是事件、数据库或某个服务。在微服务系统中实现一致性是开发人员的责任。我的做法如下：</p>\n<ol>\n<li><p>尝试设计一个不需要分布式一致性的系统。不幸的是，对于复杂的系统来说，这几乎是不可能的。</p>\n</li>\n<li><p>尝试通过一次修改一个数据源来减少不一致的数量。</p>\n</li>\n<li><p>考虑一下事件驱动的体系结构。除了松散耦合之外，事件驱动体系结构的一大优势是天然的支持基于事件的数据一致性，可以将事件作为基准数据，也可以由变更数据捕获（CDC）生成事件。</p>\n</li>\n<li><p>更复杂的场景可能仍然需要服务、故障处理和补偿之间的同步调用。要知道，有时你可能不得不在事后对账。</p>\n</li>\n<li><p>将您的服务功能设计为可逆的，决定如何处理故障场景，并在设计阶段早期实现一致性。</p>\n</li>\n</ol>\n<p><strong>参考</strong>：</p>\n<p>英文：<a href=\"https://ebaytech.berlin/data-consistency-in-microservices-architecture-bf99ba31636f\" target=\"_blank\" rel=\"noopener\">https://ebaytech.berlin/data-consistency-in-microservices-architecture-bf99ba31636f</a></p>\n<p>翻译：<a href=\"https://mp.weixin.qq.com/s/nFHkvwSmjDd9ruKHH4eomA\" target=\"_blank\" rel=\"noopener\">https://mp.weixin.qq.com/s/nFHkvwSmjDd9ruKHH4eomA</a></p>\n","categories":["translate"],"tags":["translate","microservices"]},{"title":"BitMap的JAVA实现","url":"https://sustcoder.github.io/2018/10/23/bitMap-explain-and-use/","content":"<h2 id=\"相关概念\"><a href=\"#相关概念\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><h3 id=\"基础类型\"><a href=\"#基础类型\" class=\"headerlink\" title=\"基础类型\"></a>基础类型</h3><p>在java中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">byte  -&gt;   8 bits  --&gt;1字节</span><br><span class=\"line\">char  -&gt;   16 bit  --&gt;2字节</span><br><span class=\"line\">short -&gt;   16 bits --&gt;2字节</span><br><span class=\"line\">int   -&gt;   32 bits --&gt;4字节</span><br><span class=\"line\">float -&gt;   32 bits --&gt;4字节</span><br><span class=\"line\">long  -&gt;   64 bits --&gt;8字节</span><br></pre></td></tr></table></figure>\n<h3 id=\"位运算符\"><a href=\"#位运算符\" class=\"headerlink\" title=\"位运算符\"></a>位运算符</h3><p>在java中，int数据底层以补码形式存储。int型变量使用32bit存储数据，其中最高位是符号位，0表示正数，1表示负数，可通过<code>Integer.toBinaryString()</code>转换为bit字符串，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 若最高的几位为0则不输出这几位，从为1的那一位开始输出</span><br><span class=\"line\">System.out.println(Integer.toBinaryString(10)); </span><br><span class=\"line\">System.out.println(Integer.toBinaryString(-10));</span><br><span class=\"line\">// 会输出（手工排版过，以下的输出均会被手工排版）：</span><br><span class=\"line\">                            1010</span><br><span class=\"line\">11111111111111111111111111110110</span><br></pre></td></tr></table></figure>\n<p><strong>左移&lt;&lt;</strong></p>\n<blockquote>\n<p>5&lt;&lt;2=20</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">首先会将5转为2进制表示形式: 0000 0000 0000 0000 0000 0000 0000 0101  </span><br><span class=\"line\">然后左移2位后，低位补0：    0000 0000 0000 0000 0000 0000 0001 0100  </span><br><span class=\"line\">换算成10进制为20</span><br></pre></td></tr></table></figure>\n<p><strong>右移&gt;&gt;</strong></p>\n<blockquote>\n<p>5&gt;&gt;2=1</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">还是先将5转为2进制表示形式：0000 0000 0000 0000 0000 0000 0000 0101 </span><br><span class=\"line\">然后右移2位，高位补0：     0000 0000 0000 0000 0000 0000 0000 0001</span><br><span class=\"line\">换算成十进制后是1</span><br></pre></td></tr></table></figure>\n<p><strong>无符号右移&gt;&gt;&gt;</strong></p>\n<blockquote>\n<p>5&gt;&gt;&gt;3</p>\n</blockquote>\n<p>我们知道在Java中int类型占32位，可以表示一个正数，也可以表示一个负数。正数换算成二进制后的最高位为0，负数的二进制最高为为1。<strong>对于2进制补码的加法运算，和平常的计算一样，而且符号位也参与运算，不过最后只保留32位。</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">-5换算成二进制： 1111 1111 1111 1111 1111 1111 1111 1011</span><br><span class=\"line\">-5右移3位：     1111 1111 1111 1111 1111 1111 1111 1111   // (用1进行补位，结果为-1)</span><br><span class=\"line\">-5无符号右移3位: 0001 1111 1111 1111 1111 1111 1111 1111   // (用0进行补位,结果536870911 )</span><br></pre></td></tr></table></figure>\n<p><strong>位与</strong>&amp;</p>\n<p>第一个操作数的的第n位于第二个操作数的第n位如果都是1，那么结果的第n为也为1，否则为0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101</span><br><span class=\"line\">3转换为二进制：0000 0000 0000 0000 0000 0000 0000 0011</span><br><span class=\"line\">------------------------------------------------------------</span><br><span class=\"line\">1转换为二进制：0000 0000 0000 0000 0000 0000 0000 0001</span><br></pre></td></tr></table></figure>\n<p><strong>位或|</strong></p>\n<p>第一个操作数的的第n位于第二个操作数的第n位只要有一个为1则为1，否则为0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101</span><br><span class=\"line\">3转换为二进制：0000 0000 0000 0000 0000 0000 0000 0011</span><br><span class=\"line\">-------------------------------------------------------------------------------------</span><br><span class=\"line\">6转换为二进制：0000 0000 0000 0000 0000 0000 0000 0111</span><br></pre></td></tr></table></figure>\n<p>对于移位运算，例如将x左移/右移n位，如果x是byte、short、char、int，<strong>n会先模32（即n=n%32），然后再进行移位操作</strong>。可以这样解释：int类型为32位,移动32位（或以上）没有意义。</p>\n<p>同理若x是long，n=n%64。</p>\n<p><strong>左移和右移代替乘除</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">a=a*4;</span><br><span class=\"line\">b=b/4;</span><br></pre></td></tr></table></figure>\n<p>　可以改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">a=a&lt;&lt;2;</span><br><span class=\"line\">b=b&gt;&gt;2;</span><br></pre></td></tr></table></figure>\n<p>　　<strong>说明：</strong><br>　　除2 = 右移1位 乘2 = 左移1位<br>　　除4 = 右移2位 乘4 = 左移2位<br>　　除8 = 右移3位 乘8 = 左移3位<br>　　… …<br>　　类比十进制中的满十进一，向左移动小数点后，数字就会缩小十倍，在二进制中满二进一，进行右移一次相当于缩小了2两倍，右移两位相当于缩小了4倍，右移三位相当于缩小了8倍。通常如果需要乘以或除以2的n次方，都可以用移位的方法代替。 　　<br>　　<strong>实际上，只要是乘以或除以一个整数，均可以用移位的方法得到结果，</strong>如：<br>　　a=a<em>9<br>　　分析a</em>9可以拆分成a<em>(8+1)即a</em>8+a<em>1, 因此可以改为： a=(a&lt;&lt;3)+a<br>　　a=a</em>7<br>　　分析a<em>7可以拆分成a</em>(8-1)即a<em>8-a</em>1, 因此可以改为： a=(a&lt;&lt;3)-a<br>　　关于除法读者可以类推, 此略。<br>　　【注意】由于+/-运算符优先级比移位运算符高，所以在写公式时候一定要记得添加括号，不可以 a = a*12 等价于 a = a&lt;&lt;3 +a &lt;&lt;2; 要写成a = (a&lt;&lt;3)+(a &lt;&lt;2 )。</p>\n<p><strong>与运算代替取余</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">31转换为二进制：011111,0,31</span><br><span class=\"line\">32转换为二进制：100010 与31取交集的结果是：10转换为十进制为2</span><br><span class=\"line\">31转换为二进制：100001 与31取交集的结果是：01转换为十进制为1</span><br><span class=\"line\">30转换为二进制：011110 与31取交集的结果是：11110转换为十进制为30</span><br><span class=\"line\">29转换为二进制：011101 与31取交集的结果是：11101转换为十进制为29</span><br><span class=\"line\">33转换为二进制：100001 与31取交集的结果是：1转换为十进制为1</span><br></pre></td></tr></table></figure>\n<p>31转换为二进制后，低位值全部为1，高位全为0。所以和其进行与运算，高位和0与，结果是0，相当于将高位全部截取，截取后的结果肯定小于等于31，地位全部为1，与1与值为其本身，所以相当于对数进行了取余操作。</p>\n<h3 id=\"进制转换\"><a href=\"#进制转换\" class=\"headerlink\" title=\"进制转换\"></a>进制转换</h3><ul>\n<li><code>0x</code>开头表示16进制，例如：0x2表示：2，0x2f表示48</li>\n<li><code>0</code>开头表示8进制，例如：02表示：2,010表示：8</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Integer.toHexString(int i)   // 十进制转成十六进制</span><br><span class=\"line\">Integer.toOctalString(int i) // 十进制转成八进制 </span><br><span class=\"line\">Integer.toBinaryString(int i)// 十进制转成二进制</span><br><span class=\"line\">Integer.valueOf(m,n).toString() // 把n进制的m转换为10进制</span><br></pre></td></tr></table></figure>\n<h2 id=\"BitMap实现原理\"><a href=\"#BitMap实现原理\" class=\"headerlink\" title=\"BitMap实现原理\"></a>BitMap实现原理</h2><p>在java中，一个int类型占32个字节，我们用一个int数组来表示时未new int[32],总计占用内存32*32bit,现假如我们用int字节码的每一位表示一个数字的话，那么32个数字只需要一个int类型所占内存空间大小就够了，这样在大数据量的情况下会节省很多内存。</p>\n<p>具体思路：</p>\n<p>   1个int占4字节即4*8=32位，那么我们只需要申请一个int数组长度为 int tmp[1+N/32]即可存储完这些数据，其中N代表要进行查找的总数，tmp中的每个元素在内存在占32位可以对应表示十进制数0~31,所以可得到BitMap表:</p>\n<p>tmp[0]:可表示0~31</p>\n<p>tmp[1]:可表示32~63</p>\n<p>tmp[2]可表示64~95</p>\n<p>…….</p>\n<p>那么接下来就看看十进制数如何转换为对应的bit位：</p>\n<p>假设这40亿int数据为：6,3,8,32,36,……，那么具体的BitMap表示为：</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/algorithm/bitMap.jpg\" alt=\"bitMap.jpg\"></p>\n<p>如何判断int数字在tmp数组的哪个下标，这个其实可以通过直接除以32取整数部分，例如：整数8除以32取整等于0，那么8就在tmp[0]上。另外，我们如何知道了8在tmp[0]中的32个位中的哪个位，这种情况直接mod上32就ok，又如整数8，在tmp[0]中的第8 mod上32等于8，那么整数8就在tmp[0]中的第八个bit位（从右边数起）。</p>\n<h2 id=\"BitMap源码\"><a href=\"#BitMap源码\" class=\"headerlink\" title=\"BitMap源码\"></a>BitMap源码</h2><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">long</span> length;</span><br><span class=\"line\">   <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">int</span>[] bitsMap;</span><br><span class=\"line\">   <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span>[] BIT_VALUE = &#123;<span class=\"number\">0x00000001</span>, <span class=\"number\">0x00000002</span>, <span class=\"number\">0x00000004</span>, <span class=\"number\">0x00000008</span>, <span class=\"number\">0x00000010</span>, <span class=\"number\">0x00000020</span>,</span><br><span class=\"line\">           <span class=\"number\">0x00000040</span>, <span class=\"number\">0x00000080</span>, <span class=\"number\">0x00000100</span>, <span class=\"number\">0x00000200</span>, <span class=\"number\">0x00000400</span>, <span class=\"number\">0x00000800</span>, <span class=\"number\">0x00001000</span>, <span class=\"number\">0x00002000</span>, <span class=\"number\">0x00004000</span>,</span><br><span class=\"line\">           <span class=\"number\">0x00008000</span>, <span class=\"number\">0x00010000</span>, <span class=\"number\">0x00020000</span>, <span class=\"number\">0x00040000</span>, <span class=\"number\">0x00080000</span>, <span class=\"number\">0x00100000</span>, <span class=\"number\">0x00200000</span>, <span class=\"number\">0x00400000</span>, <span class=\"number\">0x00800000</span>,</span><br><span class=\"line\">           <span class=\"number\">0x01000000</span>, <span class=\"number\">0x02000000</span>, <span class=\"number\">0x04000000</span>, <span class=\"number\">0x08000000</span>, <span class=\"number\">0x10000000</span>, <span class=\"number\">0x20000000</span>, <span class=\"number\">0x40000000</span>, <span class=\"number\">0x80000000</span>&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">BitMap2</span><span class=\"params\">(<span class=\"keyword\">long</span> length)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"keyword\">this</span>.length = length;</span><br><span class=\"line\">       <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">        * 根据长度算出，所需数组大小</span></span><br><span class=\"line\"><span class=\"comment\">        * 当 length%32=0 时大小等于</span></span><br><span class=\"line\"><span class=\"comment\">        * = length/32</span></span><br><span class=\"line\"><span class=\"comment\">        * 当 length%32&gt;0 时大小等于</span></span><br><span class=\"line\"><span class=\"comment\">        * = length/32+l</span></span><br><span class=\"line\"><span class=\"comment\">        */</span></span><br><span class=\"line\">       bitsMap = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[(<span class=\"keyword\">int</span>) (length &gt;&gt; <span class=\"number\">5</span>) + ((length &amp; <span class=\"number\">31</span>) &gt; <span class=\"number\">0</span> ? <span class=\"number\">1</span> : <span class=\"number\">0</span>)];</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">    * <span class=\"doctag\">@param</span> n 要被设置的值为n</span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setN</span><span class=\"params\">(<span class=\"keyword\">long</span> n)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (n &lt; <span class=\"number\">0</span> || n &gt; length) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"length value \"</span>+n+<span class=\"string\">\" is  illegal!\"</span>);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"comment\">// 求出该n所在bitMap的下标,等价于\"n/5\"</span></span><br><span class=\"line\">       <span class=\"keyword\">int</span> index = (<span class=\"keyword\">int</span>) n&gt;&gt;<span class=\"number\">5</span>;</span><br><span class=\"line\">       <span class=\"comment\">// 求出该值的偏移量(求余),等价于\"n%31\"</span></span><br><span class=\"line\">       <span class=\"keyword\">int</span> offset = (<span class=\"keyword\">int</span>) n &amp; <span class=\"number\">31</span>;</span><br><span class=\"line\">       <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">        * 等价于</span></span><br><span class=\"line\"><span class=\"comment\">        * int bits = bitsMap[index];</span></span><br><span class=\"line\"><span class=\"comment\">        * bitsMap[index]=bits| BIT_VALUE[offset];</span></span><br><span class=\"line\"><span class=\"comment\">        * 例如,n=3时,设置byte第4个位置为1 （从0开始计数，bitsMap[0]可代表的数为：0~31，从左到右每一个bit位表示一位数）</span></span><br><span class=\"line\"><span class=\"comment\">        * bitsMap[0]=00000000 00000000 00000000 00000000  |  00000000 00000000 00000000 00001000=00000000 00000000 00000000 00000000 00001000</span></span><br><span class=\"line\"><span class=\"comment\">        * 即: bitsMap[0]= 0 | 0x00000008 = 3</span></span><br><span class=\"line\"><span class=\"comment\">        *</span></span><br><span class=\"line\"><span class=\"comment\">        * 例如,n=4时,设置byte第5个位置为1</span></span><br><span class=\"line\"><span class=\"comment\">        * bitsMap[0]=00000000 00000000 00000000 00001000  |  00000000 00000000 00000000 00010000=00000000 00000000 00000000 00000000 00011000</span></span><br><span class=\"line\"><span class=\"comment\">        * 即: bitsMap[0]=3 | 0x00000010 = 12</span></span><br><span class=\"line\"><span class=\"comment\">        */</span></span><br><span class=\"line\">       bitsMap[index] |= BIT_VALUE[offset];</span><br><span class=\"line\"></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">    * 获取值N是否存在</span></span><br><span class=\"line\"><span class=\"comment\">    * <span class=\"doctag\">@return</span> 1：存在，0：不存在</span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">isExist</span><span class=\"params\">(<span class=\"keyword\">long</span> n)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (n &lt; <span class=\"number\">0</span> || n &gt; length) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"length value illegal!\"</span>);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">int</span> index = (<span class=\"keyword\">int</span>) n&gt;&gt;<span class=\"number\">5</span>;</span><br><span class=\"line\">       <span class=\"keyword\">int</span> offset = (<span class=\"keyword\">int</span>) n &amp; <span class=\"number\">31</span>;</span><br><span class=\"line\">       <span class=\"keyword\">int</span> bits = (<span class=\"keyword\">int</span>) bitsMap[index];</span><br><span class=\"line\">       <span class=\"comment\">// System.out.println(\"n=\"+n+\",index=\"+index+\",offset=\"+offset+\",bits=\"+Integer.toBinaryString(bitsMap[index]));</span></span><br><span class=\"line\">       <span class=\"keyword\">return</span> ((bits &amp; BIT_VALUE[offset])) &gt;&gt;&gt; offset;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"BitMap应用\"><a href=\"#BitMap应用\" class=\"headerlink\" title=\"BitMap应用\"></a>BitMap应用</h2><ol>\n<li>BitMap小小变种:2-BitMap。</li>\n</ol>\n<p>看个小场景：在3亿个整数中找出不重复的整数，限制内存不足以容纳3亿个整数。</p>\n<p>对于这种场景我可以采用2-BitMap来解决，即为每个整数分配2bit，用不同的0、1组合来标识特殊意思，如00表示此整数没有出现过，01表示出现一次，11表示出现过多次，就可以找出重复的整数了，其需要的内存空间是正常BitMap的2倍，为：3亿*2/8/1024/1024=71.5MB。</p>\n<p>具体的过程如下：</p>\n<p>   扫描着3亿个整数，组BitMap，先查看BitMap中的对应位置，如果00则变成01，是01则变成11，是11则保持不变，当将3亿个整数扫描完之后也就是说整个BitMap已经组装完毕。最后查看BitMap将对应位为11的整数输出即可。</p>\n<ol start=\"2\">\n<li>已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。</li>\n</ol>\n<p>8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。 （可以理解为从0-99 999 999的数字，每个数字对应一个Bit位，所以只需要99M个Bit==1.2MBytes，这样，就用了小小的1.2M左右的内存表示了所有的8位数的电话）</p>\n<h2 id=\"BitMap问题\"><a href=\"#BitMap问题\" class=\"headerlink\" title=\"BitMap问题\"></a>BitMap问题</h2><p>BitMap 的思想在面试的时候还是可以用来解决不少问题的，然后在很多系统中也都会用到，算是一种不错的解决问题的思路。</p>\n<p>但是 BitMap 也有一些局限，因此会有其它一些基于 BitMap 的算法出现来解决这些问题。</p>\n<ul>\n<li>数据碰撞。比如将字符串映射到 BitMap 的时候会有碰撞的问题，那就可以考虑用 Bloom Filter 来解决，Bloom Filter 使用多个 Hash 函数来减少冲突的概率。</li>\n<li>数据稀疏。又比如要存入(10,8887983,93452134)这三个数据，我们需要建立一个 99999999 长度的 BitMap ，但是实际上只存了3个数据，这时候就有很大的空间浪费，碰到这种问题的话，可以通过引入 Roaring BitMap 来解决。</li>\n</ul>\n<p><strong>参考链接</strong></p>\n<p> <a href=\"http://www.infoq.com/cn/articles/the-secret-of-bitmap\" target=\"_blank\" rel=\"noopener\">Bitmap的秘密</a></p>\n","categories":["algorithm"],"tags":["algorithm","java"]},{"title":"spark环境搭建杂记","url":"https://sustcoder.github.io/2018/10/20/spark env building process/","content":"<p><em>开发者如何进行持续高效的学习</em> ？？？</p>\n<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>最初搭建环境的目的只是为了跑spark程序，从开始搭建spark环境到最后跑spark环境断断续续用了16天，最后却发现如果想实现初衷，仅仅需要在之前运行scala程序的基础上，花费一天最多两天的时间就可以完成（实际完成这块搭建工作用了不到两小时）。严重浪费了时间，影响了学习进度。在解决问题的过程中发现一个园岭只有五个月的博客，但是高产了一百多偏博客，其中spark模块的阅读量基本在500左右，其也是从搭建环境开始写博客，内容详细且易懂。觉得是时候对此过程进行复盘了。</p>\n<h2 id=\"心路历程\"><a href=\"#心路历程\" class=\"headerlink\" title=\"心路历程\"></a>心路历程</h2><p>我是照着<a href=\"http://allaboutscala.com\" target=\"_blank\" rel=\"noopener\">http://allaboutscala.com</a> 网站进行学习的，在学完scala后，开始学习spark了，但是第一步就卡主了，网站没有搭建开发环境的教程。但是示例中有提到SBT，所以就弃用maven转战SBT了，本以为和maven类似，配置完仓库自动下载就ok了，但事与愿违，依赖总是下载失败，在各种google后，<a href=\"https://sustcoder.github.io/2018/09/13/sbt%E5%85%A5%E9%97%A8/\">终于搭建好了sbt环境</a>。此过程浪费时间主要在于好多博客介绍方法比如oschina的仓库已经过时，阿里云的仓库数据不全等。</p>\n<p>sbt环境搭建完毕后开始写spark程序，但是spark-context直接就报错了，然后就开始了搭建spark环境，因为vm上已经有一个伪分布式的spark集群了，所以就想着通过idea直连伪集群，将代码直接提交到伪集群上运行。按照这个思想在idea里面写了一个程序后就直接跑了，各种报错各种找原因，最后博友建议先保证服务器环境没问题，所以开始了spark-submit提交Spark-Pi.jar。此过程是照着spark官网给的demo进行测试的，在local模式下没问题，但是在cluster模式下一直报错，然后开始疯狂寻找答案，网上也给报错对应了很多解决方法。但无一例外失败告终，最后无奈之下，放弃了在伪分布式集群上提交jar，转真正的全分布式。</p>\n<p>之前有搭建过hadoop的高可用集群，想着这个过程应该轻车熟路了吧，但是依旧有好多坑，记录的笔记有遗漏点，本着对自己的信任，照着笔记按步照班，不幸的是这个过程中错误百出。好多细节笔记上没有记录，细节的叠加导致新的问题，只能再次疯狂的寻找各种答案各种尝试。</p>\n<p>集群搭建完成后，开始配置spark环境，发现之前在伪分布式上搜索的好多答案都是针对完全分布式的解决方法，心痛。配置完成后，通过spark-submit也可以提交jar了，开始运行idea中的spark程序，不出意外还是报错，报错原因和最开始调用伪分布式环境时的类似，而且都有类似前提就是本地需要spark环境，在最开始就是为了不想在本地配置spark环境才开始直接在服务器上配置的，无奈之下配置好本地spark环境，意外发现通过local模式可以不连接服务器也可以在idea跑spark程序了，这不就是我最初的目标吗？伤心欲绝，打算暂停本地访问yarn集群时，还是决定最后一搏，通过报错信息搜索到了一个解决方法，初看不是很靠谱，但是还是照着其步骤修改了，惊喜随之而来，至此环境搭建完成。</p>\n<h2 id=\"复盘\"><a href=\"#复盘\" class=\"headerlink\" title=\"复盘\"></a>复盘</h2><h3 id=\"决策\"><a href=\"#决策\" class=\"headerlink\" title=\"决策\"></a>决策</h3><p>决策取决于很多因素，在完全不懂spark环境时，是根据自己仅有的知识去判断下一步应该怎么做，导致路线方针性错误。如果最开始直接搭建本地环境，则不会有这么曲折的过程，还有过程中在伪分布式上跑jar，使用sbt还是maven等。</p>\n<p>怎样才能做好决策。<strong>首先得弄清楚自己的根本需求是什么，我接下来要干什么</strong>，我的最初目标只是为了让spark程序跑起来，至于怎么跑我是不关心的。那么去寻找解决办法时就会变成“怎样跑spark程序”，然后就会发现可以直接在本地运行，也可以在服务器上运行。然后就是选择问题了，我到底选择哪一种呢，那么就得需要<strong>收集各种方法的优劣来进行权衡</strong>，但是如果再选一次我依旧会选择在服务器上跑吧，因为伪分布式环境是现成的，这中间又牵引出另一个问题，什么时候进行放弃，当此路不通的时候，我是誓死完成，还是换条路，我当时的想法是非要把你弄出来。</p>\n<h3 id=\"放弃\"><a href=\"#放弃\" class=\"headerlink\" title=\"放弃\"></a>放弃</h3><p>什么时候放弃当前的方向。我们经常会因为一个问题阻塞很久，但是不愿放弃当前的方向，主要原因有两个：不想放弃已有成果、再想想办法肯定可以解决。但往往也是在此时陷入了死循环。那么在什么时候放弃呢，就目前经验而言：</p>\n<ol>\n<li>搜索出来的解决方案大同小异，而且都尝试过。此时也需要考虑搜索方式是否正确</li>\n<li>停止寻找答案，对问题再次进行分析，找次思路可行性的论证，如果发现当前方向没有理论支持时</li>\n</ol>\n<h3 id=\"搜索\"><a href=\"#搜索\" class=\"headerlink\" title=\"搜索\"></a>搜索</h3><p>越来越觉得搜索是门艺术，许多问题的解决都是在输入了对应关键字后，得到了搜索结果。一个是不断整理搜索引擎的使用技巧，google有联系搜索的网站，后期有时间了还是需要学习一下。另一个就是当搜索结果没有答案时，就需要考虑自己的关键字是否正确了，还是那个问题，我到底要什么，这个问题的根本原因是什么，大家在记录此问题时会有哪些关键信息，这个关键字还有哪些叫法等等，此处还需加强联系，搜索技能太落后。</p>\n<h3 id=\"信息\"><a href=\"#信息\" class=\"headerlink\" title=\"信息\"></a>信息</h3><p>目前解决问题的问题比较单一，主要还是通过google,对源码和官方文档的依赖较少，对于初学者直接阅读源码效果较慢但是可以更深入理解问题原因。官方文档需要去快速定位到文档位置，此处就需要英文阅读能力了，因为英文的原因，总是对阅读文档有内心的抗拒，博客的内容都是在特定场景下发生的，不一定适用自己，而文档的内容是最权威的。还有一个就是对于论坛，文档搜索答案困难可以去其对应论坛找答案比在博客找到的答案更精确，解答也更深入，Spark user list是spark的问题的一个问题聚集地，在解决掉问题后才发现这个地方，以后应主动去寻找类似论坛，然后去寻找答案。</p>\n","categories":["replay"],"tags":["replay"]},{"title":"RDD详解","url":"https://sustcoder.github.io/2018/10/16/RDD explain/","content":"<h2 id=\"1-1-什么是RDD\"><a href=\"#1-1-什么是RDD\" class=\"headerlink\" title=\"1.1 什么是RDD\"></a>1.1 什么是RDD</h2><h3 id=\"1-1-1-产生背景\"><a href=\"#1-1-1-产生背景\" class=\"headerlink\" title=\"1.1.1 产生背景\"></a>1.1.1 产生背景</h3><p>当初设计RDD主要是为了解决三个问题：</p>\n<ul>\n<li><strong>Fast</strong>: Spark之前的Hadoop用的是MapReduce的编程模型，没有很好的利用分布式内存系统，中间结果都需要保存到external disk，运行效率很低。RDD模型是in-memory computing的，中间结果不需要被物化（materialized），它的<strong>persistence</strong>机制，可以保存中间结果重复使用，对需要迭代运算的机器学习应用和交互式数据挖掘应用，加速显著。Spark快还有一个原因是开头提到过的<strong>Delay Scheduling</strong>机制，它得益于RDD的Dependency设计。</li>\n<li><strong>General: MapReduce</strong>编程模型只能提供有限的运算种类（Map和Reduce），RDD希望支持更广泛更多样的operators（map，flatMap，filter等等），然后用户可以任意地组合他们。</li>\n</ul>\n<blockquote>\n<p>The ability of RDDs to accommodate computing needs that were previously met only by introducing new frameworks is, we believe, the most credible evidence of the power of the RDD abstraction.</p>\n</blockquote>\n<ul>\n<li><strong>Fault tolerance</strong>: 其他的in-memory storage on clusters，基本单元是可变的，用细粒度更新（<strong>fine-grained updates</strong>）方式改变状态，如改变table/cell里面的值，这种模型的容错只能通过复制多个数据copy，需要传输大量的数据，容错效率低下。而RDD是<strong>不可变的（immutable）</strong>，通过粗粒度变换（<strong>coarse-grained transformations</strong>），比如map，filter和join，可以把相同的运算同时作用在许多数据单元上，这样的变换只会产生新的RDD而不改变旧的RDD。这种模型可以让Spark用<strong>Lineage</strong>很高效地容错（后面会有介绍）。</li>\n</ul>\n<h3 id=\"1-1-2-RDD定义\"><a href=\"#1-1-2-RDD定义\" class=\"headerlink\" title=\"1.1.2 RDD定义\"></a>1.1.2 <strong>RDD定义</strong></h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents（代表） an immutable(不变的),partitioned collection of elements that can be operated on in parallel</span><br></pre></td></tr></table></figure>\n<p>RDD是spark的核心，也是整个spark的架构基础，RDD是弹性分布式集合（<code>Resilient Distributed Datasets</code>）的简称。</p>\n<h3 id=\"1-1-3-RDD特点\"><a href=\"#1-1-3-RDD特点\" class=\"headerlink\" title=\"1.1.3 RDD特点\"></a>1.1.3 <strong>RDD特点</strong></h3><ul>\n<li>immutable：只读，任何操作都不会改变RDD本身，只会创造新的RDD</li>\n<li>fault-tolerant：容错，通过Lineage可以高效容错</li>\n<li>partitioned：分片，RDD以partition作为最小存储和计算单元，分布在cluster的不同nodes上，一个node可以有多个partitions，一个partition只能在一个node上</li>\n<li><p>in parallel：并行，一个Task对应一个partition，Tasks之间相互独立可以并行计算</p>\n</li>\n<li><p>persistence：持久化，用户可以把会被重复使用的RDDs保存到storage上（内存或者磁盘）</p>\n</li>\n<li>partitioning：分区，用户可以选择RDD元素被partitioned的方式来优化计算，比如两个需要被join的数据集可以用相同的方式做hash-partitioned，这样可以减少shuffle提高性能</li>\n</ul>\n<h3 id=\"1-1-4-RDD抽象概念\"><a href=\"#1-1-4-RDD抽象概念\" class=\"headerlink\" title=\"1.1.4 RDD抽象概念\"></a>1.1.4 <strong>RDD抽象概念</strong></h3><p>一个RDD定义了对数据的一个操作过程, 用户提交的计算任务可以由多个RDD构成。多个RDD可以是对单个/多个数据的多个操作过程。多个RDD之间的关系使用依赖来表达。操作过程就是用户自定义的函数。</p>\n<p>RDD(弹性分布式数据集)去掉形容词，主体为：数据集。如果认为RDD就是数据集，那就有点理解错了。个人认为：RDD是定义对partition数据项转变的高阶函数，应用到输入源数据，输出转变后的数据，即：<strong>RDD是一个数据集到另外一个数据集的映射，而不是数据本身。</strong> 这个概念类似数学里的函数<code>f(x) = ax^2 + bx + c</code>。这个映射函数可以被序列化，所要被处理的数据被分区后分布在不同的机器上，应用一下这个映射函数，得出结果，聚合结果。</p>\n<p>这些集合是弹性的，如果数据集一部分丢失，则可以对它们进行重建。具有自动容错、位置感知调度和可伸缩性，而容错性是最难实现的，大多数分布式数据集的容错性有两种方式：数据检查点和记录数据的更新。对于大规模数据分析系统，数据检查点操作成本高，主要原因是大规模数据在服务器之间的传输带来的各方面的问题，相比记录数据的更新，RDD也只支持粗粒度的转换共享状态而非细粒度的更新共享状态，也就是记录如何从其他RDD转换而来(即lineage)，以便恢复丢失的分区。 </p>\n<p>RDDs 非常适合将相同操作应用在整个数据集的所有的元素上的批处理应用. 在这些场景下, RDDs 可以利用血缘关系图来高效的记住每一个 transformations 的步骤, 并且不需要记录大量的数据就可以恢复丢失的分区数据. RDDs 不太适合用于需要异步且细粒度的更新共享状态的应用, 比如一个 web 应用或者数据递增的 web 爬虫应用的存储系统。</p>\n<h2 id=\"1-2-RDD特点\"><a href=\"#1-2-RDD特点\" class=\"headerlink\" title=\"1.2 RDD特点\"></a>1.2 RDD特点</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Internally, each RDD is characterized by five main properties:</span><br><span class=\"line\">- A list of partitions</span><br><span class=\"line\">- A function for computing each split</span><br><span class=\"line\">- A list of dependencies on other RDDs</span><br><span class=\"line\">- Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</span><br><span class=\"line\">- Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</span><br></pre></td></tr></table></figure>\n<p>每个特性都对应RDD.scala中的一个方法实现：</p>\n<ul>\n<li><p><code>a list of partition</code>  由多个机器里面的partition组成的</p>\n</li>\n<li><p><code>a function for computing each split</code>  并行计算 </p>\n</li>\n<li><p><code>a list of dependencies on other RDDS</code> rdd间存在依赖关系,记录数据转换间的依赖</p>\n</li>\n<li><p><code>a partitioner for key-vaue</code> RDDS 可进行重新分区（只有key value的partition有）</p>\n</li>\n<li><p><code>a list of preferred locations to compute each spilt on</code>  用最期望的位置进行计算</p>\n</li>\n</ul>\n<h2 id=\"1-3-RDD操作\"><a href=\"#1-3-RDD操作\" class=\"headerlink\" title=\"1.3 RDD操作\"></a>1.3 RDD操作</h2><h3 id=\"1-3-1-RDD创建\"><a href=\"#1-3-1-RDD创建\" class=\"headerlink\" title=\"1.3.1 RDD创建\"></a>1.3.1 RDD创建</h3><ol>\n<li><code>parallelize</code>:从普通Scala集合创建</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> data = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> distData = sc.parallelize(data)</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>从Hadoop文件系统或与Hadoop兼容的其他持久化存储系统创建，如Hive、HBase</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> distFile = sc.textFile(<span class=\"string\">\"data.txt\"</span>)</span><br><span class=\"line\">distFile: org.apache.spark.rdd.<span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = data.txt <span class=\"type\">MapPartitionsRDD</span>[<span class=\"number\">10</span>] at textFile at &lt;console&gt;:<span class=\"number\">26</span></span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>从父RDD转换得到新的RDD</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> fromParent=distFile.map(s=&gt;s.length)</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-2-操作方式\"><a href=\"#1-3-2-操作方式\" class=\"headerlink\" title=\"1.3.2 操作方式\"></a>1.3.2 操作方式</h3><p>RDD在宏观来看类似于java中对象的概念，我们在java中对对象上作用一系列操作（方法）得到最终结果。同样的我们在RDD上进行一系列操作（算子）将一个RDD转换为另一个RDD，最终得到我们所需要的RDD。RDD算子主要包括：</p>\n<ul>\n<li><p><code>Transformation</code>算子：Transformation操作是延迟计算的，即从一个RDD转换成另一个RDD的转换操作不是 马上执行，需要等到有Action操作时，才真正出发执行，如Map、Filter等操作</p>\n</li>\n<li><p><code>Action</code>算子：Action算子会出发Spark提交作业（Job），并将数据输出到Spark系统，如collect、count等</p>\n</li>\n</ul>\n<p>RDD操作特点 <strong>惰性求值</strong>：</p>\n<p>transformation算子作用在RDD时，并不是立即触发计算，只是记录需要操作的指令。等到有Action算子出现时才真正开始触发计算。</p>\n<p>textFile等读取数据操作和persist和cache缓存操作也是惰性的</p>\n<p>为什么要使用惰性求值呢：使用惰性求值可以把一些操作合并到一起来减少数据的计算步骤，提高计算效率。</p>\n<p>从惰性求值角度看RDD就是一组spark计算指令的列表</p>\n<h3 id=\"1-3-4-缓存策略\"><a href=\"#1-3-4-缓存策略\" class=\"headerlink\" title=\"1.3.4 缓存策略\"></a>1.3.4 缓存策略</h3><p>RDD的缓存策略在<code>StorageLevel</code>中实现，通过对是否序列化，是否存储多个副本等条件的组合形成了多种缓存方式。例如：<code>MEMORY_ONLY_SER</code>存储在内存中并进行序列化，当内存不足时，不进行本地化；<code>MEMORY_AND_DISK_2</code>优先存储内存中，内存中无空间时，存储在本地磁盘，并有两个副本。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StorageLevel</span> <span class=\"title\">private</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    // 缓存方式</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _useDisk: <span class=\"type\">Boolean</span>, \t\t// 是否使用磁盘</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _useMemory: <span class=\"type\">Boolean</span>, \t// 是否使用内存</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _useOffHeap: <span class=\"type\">Boolean</span>,\t// 是否使用堆外内存</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _deserialized: <span class=\"type\">Boolean</span>, // 是否序列化</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _replication: <span class=\"type\">Int</span> = 1</span>)\t<span class=\"title\">//</span> <span class=\"title\">存储副本，默认一个</span></span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Externalizable</span> </span>&#123;</span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 条件组合结果</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">NONE</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">DISK_ONLY</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">DISK_ONLY_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY_SER</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY_SER_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK_SER</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK_SER_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">OFF_HEAP</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p><strong>策略选择顺序</strong>：</p>\n<ul>\n<li><p>默认选择<code>MEMORY_ONLY</code></p>\n</li>\n<li><p>如果内存不足，选择<code>MEMORY_ONLY_SER</code></p>\n</li>\n<li><p>如果需要做容错,选择<code>MEMORY_ONLY_SER_2</code></p>\n</li>\n<li><p>如果中间计算RDD的代价比较大时，选择<code>MEMORY_AND_DISK</code></p>\n</li>\n</ul>\n<p><strong>控制操作</strong>：</p>\n<ol>\n<li><code>persist</code>操作，可以将RDD持久化到不同层次的存储介质，以便后续操作重复使用。</li>\n</ol>\n<p>　　  1)cache:RDD[T]  默认使用<code>MEMORY_ONLY</code></p>\n<p>　　  2)persist:RDD[T] 默认使用<code>MEMORY_ONLY</code></p>\n<p>　　  3)Persist(level:StorageLevel):RDD[T] eg: <code>myRdd.persist(StorageLevels.MEMORY_ONLY_SER)</code></p>\n<ol start=\"2\">\n<li><code>checkpoint</code></li>\n</ol>\n<p>　　将RDD持久化到HDFS中，与persist操作不同的是checkpoint会切断此RDD之前的依赖关系，而persist依然保留RDD的依赖关系。</p>\n<h3 id=\"1-3-5-RDD回收\"><a href=\"#1-3-5-RDD回收\" class=\"headerlink\" title=\"1.3.5 RDD回收\"></a>1.3.5 RDD回收</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Spark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the `RDD.unpersist()` method.</span><br></pre></td></tr></table></figure>\n<p>spark有一个监控线程去检测内存使用情况，当内存不足时使用LRU进行淘汰old data,也可以通过<code>RDD.unpersist()</code>方法手动移除缓存。</p>\n<h3 id=\"1-3-6-RDD保存\"><a href=\"#1-3-6-RDD保存\" class=\"headerlink\" title=\"1.3.6 RDD保存\"></a>1.3.6 RDD保存</h3><ul>\n<li><code>saveAsTextFile()</code>将RDD中的元素保存在指定目录中，这个目录位于任何Hadoop支持的存储系统中</li>\n<li><code>saveAsObjectFile()</code>将原RDD中的元素序列化成Java对象，存储在指定目录中</li>\n<li><code>saveAsSequenceFile()</code> 将键值对型RDD以SequenceFile的格式保存。键值对型RDD也可以以文本形式保存</li>\n</ul>\n<p>需要注意的是，上面的方法都把一个目录名字作为入参，然后在这个目录为每个RDD分区创建一个文件夹。这种设计不仅可以高效而且可容错。因为每个分区被存成一个文件，所以Spark在保存RDD的时候可以启动多个任务，并行执行，将数据写入文件系统中，这样也保证了写入数据的过程中可容错，一旦有一个分区写入文件的任务失败了，Spark可以在重启一个任务，重写刚才失败任务创建的文件。</p>\n<h1 id=\"2-RDD详解\"><a href=\"#2-RDD详解\" class=\"headerlink\" title=\"2. RDD详解\"></a>2. RDD详解</h1><h2 id=\"2-1-RDD分区\"><a href=\"#2-1-RDD分区\" class=\"headerlink\" title=\"2.1 RDD分区\"></a>2.1 RDD分区</h2><p><strong>RDD 表示并行计算的计算单元是使用分区（Partition）</strong>。</p>\n<h3 id=\"2-1-1-分区实现\"><a href=\"#2-1-1-分区实现\" class=\"headerlink\" title=\"2.1.1 分区实现\"></a>2.1.1 分区实现</h3><p>RDD 内部的数据集合在逻辑上和物理上被划分成多个小子集合，这样的每一个子集合我们将其称为分区，分区的个数会决定并行计算的粒度，而每一个分区数值的计算都是在一个单独的任务中进行，因此并行任务的个数，也是由 RDD（实际上是一个阶段的末 RDD，调度章节会介绍）分区的个数决定的。</p>\n<p>RDD 只是数据集的抽象，分区内部并不会存储具体的数据。<code>Partition</code> 类内包含一个 <code>index</code> 成员，表示该分区在 RDD 内的编号，<strong>通过 RDD 编号 + 分区编号可以唯一确定该分区对应的块编号</strong>，利用底层数据存储层提供的接口，就能从存储介质（如：HDFS、Memory）中提取出分区对应的数据</p>\n<p>怎么切分是<code>Partitioner</code>定义的, <code>Partitioner</code>有两个接口: <code>numPartitions</code>分区数, <code>getPartition(key: Any): Int</code>根据传入的参数确定分区号。实现了Partitioner的有：</p>\n<ol>\n<li>HashPartitioner</li>\n<li>RangePartitioner</li>\n<li>GridPartitioner</li>\n<li>PythonPartitioner</li>\n</ol>\n<p>一个RDD有了Partitioner, 就可以对当前RDD持有的数据进行划分</p>\n<h3 id=\"2-1-2-分区个数\"><a href=\"#2-1-2-分区个数\" class=\"headerlink\" title=\"2.1.2 分区个数\"></a>2.1.2 分区个数</h3><p>RDD 分区的一个分配原则是：<strong>尽可能使得分区的个数等于集群的CPU核数</strong>。</p>\n<p>RDD 可以通过创建操作或者转换操作得到。转换操作中，分区的个数会根据转换操作对应多个 RDD 之间的依赖关系确定，窄依赖子 RDD 由父 RDD 分区个数决定，Shuffle 依赖由子 RDD 分区器决定。</p>\n<p>创建操作中，程序开发者可以手动指定分区的个数，例如 <code>sc.parallelize (Array(1, 2, 3, 4, 5), 2)</code> 表示创建得到的 RDD 分区个数为 2，在没有指定分区个数的情况下，Spark 会根据集群部署模式，来确定一个分区个数默认值。</p>\n<p>对于 <code>parallelize</code> 方法，默认情况下，分区的个数会受 Apache Spark 配置参数 <code>spark.default.parallelism</code> 的影响,无论是以本地模式、Standalone 模式、Yarn 模式或者是 Mesos 模式来运行 Apache Spark，分区的默认个数等于对 <code>spark.default.parallelism</code> 的指定值，若该值未设置，则 Apache Spark 会根据不同集群模式的特征，来确定这个值。</p>\n<p>本地模式，默认分区个数等于本地机器的 CPU 核心总数（或者是用户通过 <code>local[N]</code> 参数指定分配给 Apache Spark 的核心数目),集群模式（Standalone 或者 Yarn）默认分区个数等于集群中所有核心数目的总和，或者 2，取两者中的较大值(<code>conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))</code>)</p>\n<p>对于 <code>textFile</code> 方法，默认分区个数等于 <code>min(defaultParallelism, 2)</code></p>\n<h3 id=\"2-1-3-分区内部记录个数\"><a href=\"#2-1-3-分区内部记录个数\" class=\"headerlink\" title=\"2.1.3 分区内部记录个数\"></a>2.1.3 分区内部记录个数</h3><p>分区分配的另一个分配原则是：尽可能使同一 RDD 不同分区内的记录的数量一致。</p>\n<p>对于转换操作得到的 RDD，如果是窄依赖，则分区记录数量依赖于父 RDD 中相同编号分区是如何进行数据分配的，如果是 Shuffle 依赖，则分区记录数量依赖于选择的分区器，<strong>分区器有哈希分区和范围分区</strong>。哈希分区器无法保证数据被平均分配到各个分区，而范围分区器则能做到这一点</p>\n<p>对于<code>textFile</code> 方法分区内数据的大小则是由 Hadoop API 接口 <code>FileInputFormat.getSplits</code> 方法决定（见 <code>HadoopRDD</code> 类），<strong>得到的每一个分片即为 RDD 的一个分区</strong>，分片内数据的大小会受文件大小、文件是否可分割、HDFS 中块大小等因素的影响，但总体而言会是比较均衡的分配</p>\n<h2 id=\"2-2-RDD依赖\"><a href=\"#2-2-RDD依赖\" class=\"headerlink\" title=\"2.2 RDD依赖\"></a>2.2 RDD依赖</h2><h3 id=\"2-2-1-依赖与-RDD\"><a href=\"#2-2-1-依赖与-RDD\" class=\"headerlink\" title=\"2.2.1 依赖与 RDD\"></a>2.2.1 <strong>依赖与 RDD</strong></h3><p>RDD 的容错机制是通过记录更新来实现的，且记录的是粗粒度的转换操作。在外部，我们将记录的信息称为<strong>血统（Lineage）关系</strong>，而到了源码级别，Apache Spark 记录的则是 RDD 之间的<strong>依赖（Dependency）</strong>关系。在一次转换操作中，创建得到的新 RDD 称为子 RDD，提供数据的 RDD 称为父 RDD，父 RDD 可能会存在多个，我们把子 RDD 与父 RDD 之间的关系称为依赖关系，或者可以说是子 RDD 依赖于父 RDD。</p>\n<p>依赖只保存父 RDD 信息，转换操作的其他信息，如数据处理函数，会在创建 RDD 时候，保存在新的 RDD 内。依赖在 Apache Spark 源码中的对应实现是 <code>Dependency</code> 抽象类，每个 <code>Dependency</code> 子类内部都会存储一个 <code>RDD</code> 对象，对应一个父 RDD，如果一次转换转换操作有多个父 RDD，就会对应产生多个 <code>Dependency</code> 对象，所有的 <code>Dependency</code> 对象存储在子 RDD 内部，通过遍历 RDD 内部的 <code>Dependency</code> 对象，就能获取该 RDD 所有依赖的父 RDD。</p>\n<h3 id=\"2-2-2-依赖分类\"><a href=\"#2-2-2-依赖分类\" class=\"headerlink\" title=\"2.2.2 依赖分类\"></a>2.2.2 依赖分类</h3><p>Apache Spark 将依赖进一步分为两类，分别是<strong>窄依赖（Narrow Dependency）</strong>和 <strong>Shuffle 依赖（Shuffle Dependency，在部分文献中也被称为 Wide Dependency，即宽依赖）</strong>。</p>\n<p>窄依赖中，父 RDD 中的一个分区最多只会被子 RDD 中的一个分区使用，换句话说，父 RDD 中，一个分区内的数据是不能被分割的，必须整个交付给子 RDD 中的一个分区。</p>\n<p>窄依赖可进一步分类成一对一依赖和范围依赖，对应实现分别是 <code>OneToOneDependency</code> 类和<code>RangeDependency</code> 类。一对一依赖表示子 RDD 分区的编号与父 RDD 分区的编号完全一致的情况，若两个 RDD 之间存在着一对一依赖，则子 RDD 的分区个数、分区内记录的个数都将继承自父 RDD。范围依赖是依赖关系中的一个特例，只被用于表示 <code>UnionRDD</code> 与父 RDD 之间的依赖关系。相比一对一依赖，除了第一个父 RDD，其他父 RDD 和子 RDD 的分区编号不再一致，Apache Spark 统一将<code>unionRDD</code>与父 RDD 之间（包含第一个 RDD）的关系都叫做范围依赖。</p>\n<p>依赖类图：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">graph TD</span><br><span class=\"line\">A[Dependency&lt;br&gt;依赖关系基类]--- B[NarrowDependency&lt;br&gt;窄依赖]</span><br><span class=\"line\">A---C[ShuffleDenpendency&lt;br&gt;shuffle依赖]</span><br><span class=\"line\">B---D[OneToOneDependency&lt;br&gt;一对一依赖]</span><br><span class=\"line\">B---E[RangeDependency&lt;br&gt;范围依赖]</span><br></pre></td></tr></table></figure>\n<p>下图展示了几类常见的窄依赖及其对应的转换操作。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/narrowDependency.png\" alt=\"窄依赖\"></p>\n<p>Shuffle 依赖中，父 RDD 中的分区可能会被多个子 RDD 分区使用。因为父 RDD 中一个分区内的数据会被分割，发送给子 RDD 的所有分区，因此 Shuffle 依赖也意味着父 RDD 与子 RDD 之间存在着 Shuffle 过程。下图展示了几类常见的Shuffle依赖及其对应的转换操作。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/wideDependency.png\" alt=\"窄依赖\"></p>\n<p>Shuffle 依赖的对应实现为<code>ShuffleDependency</code> 类,其实现比较复杂，主要通过以下成员完成：</p>\n<ul>\n<li><code>rdd</code>：用于表示 Shuffle 依赖中，子 RDD 所依赖的父 RDD。</li>\n<li><code>shuffleId</code>：Shuffle 的 ID 编号，在一个 Spark 应用程序中，每个 Shuffle 的编号都是唯一的。</li>\n<li><code>shuffleHandle</code>：Shuffle 句柄，<code>ShuffleHandle</code> 内部一般包含 Shuffle ID、Mapper 的个数以及对应的 Shuffle 依赖，在执行 <code>ShuffleMapTask</code> 时候，任务可以通过 <code>ShuffleManager</code> 获取得到该句柄，并进一步得到 Shuffle 相关信息。</li>\n<li><code>partitioner</code>：分区器，用于决定 Shuffle 过程中 Reducer 的个数（实际上是子 RDD 的分区个数）以及 Map 端的一条数据记录应该分配给哪一个 Reducer，也可以被用在 <code>CoGroupedRDD</code> 中，确定父 RDD 与子 RDD 之间的依赖关系类型。</li>\n<li><code>serializer</code>：序列化器。用于 Shuffle 过程中 Map 端数据的序列化和 Reduce 端数据的反序列化。</li>\n<li><code>KeyOrdering</code>：键值排序策略，用于决定子 RDD 的一个分区内，如何根据键值对 类型数据记录进行排序。</li>\n<li><code>Aggregator</code>：聚合器，内部包含了多个聚合函数，比较重要的函数有 <code>createCombiner：V =&gt; C</code>，<code>mergeValue: (C, V) =&gt; C</code> 以及 <code>mergeCombiners: (C, C) =&gt; C</code>。例如，对于 <code>groupByKey</code> 操作，<code>createCombiner</code> 表示把第一个元素放入到集合中，<code>mergeValue</code> 表示一个元素添加到集合中，<code>mergeCombiners</code> 表示把两个集合进行合并。这些函数被用于 Shuffle 过程中数据的聚合。</li>\n<li><code>mapSideCombine</code>：用于指定 Shuffle 过程中是否需要在 map 端进行 combine 操作。如果指定该值为 <code>true</code>，由于 combine 操作需要用到聚合器中的相关聚合函数，因此 <code>Aggregator</code> 不能为空，否则 Apache Spark 会抛出异常。例如：<code>groupByKey</code> 转换操作对应的<code>ShuffleDependency</code>中，<code>mapSideCombine = false</code>，而 <code>reduceByKey</code> 转换操作中，<code>mapSideCombine = true</code>。</li>\n</ul>\n<p>依赖关系是两个 RDD 之间的依赖，因此若一次转换操作中父 RDD 有多个，则可能会同时包含窄依赖和 Shuffle 依赖，下图所示的 <code>Join</code> 操作，RDD a 和 RDD c 采用了相同的分区器，两个 RDD 之间是窄依赖，Rdd b 的分区器与 RDD c 不同，因此它们之间是 Shuffle 依赖，具体实现可参见 <code>CoGroupedRDD</code> 类的 <code>getDependencies</code> 方法。这里能够再次发现：<strong>一个依赖对应的是两个 RDD，而不是一次转换操作。</strong></p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/mixDependency.png\" alt=\"窄依赖\"></p>\n<h3 id=\"2-2-3-依赖与容错机制\"><a href=\"#2-2-3-依赖与容错机制\" class=\"headerlink\" title=\"2.2.3 依赖与容错机制\"></a>2.2.3 依赖与容错机制</h3><p>介绍完依赖的类别和实现之后，回过头来，从分区的角度继续探究 Apache Spark 是如何通过依赖关系来实现容错机制的。下图给出了一张依赖关系图，<code>fileRDD</code> 经历了 <code>map</code>、<code>reduce</code> 以及<code>filter</code> 三次转换操作，得到了最终的 RDD，其中，<code>map</code>、<code>filter</code> 操作对应的依赖为窄依赖，<code>reduce</code> 操作对应的是 Shuffle 依赖。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/fault-tolrarnt0.png\" alt=\"fault-tolrarnt0\"></p>\n<p>假设最终 RDD 第一块分区内的数据因为某些原因丢失了，由于 RDD 内的每一个分区都会记录其对应的父 RDD 分区的信息，因此沿着下图所示的依赖关系往回走，我们就能找到该分区数据最终来源于 <code>fileRDD</code> 的所有分区，再沿着依赖关系往后计算路径中的每一个分区数据，即可得到丢失的分区数据。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/fault-tolrarnt1.png\" alt=\"fault-tolrarnt0\"></p>\n<p>这个例子并不是特别严谨，按照我们的思维，只有执行了持久化，存储在存储介质中的 RDD 分区才会出现数据丢失的情况，但是上例中最终的 RDD 并没有执行持久化操作。事实上，Apache Spark 将没有被持久化数据重新被计算，以及持久化的数据第一次被计算，也等价视为数据“丢失”，在 1.7 节中我们会看到这一点。</p>\n<h3 id=\"2-2-4-依赖与并行计算\"><a href=\"#2-2-4-依赖与并行计算\" class=\"headerlink\" title=\"2.2.4 依赖与并行计算\"></a>2.2.4 依赖与并行计算</h3><p>在上一节中我们看到，在 RDD 中，可以通过<strong>计算链（Computing Chain）</strong>来计算某个 RDD 分区内的数据，我们也知道分区是并行计算的基本单位，这时候可能会有一种想法：能否把 RDD 每个分区内数据的计算当成一个并行任务，每个并行任务包含一个计算链，将一个计算链交付给一个 CPU 核心去执行，集群中的 CPU 核心一起把 RDD 内的所有分区计算出来。</p>\n<p>答案是可以，这得益于 RDD 内部分区的数据依赖相互之间并不会干扰，而 Apache Spark 也是这么做的，但在实现过程中，仍有很多实际问题需要去考虑。进一步观察窄依赖、Shuffle 依赖在做并行计算时候的异同点。</p>\n<p>先来看下方左侧的依赖图，依赖图中所有的依赖关系都是窄依赖（包括一对一依赖和范围依赖），可以看到，不仅计算链是独立不干扰的（所以可以并行计算），所有计算链内的每个分区单元的计算工作也不会发生重复，如右侧的图所示。这意味着除非执行了持久化操作，否则计算过程中产生的中间数据我们没有必要保留 —— 因为当前分区的数据只会给计算链中的下一个分区使用，而不用专门保留给其他计算链使用。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/paralle1.png\" alt=\"paralle1\"></p>\n<p>再来观察 Shuffle 依赖的计算链，如图下方左侧的图中，既有窄依赖，又有 Shuffle 依赖，由于 Shuffle 依赖中，子 RDD 一个分区的数据依赖于父 RDD 内所有分区的数据，当我们想计算末 RDD 中一个分区的数据时，Shuffle 依赖处需要把父 RDD 所有分区的数据计算出来，如右侧的图所示（紫色表示最后两个分区计算链共同经过的地方） —— 而这些数据，在计算末 RDD 另外一个分区的数据时候，同样会被用到。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/paralle2.png\" alt=\"paralle2\"></p>\n<p>如果我们做到计算链的并行计算的话，这就意味着，要么 Shuffle 依赖处父 RDD 的数据在每次需要使用的时候都重复计算一遍，要么想办法把父 RDD 数据保存起来，提供给其余分区的数据计算使用。</p>\n<p>Apache Spark 采用的是第二种办法，但保存数据的方法可能与想象中的会有所不同，<strong>Spark 把计算链从 Shuffle 依赖处断开</strong>，划分成不同的<strong>阶段（Stage）</strong>，阶段之间存在依赖关系（其实就是 Shuffle 依赖），从而可以构建一张不同阶段之间的<strong>有向无环图（DAG）</strong>。</p>\n<h2 id=\"2-3-RDD-计算函数\"><a href=\"#2-3-RDD-计算函数\" class=\"headerlink\" title=\"2.3 RDD 计算函数\"></a>2.3 RDD 计算函数</h2><p>todo compute</p>\n<h2 id=\"2-4-RDD-分区器\"><a href=\"#2-4-RDD-分区器\" class=\"headerlink\" title=\"2.4 RDD 分区器\"></a>2.4 RDD 分区器</h2><p>todo partiner</p>\n<h2 id=\"2-5-RDD-血缘\"><a href=\"#2-5-RDD-血缘\" class=\"headerlink\" title=\"2.5 RDD 血缘\"></a>2.5 RDD 血缘</h2><p>todo lineage</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations</a></p>\n<p><a href=\"https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html\" target=\"_blank\" rel=\"noopener\">https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html</a></p>\n<p><a href=\"http://spark.apachecn.org/paper/zh/spark-rdd.html\" target=\"_blank\" rel=\"noopener\">http://spark.apachecn.org/paper/zh/spark-rdd.html</a></p>\n","categories":["spark"],"tags":["spark","RDD"]},{"title":"spark集群环境搭建","url":"https://sustcoder.github.io/2018/09/28/spark on yarn/","content":"<h1 id=\"spark-on-yarn\"><a href=\"#spark-on-yarn\" class=\"headerlink\" title=\"spark on yarn\"></a>spark on yarn</h1><h1 id=\"软件安装\"><a href=\"#软件安装\" class=\"headerlink\" title=\"软件安装\"></a>软件安装</h1><h2 id=\"当前环境\"><a href=\"#当前环境\" class=\"headerlink\" title=\"当前环境\"></a>当前环境</h2><p>hadoop环境搭建参考：<a href=\"https://my.oschina.net/freelili/blog/1834706\" target=\"_blank\" rel=\"noopener\">hadoop集群安装</a></p>\n<ul>\n<li>hadoop2.6</li>\n<li>spark-2.2.0-bin-hadoop2.6.tgz</li>\n<li>scala-2.11.12</li>\n</ul>\n<h2 id=\"安装scala\"><a href=\"#安装scala\" class=\"headerlink\" title=\"安装scala\"></a>安装scala</h2><blockquote>\n<p>tar -zxvf scala-2.11.12.tgz</p>\n<p>vi /etc/profile</p>\n</blockquote>\n<p>添加以下内容</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">export SCALA_HOME=/home/hadoop/app/scala</span><br><span class=\"line\">export PATH=$PATH:$SCALA_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>使配置生效</p>\n<blockquote>\n<p>source /etc/profile</p>\n</blockquote>\n<p>查看scala版本号</p>\n<blockquote>\n<p>scala -version</p>\n</blockquote>\n<p>注意： <strong>用root账户修改完变量后，需要重新打开ssh链接，配置才能生效</strong></p>\n<h2 id=\"安装spark\"><a href=\"#安装spark\" class=\"headerlink\" title=\"安装spark\"></a>安装spark</h2><blockquote>\n<p>tar -zvf spark-2.2.0-bin-without-hadoop.tgz</p>\n<p>vi /etc/profile</p>\n</blockquote>\n<p>添加以下内容</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">export SPARK_HOME=/home/hadoop/app/spark2.2.0</span><br><span class=\"line\">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>修改spark环境变量</p>\n<blockquote>\n<p>cp spark-env.sh.template spark-env.sh</p>\n<p>vi conf/spark-evn.sh</p>\n</blockquote>\n<p>添加以下内容</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">SPARK_DRIVER_MEMORY=512m</span><br><span class=\"line\">SPARK_DIST_CLASSPATH=$(/home/hadoop/app/hadoop-2.6.0/bin/hadoop classpath)</span><br><span class=\"line\">SPARK_LOCAL_DIRS=/home/hadoop/app/spark2.2.0</span><br><span class=\"line\">export SPARK_MASTER_IP=192.168.10.125</span><br><span class=\"line\"></span><br><span class=\"line\">export JAVA_HOME=/home/app/jdk8</span><br><span class=\"line\">export SCALA_HOME=/home/hadoop/app/scala</span><br><span class=\"line\">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class=\"line\">export HADOOP_CONF_DIR=/home/hadoop/app/hadoop/etc/hadoop</span><br><span class=\"line\">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SCALA_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>使配置变量生效</p>\n<blockquote>\n<p>source /etc/profile</p>\n</blockquote>\n<p>配置slaves</p>\n<blockquote>\n<p>vi slaves</p>\n</blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">node3</span><br><span class=\"line\">node4</span><br></pre></td></tr></table></figure>\n<p>将配置文件下发到从节点</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">scp slaves hadoop@node3:/home/hadoop/app/spark2.2.0/conf</span><br><span class=\"line\">scp slaves hadoop@node4:/home/hadoop/app/spark2.2.0/conf</span><br><span class=\"line\">scp spark-env.sh hadoop@node3:/home/hadoop/app/spark2.2.0/conf</span><br><span class=\"line\">scp spark-env.sh hadoop@node4:/home/hadoop/app/spark2.2.0/conf</span><br></pre></td></tr></table></figure>\n<h2 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h2><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动zookeeper,可能存在选举延迟，可多执行几次./zkServer.sh status查看启动结果</span></span><br><span class=\"line\">./runRemoteCmd.sh \"/home/hadoop/app/zookeeper/bin/zkServer.sh start\" zookeeper</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2节点上执行,启动HDFS</span></span><br><span class=\"line\">sbin/start-dfs.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2节点上执行,启动YARN</span></span><br><span class=\"line\">sbin/start-yarn.sh </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node4节点上面执行,启动resourcemanager</span></span><br><span class=\"line\">sbin/yarn-daemon.sh start resourcemanager</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2上启动spark</span></span><br><span class=\"line\">sbin/start-all.sh</span><br></pre></td></tr></table></figure>\n<h2 id=\"关闭\"><a href=\"#关闭\" class=\"headerlink\" title=\"关闭\"></a>关闭</h2><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 关闭spark</span></span><br><span class=\"line\">sbin/stop-all.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node4上执行</span></span><br><span class=\"line\">sbin/yarn-daemon.sh stop resourcemanager</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2上执行</span></span><br><span class=\"line\">sbin/stop-yarn.sh </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2上执行</span></span><br><span class=\"line\">sbin/stop-dfs.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 关闭zookeeper</span></span><br><span class=\"line\">runRemoteCmd.sh \"/home/hadoop/app/zookeeper/bin/zkServer.sh stop\" zookeeper</span><br></pre></td></tr></table></figure>\n<p>查看启动情况</p>\n<blockquote>\n<p>jps</p>\n</blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hdfs进程</span></span><br><span class=\"line\">1661 NameNode</span><br><span class=\"line\">1934 SecondaryNameNode</span><br><span class=\"line\">1750 DataNode</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> yarn进程</span></span><br><span class=\"line\">8395 ResourceManager</span><br><span class=\"line\">7725 NameNode</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> namenode HA</span></span><br><span class=\"line\">8256 DFSZKFailoverController</span><br><span class=\"line\">7985 JournalNode</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> zookeeper进程</span></span><br><span class=\"line\">1286 QuorumPeerMain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> spark进程</span></span><br><span class=\"line\">2551 Master</span><br><span class=\"line\">2641 Worker</span><br></pre></td></tr></table></figure>\n<h2 id=\"管理界面\"><a href=\"#管理界面\" class=\"headerlink\" title=\"管理界面\"></a>管理界面</h2><figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hadoop</span>:  http://node2:8088/</span><br><span class=\"line\"><span class=\"attribute\">nameNode</span>: http://node2:50070/</span><br><span class=\"line\"><span class=\"attribute\">nodeManager</span>:  http://node2:8042/</span><br><span class=\"line\">spark master:  http://node2:8080/</span><br><span class=\"line\">spark worker:  http://node2:8081/</span><br><span class=\"line\">spark jobs:  http://node2:4040/</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行示例\"><a href=\"#运行示例\" class=\"headerlink\" title=\"运行示例\"></a>运行示例</h2><p><strong>Spark-shell</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">vi test.text# 在文件中添加 hello spark</span><br><span class=\"line\">hdfs dfs -mkdir /test # 创建文件夹</span><br><span class=\"line\">hdfs dfs -put test.txt /test # 上传文件到hdfs</span><br><span class=\"line\">hdfs dfs -ls /test # 查看是否上传成功</span><br><span class=\"line\">./bin/spark-shell</span><br><span class=\"line\">sc.textFile(\"hdfs://node2:9000/test/test.txt\") # 从hdfs上获取文件</span><br><span class=\"line\">sc.first() # 获取文件的第一行数据</span><br></pre></td></tr></table></figure>\n<p><strong>Run application locally(Local模式)</strong></p>\n<blockquote>\n<p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master local[4] /home/hadoop/app/spark2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar</p>\n</blockquote>\n<p><strong>Run on a Spark standalone cluster(Standalone模式,使用Spark自带的简单集群管理器)</strong></p>\n<blockquote>\n<p>./bin/spark-submit \\<br>–class org.apache.spark.examples.SparkPi \\<br>–master spark://node2:7077 \\<br>–executor-memory 512m \\<br>–total-executor-cores 4 \\<br>/home/hadoop/app/spark2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar 10</p>\n</blockquote>\n<p><strong>Run on yarn(YARN模式，使用YARN作为集群管理器)</strong></p>\n<blockquote>\n<p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn –deploy-mode client examples/jars/spark-examples*.jar 10</p>\n</blockquote>\n<h1 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h1><blockquote>\n<p>HADOOP_CONF_DIR=/home/hadoop/app/hadoop/etc/hadoop</p>\n</blockquote>\n<p>确保 <code>HADOOP_CONF_DIR</code> 或者 <code>YARN_CONF_DIR</code> 指向包含 Hadoop 集群的（客户端）配置文件的目录。这些配置被用于写入 HDFS 并连接到 YARN ResourceManager 。此目录中包含的配置将被分发到 YARN 集群，以便 application（应用程序）使用的所有的所有 containers（容器）都使用相同的配置。如果配置引用了 Java 系统属性或者未由 YARN 管理的环境变量，则还应在 Spark 应用程序的配置（driver（驱动程序），executors（执行器），和在客户端模式下运行时的 AM ）。</p>\n<blockquote>\n<p> SPARK_DIST_CLASSPATH=$(/home/hadoop/app/hadoop-2.6.0/bin/hadoop classpath)</p>\n</blockquote>\n<p>Pre-build with user-provided Hadoop: 属于“Hadoop free”版,不包含hadoop的jar等，这样，下载到的Spark，可应用到任意Hadoop 版本。但是需要在spark的spar-evn.sh中指定配置hadoop的安装路径。</p>\n<blockquote>\n<p>spark-submit</p>\n</blockquote>\n<p>如果用户的应用程序被打包好了，它可以使用 <code>bin/spark-submit</code> 脚本来启动。这个脚本负责设置 Spark 和它的依赖的 classpath，并且可以支持 Spark 所支持的不同的 Cluster Manager 以及 deploy mode（部署模式）:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/spark-submit \\</span><br><span class=\"line\">  --class &lt;main-class&gt; \\</span><br><span class=\"line\">  --master &lt;master-url&gt; \\</span><br><span class=\"line\">  --deploy-mode &lt;deploy-mode&gt; \\</span><br><span class=\"line\">  --conf &lt;key&gt;=&lt;value&gt; \\</span><br><span class=\"line\">  ... # other options</span><br><span class=\"line\">  &lt;application-jar&gt; \\</span><br><span class=\"line\">  [application-arguments]</span><br></pre></td></tr></table></figure>\n<p>一些常用的 options（选项）有 :</p>\n<ul>\n<li><code>--class</code>: 您的应用程序的入口点（例如。 <code>org.apache.spark.examples.SparkPi</code>)</li>\n<li><code>--master</code>: standalone模式下是集群的 master URL，on yarn模式下值是<code>yarn</code></li>\n<li><code>--deploy-mode</code>: 是在 worker 节点(<code>cluster</code>) 上还是在本地作为一个外部的客户端(<code>client</code>) 部署您的 driver(默认: <code>client</code>)</li>\n<li><code>--conf</code>: 按照 key=value 格式任意的 Spark 配置属性。对于包含空格的 value（值）使用引号包 “key=value” 起来。</li>\n<li><code>application-jar</code>: 包括您的应用以及所有依赖的一个打包的 Jar 的路径。该 URL 在您的集群上必须是全局可见的，例如，一个 <code>hdfs://</code> path 或者一个 <code>file://</code> 在所有节点是可见的。</li>\n<li><code>application-arguments</code>: 传递到您的 main class 的 main 方法的参数，如果有的话。</li>\n</ul>\n<h1 id=\"异常处理\"><a href=\"#异常处理\" class=\"headerlink\" title=\"异常处理\"></a>异常处理</h1><p><strong>执行脚本</strong></p>\n<blockquote>\n<p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn –deploy-mode client /home/hadoop/app/spark2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar</p>\n</blockquote>\n<p><strong>报错信息一</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">Application application_1537990303043_0001 failed 2 times due to AM Container for appattempt_1537990303043_0001_000002 exited with  exitCode: -103</span><br><span class=\"line\">Diagnostics: </span><br><span class=\"line\">Container [pid=2344,containerID=container_1537990303043_0001_02_000001] is running beyond virtual memory limits. </span><br><span class=\"line\">Current usage: 74.0 MB of 1 GB physical memory used; </span><br><span class=\"line\">2.2 GB of 2.1 GB virtual memory used. Killing container.</span><br></pre></td></tr></table></figure>\n<p><strong>问题原因</strong></p>\n<p>虚拟机物理内存设置的是1G，则对应虚拟内存最大为1*2.1=2.1GB,实际使用了2.2[此处疑问：为什么就使用了2.2，单个任务默认分配1024M，加上一个任务的Container默认1024M导致吗？]，所以需要扩大虚拟内存的比例，或者限制container和task的大小，或者关闭掉对虚拟内存的检测。</p>\n<p><strong>解决方法</strong></p>\n<p>修改<code>yarn-site.xml</code>文件，新增以下内容，详情原因请参考：<a href=\"https://sustcoder.github.io/2018/09/27/YARN%20%E5%86%85%E5%AD%98%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3/\">YARN 内存参数详解</a></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>虚拟内存和物理内存比率，默认为2.1<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>不检查虚拟内存，默认为true<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p><strong>报错二</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Exception in thread <span class=\"string\">\"main\"</span> org.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.</span><br><span class=\"line\">\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:<span class=\"number\">85</span>)</span><br><span class=\"line\">\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:<span class=\"number\">62</span>)</span><br><span class=\"line\">\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:<span class=\"number\">173</span>)</span><br><span class=\"line\">\tat org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:<span class=\"number\">509</span>)</span><br><span class=\"line\">\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:<span class=\"number\">2509</span>)</span><br><span class=\"line\">\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$<span class=\"number\">6</span>.apply(SparkSession.scala:<span class=\"number\">909</span>)</span><br><span class=\"line\">\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$<span class=\"number\">6</span>.apply(SparkSession.scala:<span class=\"number\">901</span>)</span><br><span class=\"line\">\tat scala.Option.getOrElse(Option.scala:<span class=\"number\">121</span>)</span><br><span class=\"line\">\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:<span class=\"number\">901</span>)</span><br><span class=\"line\">\tat org.apache.spark.examples.SparkPi$.main(SparkPi.scala:<span class=\"number\">31</span>)</span><br><span class=\"line\">\tat org.apache.spark.examples.SparkPi.main(SparkPi.scala)</span><br><span class=\"line\">\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class=\"line\">\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class=\"number\">43</span>)</span><br><span class=\"line\">\tat java.lang.reflect.Method.invoke(Method.java:<span class=\"number\">498</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:<span class=\"number\">755</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$<span class=\"number\">1</span>(SparkSubmit.scala:<span class=\"number\">180</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:<span class=\"number\">205</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:<span class=\"number\">119</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class=\"line\"><span class=\"number\">18</span>/<span class=\"number\">09</span>/<span class=\"number\">04</span> <span class=\"number\">17</span>:<span class=\"number\">01</span>:<span class=\"number\">43</span> INFO util.ShutdownHookManager: Shutdown hook called</span><br></pre></td></tr></table></figure>\n<p><strong>问题原因</strong></p>\n<p>以上报错是在伪集群上运行时报错信息，具体报错原因未知，在切换到真正的集群环境后无此报错</p>\n<h1 id=\"配置链接\"><a href=\"#配置链接\" class=\"headerlink\" title=\"配置链接\"></a>配置链接</h1><p><strong>hadoop</strong></p>\n<ul>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/core-site.xml\" target=\"_blank\" rel=\"noopener\">core-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/hdfs-site.xml\" target=\"_blank\" rel=\"noopener\">hdfs-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/mapred-site.xml\" target=\"_blank\" rel=\"noopener\">mapred-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/yarn-site.xml\" target=\"_blank\" rel=\"noopener\">yarn-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/hadoop-env.sh\" target=\"_blank\" rel=\"noopener\">hadoop-env.sh</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/masters\" target=\"_blank\" rel=\"noopener\">masters</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/slaves\" target=\"_blank\" rel=\"noopener\">slaves</a></p>\n</li>\n</ul>\n<p><strong>zookeeper</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/zookeeper/zoo.cfg\" target=\"_blank\" rel=\"noopener\">zoo.cfg</a></li>\n</ul>\n<p><strong>spark</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/spark/spark-env.sh\" target=\"_blank\" rel=\"noopener\">spark-env.sh</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018//hadoop/conf/spark/slaves\" target=\"_blank\" rel=\"noopener\">slaves</a></li>\n</ul>\n<p><strong>env</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/env/hosts\" target=\"_blank\" rel=\"noopener\">hosts</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/env/profile\" target=\"_blank\" rel=\"noopener\">profile</a></li>\n</ul>\n<p><strong>tools</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/tools/deploy.conf\" target=\"_blank\" rel=\"noopener\">deploy.conf</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/tools/deploy.sh\" target=\"_blank\" rel=\"noopener\">deploy.sh</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/tools/runRemoteCmd.sh\" target=\"_blank\" rel=\"noopener\">runRemoteCmd.sh</a></li>\n</ul>\n","categories":["spark"],"tags":["spark","环境"]},{"title":"YARN 内存参数详解","url":"https://sustcoder.github.io/2018/09/27/YARN 内存参数详解/","content":"<h2 id=\"yarn组件依赖关系\"><a href=\"#yarn组件依赖关系\" class=\"headerlink\" title=\"yarn组件依赖关系\"></a>yarn组件依赖关系</h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/structure/yarn_structure.png\" alt=\"yarn结果图\"></p>\n<p>yarn主要由两部分组成，ResourceManager和NodeManger。NodeManager里面包含多个Container，每个Container里可以运行多个task，比如MapTask和ReduceTask等。ApplicationMaster也是在Container中运行。</p>\n<p>在YARN中，资源管理由ResourceManager和NodeManager共同完成，其中，<strong>ResourceManager中的调度器负责资源的分配，而NodeManager则负责资源的供给和隔离</strong>。ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的“资源调度”）后，NodeManager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。</p>\n<p>关于yarn的详细介绍可参考：<a href=\"https://my.oschina.net/freelili/blog/1853714\" target=\"_blank\" rel=\"noopener\">yarn的架构及作业调度</a></p>\n<h2 id=\"内存相关参数\"><a href=\"#内存相关参数\" class=\"headerlink\" title=\"内存相关参数\"></a>内存相关参数</h2><p><a href=\"https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml\" target=\"_blank\" rel=\"noopener\">yarn-site.xml官网参数表及其解释</a></p>\n<table>\n<thead>\n<tr>\n<th>配置文件</th>\n<th>配置设置</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.resource.memory-mb</td>\n<td>-1</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.vmem-pmem-ratio</td>\n<td>2.1</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.vmem-check-enabled</td>\n<td>true</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.pmem-check-enabled</td>\n<td>true</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.minimum-allocation-mb</td>\n<td>1024MB</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.maximum-allocation-mb</td>\n<td>8192 MB</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.resource.cpu-vcores</td>\n<td>8</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.minimum-allocation-vcores</td>\n<td>1</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.maximum-allocation-vcores</td>\n<td>32</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>yarn.app.mapreduce.am.resource.mb</td>\n<td>1536 MB</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>yarn.app.mapreduce.am.command-opts</td>\n<td>-Xmx1024m</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.map.memory.mb</td>\n<td>1024 MB</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.reduce.memory.mb</td>\n<td>1024 MB</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.map.java.opts</td>\n<td>最新版已经去掉</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.reduce.java.opts</td>\n<td>最新版已经去掉</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"参数解释\"><a href=\"#参数解释\" class=\"headerlink\" title=\"参数解释\"></a>参数解释</h2><h3 id=\"NodeManager\"><a href=\"#NodeManager\" class=\"headerlink\" title=\"NodeManager\"></a>NodeManager</h3><ul>\n<li><code>yarn.nodemanager.resource.memory-mb</code>：节点最大可用内存，如果值为-1且<code>yarn.nodemanager.resource.detect-hardware-capabilities</code>值为<code>true</code>，则根据系统内存自动计算，否则默认值为8192M</li>\n<li><code>yarn.nodemanager.vmem-pmem-ratio</code>：虚拟内存率，Container 的虚拟内存大小的限制，每使用1MB物理内存，最多可用的虚拟内存数</li>\n<li><code>yarn.nodemanager.pmem-check-enabled</code>：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true;</li>\n<li><code>yarn.nodemanager.vmem-check-enabled</code>：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true</li>\n</ul>\n<h3 id=\"ResourceManager\"><a href=\"#ResourceManager\" class=\"headerlink\" title=\"ResourceManager\"></a>ResourceManager</h3><ul>\n<li><code>yarn.scheduler.minimum-allocation-mb</code>：单个任务可申请的最少物理内存量，默认是1024（MB），如果一个任务申请的物理内存量少于该值，则该对应的值改为这个数</li>\n<li><code>yarn.scheduler.maximum-allocation-mb</code>：单个任务可申请的最多物理内存量，默认是8192（MB）。</li>\n</ul>\n<h3 id=\"ApplicationMaster\"><a href=\"#ApplicationMaster\" class=\"headerlink\" title=\"ApplicationMaster\"></a>ApplicationMaster</h3><ul>\n<li><code>mapreduce.map.memory.mb</code>：分配给 Map Container的内存大小，默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。</li>\n<li><code>mapreduce.reduce.memory.mb</code>：分配给 Reduce Container的内存大小，默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。</li>\n<li><code>mapreduce.map.java.opts</code>：运行 Map 任务的 jvm 参数，如 -Xmx，-Xms 等选项</li>\n<li><code>mapreduce.reduce.java.opts</code>：运行 Reduce 任务的 jvm 参数，如-Xmx，-Xms等选项</li>\n</ul>\n<h3 id=\"CPU-资源\"><a href=\"#CPU-资源\" class=\"headerlink\" title=\"CPU 资源\"></a>CPU 资源</h3><ul>\n<li><code>yarn.nodemanager.resource.cpu-vcores</code>：该节点上 YARN 可使用的虚拟 CPU 个数，如果值是-1且<code>yarn.nodemanager.resource.detect-hardware-capabilities</code>值为·true`，则其cpu数目根据系统而定，否则默认值为8</li>\n<li><code>yarn.scheduler.minimum-allocation-vcores</code>：单个任务可申请的最小虚拟CPU个数, 默认是1</li>\n<li><code>yarn.scheduler.maximum-allocation-vcores</code>：单个任务可申请的最多虚拟CPU个数，默认是4</li>\n</ul>\n<h2 id=\"Killing-Container\"><a href=\"#Killing-Container\" class=\"headerlink\" title=\"Killing Container\"></a>Killing Container</h2><p>在本地虚拟机上跑task时报错如下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">Application application_1537990303043_0001 failed 2 times due to AM Container for appattempt_1537990303043_0001_000002 exited with  exitCode: -103</span><br><span class=\"line\">Diagnostics: </span><br><span class=\"line\">Container [pid=2344,containerID=container_1537990303043_0001_02_000001] is running beyond virtual memory limits. </span><br><span class=\"line\">Current usage: 74.0 MB of 1 GB physical memory used; </span><br><span class=\"line\">2.2 GB of 2.1 GB virtual memory used. Killing container.</span><br></pre></td></tr></table></figure>\n<p>虚拟机物理内存设置的是1G，则对应虚拟内存最大为1*2.1=2.1GB,实际使用了2.2[此处疑问：为什么就使用了2.2，单个任务默认分配1024M，加上一个任务的Container默认1024M导致吗？]，所以需要扩大虚拟内存的比例，或者限制container和task的大小，或者关闭掉对虚拟内存的检测。</p>\n<h3 id=\"yarn-site-xml\"><a href=\"#yarn-site-xml\" class=\"headerlink\" title=\"yarn-site.xml\"></a>yarn-site.xml</h3><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">&lt;property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;value&gt;256&lt;/value&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;description&gt;每个container可申请最小内存&lt;/description&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;/property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;value&gt;512&lt;/value&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;description&gt;每个container可申请最大内存&lt;/description&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;/property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>虚拟内存和物理内存比率，默认为2.1<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>不检查虚拟内存，默认为true<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"混淆点\"><a href=\"#混淆点\" class=\"headerlink\" title=\"混淆点\"></a>混淆点</h2><p><code>yarn.scheduler.minimum-allocation-mb</code>和<code>yarn.scheduler.maximum-allocation-mb</code>这两个参数不能限制任务的真正大小？？？</p>\n<p>这两个参数是管理员用来设置用户能够设置的每个任务可申请的最小和最大内存资源。具体每个任务到底申请多少，由各个应用程序单独设置，如果是mapreduce程序，可以map task申请的资源可通过mapreduce.map.memory.mb指定，reduce task的资源可通过mapreduce.reduce.memory.mb指定，这两个参数最大不能超过yarn.scheduler.maximum-allocation-mb</p>\n","categories":["spark"],"tags":["spark","环境"]},{"title":"spark数据倾斜调优[转]","url":"https://sustcoder.github.io/2018/09/18/Spark数据倾斜调优/","content":"<h1 id=\"一、数据倾斜发生的原理\"><a href=\"#一、数据倾斜发生的原理\" class=\"headerlink\" title=\"一、数据倾斜发生的原理\"></a>一、数据倾斜发生的原理</h1><ul>\n<li><p><strong>原理</strong>：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。数据倾斜只会发生在shuffle过程中。常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p>\n</li>\n<li><p><strong>表现</strong>：Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p>\n</li>\n<li><p><strong>定位</strong>：</p>\n<ul>\n<li><p>确定数据倾斜发生在第几个stage中。</p>\n<p>可以通过Spark Web UI来查看当前运行到了第几个stage。并深入看一下当前这个stage各个task分配的数据量及运行时间</p>\n</li>\n<li><p>根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分。</p>\n</li>\n<li><p>分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"二、数据倾斜解决方案\"><a href=\"#二、数据倾斜解决方案\" class=\"headerlink\" title=\"二、数据倾斜解决方案\"></a>二、数据倾斜解决方案</h1><h2 id=\"方案一：使用Hive-ETL预处理数据\"><a href=\"#方案一：使用Hive-ETL预处理数据\" class=\"headerlink\" title=\"方案一：使用Hive ETL预处理数据\"></a>方案一：使用Hive ETL预处理数据</h2><ul>\n<li><strong>方案适用场景</strong>：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</li>\n<li><strong>方案实现思路</strong>：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</li>\n<li><strong>方案实现原理</strong>：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</li>\n<li><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</li>\n<li><strong>方案缺点</strong>：治标不治本，Hive ETL中还是会发生数据倾斜。</li>\n<li><strong>方案实践经验</strong>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</li>\n<li><strong>项目实践经验</strong>：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</li>\n</ul>\n<h2 id=\"方案二：过滤少数导致倾斜的key\"><a href=\"#方案二：过滤少数导致倾斜的key\" class=\"headerlink\" title=\"方案二：过滤少数导致倾斜的key\"></a>方案二：过滤少数导致倾斜的key</h2><ul>\n<li><strong>方案适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</li>\n<li><strong>方案实现思路</strong>：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</li>\n<li><strong>方案实现原理</strong>：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</li>\n<li><strong>方案优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</li>\n<li><strong>方案缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</li>\n<li><strong>方案实践经验</strong>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</li>\n</ul>\n<h2 id=\"方案三：提高shuffle操作的并行度\"><a href=\"#方案三：提高shuffle操作的并行度\" class=\"headerlink\" title=\"方案三：提高shuffle操作的并行度\"></a>方案三：提高shuffle操作的并行度</h2><ul>\n<li><strong>方案适用场景</strong>：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</li>\n<li><strong>方案实现思路</strong>：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</li>\n<li><strong>方案实现原理</strong>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</li>\n<li><strong>方案优点</strong>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</li>\n<li><strong>方案缺点</strong>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</li>\n<li><strong>方案实践经验</strong>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</li>\n</ul>\n<h2 id=\"方案四：两阶段聚合（局部聚合-全局聚合）\"><a href=\"#方案四：两阶段聚合（局部聚合-全局聚合）\" class=\"headerlink\" title=\"方案四：两阶段聚合（局部聚合+全局聚合）\"></a>方案四：两阶段聚合（局部聚合+全局聚合）</h2><ul>\n<li><strong>方案适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</li>\n<li><strong>方案实现思路</strong>：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</li>\n<li><strong>方案实现原理</strong>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。</li>\n<li><strong>方案优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</li>\n<li><strong>方案缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</li>\n</ul>\n<h2 id=\"方案五：将reduce-join转为map-join\"><a href=\"#方案五：将reduce-join转为map-join\" class=\"headerlink\" title=\"方案五：将reduce join转为map join\"></a>方案五：将reduce join转为map join</h2><ul>\n<li><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</li>\n<li><strong>方案实现思路</strong>：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</li>\n<li><strong>方案实现原理</strong>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</li>\n<li><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</li>\n<li><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</li>\n</ul>\n<h2 id=\"方案六：采样倾斜key并分拆join操作\"><a href=\"#方案六：采样倾斜key并分拆join操作\" class=\"headerlink\" title=\"方案六：采样倾斜key并分拆join操作\"></a>方案六：采样倾斜key并分拆join操作</h2><ul>\n<li><strong>方案适用场景</strong>：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</li>\n<li><p><strong>方案实现思路</strong>：</p>\n<ul>\n<li><p>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</p>\n</li>\n<li><p>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</p>\n</li>\n<li>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</li>\n<li>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。</li>\n<li>而另外两个普通的RDD就照常join即可。</li>\n<li>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</li>\n</ul>\n</li>\n<li><p><strong>方案实现原理</strong>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。</p>\n</li>\n<li><strong>方案优点</strong>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</li>\n<li><strong>方案缺点</strong>：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</li>\n</ul>\n<h2 id=\"方案七：使用随机前缀和扩容RDD进行join\"><a href=\"#方案七：使用随机前缀和扩容RDD进行join\" class=\"headerlink\" title=\"方案七：使用随机前缀和扩容RDD进行join\"></a>方案七：使用随机前缀和扩容RDD进行join</h2><ul>\n<li><strong>方案适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</li>\n<li><p><strong>方案实现思路</strong>：</p>\n<ul>\n<li>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。</li>\n<li>然后将该RDD的每条数据都打上一个n以内的随机前缀。</li>\n<li>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</li>\n<li>最后将两个处理后的RDD进行join即可。</li>\n</ul>\n</li>\n<li><p><strong>方案实现原理</strong>：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p>\n</li>\n<li><strong>方案优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</li>\n<li><strong>方案缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</li>\n<li><strong>方案实践经验</strong>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</li>\n</ul>\n<h2 id=\"方案八：多种方案组合使用\"><a href=\"#方案八：多种方案组合使用\" class=\"headerlink\" title=\"方案八：多种方案组合使用\"></a>方案八：多种方案组合使用</h2><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>\n","categories":["spark"],"tags":["spark","调优","转载"]},{"title":"windows下JDK版本切换脚本","url":"https://sustcoder.github.io/2018/09/18/windows下JDK版本切换脚本/","content":"<h1 id=\"初衷\"><a href=\"#初衷\" class=\"headerlink\" title=\"初衷\"></a>初衷</h1><p>前几天在一个技术交流群中看到技术人应该怎样去扩展自己的知识，去发现新的技术。其中有一条就是：<strong>当你对当前的工作感到厌倦的时候就应该去思考是否可以对其进行优化</strong>，比如我在重复的打开环境变量，修改JDK版本号的时候，就为每天都要进行此操作而感到厌倦，以至于内心开始拒绝去切换JDK版本，拒绝去做需要在另一个版本上的工作。</p>\n<h1 id=\"research过程\"><a href=\"#research过程\" class=\"headerlink\" title=\"research过程\"></a>research过程</h1><p>首先我搜索的关键字是<code>jdk版本切换</code>，其搜索结果都是怎样设置多JDK版本，怎样去修改环境变量。但是这些结果并不是我想要的，不过我确实是想要切换JDK版本啊，为什么没搜到结果呢。</p>\n<p><strong>当搜索不到结果的时候，首先考虑我们的搜索关键字是否准备</strong></p>\n<p>再次思考，我其实不是想切换JDK版本，而是想更方便的切换JDK版本，怎样会更方便呢，比如只点一个按钮即可。那其实可以通过脚本去实现这个功能，所以我的搜索条件变成了<code>windos切换JDK版本脚本</code>然后就搜索到了想要的结果。</p>\n<h1 id=\"脚本\"><a href=\"#脚本\" class=\"headerlink\" title=\"脚本\"></a>脚本</h1><h2 id=\"运行截图\"><a href=\"#运行截图\" class=\"headerlink\" title=\"运行截图\"></a>运行截图</h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/jdkVSwitch.png\" alt=\"运行截图\"></p>\n<h2 id=\"脚本内容\"><a href=\"#脚本内容\" class=\"headerlink\" title=\"脚本内容\"></a>脚本内容</h2><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@<span class=\"built_in\">echo</span> off</span><br><span class=\"line\"></span><br><span class=\"line\">rem --- Base Config 配置JDK的安装目录 ---</span><br><span class=\"line\">:init </span><br><span class=\"line\"><span class=\"built_in\">set</span> JAVA_HOME_1_8=C:\\Program Files\\Java\\jdk8</span><br><span class=\"line\"><span class=\"built_in\">set</span> JRE_HOME_1_8=C:\\Program Files\\Java\\jre8</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">set</span> JAVA_HOME_1_7=C:\\Program Files\\Java\\jdk7</span><br><span class=\"line\"><span class=\"built_in\">set</span> JRE_HOME_1_7=C:\\Program Files\\Java\\jre7</span><br><span class=\"line\">:start </span><br><span class=\"line\"><span class=\"built_in\">echo</span> 当前使用的JDK 版本: </span><br><span class=\"line\">java -version </span><br><span class=\"line\"><span class=\"built_in\">echo</span>. </span><br><span class=\"line\"><span class=\"built_in\">echo</span> ============================================= </span><br><span class=\"line\"><span class=\"built_in\">echo</span> jdk版本列表: </span><br><span class=\"line\"><span class=\"built_in\">echo</span>  jdk1.8 </span><br><span class=\"line\"><span class=\"built_in\">echo</span>  jdk1.7</span><br><span class=\"line\"><span class=\"built_in\">echo</span> ============================================= </span><br><span class=\"line\"></span><br><span class=\"line\">:select</span><br><span class=\"line\"><span class=\"built_in\">set</span> /p opt=请输入JDK版本。[7代表jdk1.7],[8代表jdk1.8]： </span><br><span class=\"line\"><span class=\"keyword\">if</span> %opt%==8 (</span><br><span class=\"line\">    <span class=\"built_in\">set</span> TARGET_JAVA_HOME=%JAVA_HOME_1_8%</span><br><span class=\"line\">\t<span class=\"built_in\">set</span> TARGET_JRE_HOME=%JRE_HOME_1_8%</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">if</span> %opt%==7 (</span><br><span class=\"line\">    <span class=\"built_in\">set</span> TARGET_JAVA_HOME=%JAVA_HOME_1_7%</span><br><span class=\"line\">\t<span class=\"built_in\">set</span> TARGET_JRE_HOME=%JRE_HOME_1_7%</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">echo</span> 当前选择的Java路径:</span><br><span class=\"line\"><span class=\"built_in\">echo</span> JAVE_HOME:%TARGET_JAVA_HOME%</span><br><span class=\"line\"><span class=\"built_in\">echo</span> JRE_HOME:%TARGET_JRE_HOME%</span><br><span class=\"line\"></span><br><span class=\"line\">wmic ENVIRONMENT <span class=\"built_in\">where</span> <span class=\"string\">\"name='JAVA_HOME'\"</span> delete</span><br><span class=\"line\">wmic ENVIRONMENT create name=<span class=\"string\">\"JAVA_HOME\"</span>,username=<span class=\"string\">\"&lt;system&gt;\"</span>,VariableValue=<span class=\"string\">\"%TARGET_JAVA_HOME%\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">wmic ENVIRONMENT <span class=\"built_in\">where</span> <span class=\"string\">\"name='JRE_HOME'\"</span> delete</span><br><span class=\"line\">wmic ENVIRONMENT create name=<span class=\"string\">\"JRE_HOME\"</span>,username=<span class=\"string\">\"&lt;system&gt;\"</span>,VariableValue=<span class=\"string\">\"%TARGET_JRE_HOME%\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">rem -- refresh env ---</span><br><span class=\"line\">call RefreshEnv</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">echo</span> 请按任意键退出!   </span><br><span class=\"line\">pause&gt;nul</span><br><span class=\"line\"></span><br><span class=\"line\">@<span class=\"built_in\">echo</span> on</span><br></pre></td></tr></table></figure>\n<h2 id=\"脚本下载\"><a href=\"#脚本下载\" class=\"headerlink\" title=\"脚本下载\"></a>脚本下载</h2><ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/RefreshEnv.exe\" target=\"_blank\" rel=\"noopener\">RefreshEnv.exe</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/switchVersion.bat\" target=\"_blank\" rel=\"noopener\">switchVersion.bat</a></li>\n</ul>\n<h1 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h1><ul>\n<li>是否需要配置<code>JRE_HOME</code>和安装JDK的路径有关系，下图是我的安装路径</li>\n</ul>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/jdkPath.png\" alt=\"jdk安装路径\"> </p>\n<ul>\n<li>需要修改<code>JAVA_HOME</code>的值为你对应的JDK安装路径</li>\n<li>需要以管理员权限运行脚本</li>\n</ul>\n","categories":["tools"],"tags":["tools","jdk","思考"]},{"title":"sbt安装与仓库设置","url":"https://sustcoder.github.io/2018/09/13/sbt入门/","content":"<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><ul>\n<li><code>java 1.8</code></li>\n<li><code>scala 2.12.6</code></li>\n<li><code>sbt 1.2.1</code></li>\n<li><code>idea2.18.3</code><h2 id=\"软件\"><a href=\"#软件\" class=\"headerlink\" title=\"软件\"></a>软件</h2></li>\n<li><code>idea sbt</code>插件</li>\n<li><code>idea scala</code>插件</li>\n<li>sbt安装包 <code>https://sbt-downloads.cdnedge.bluemix.net/releases/v1.2.1/sbt-1.2.1.msi</code>,非必须，可直接使用idea的sbt插件做对应配置<h2 id=\"安装sbt\"><a href=\"#安装sbt\" class=\"headerlink\" title=\"安装sbt\"></a>安装sbt</h2></li>\n</ul>\n<ol>\n<li>新建sbt安装路径，注意：<strong>sbt安装路径中不能含有空格和中文</strong>，将<code>sbt-1.2.1.msi</code>安装到此路径。</li>\n<li>配置环境变量</li>\n</ol>\n<ul>\n<li>新建变量sbt<blockquote>\n<p>SBT_HOME  D:\\ProgramFile\\sbt</p>\n</blockquote>\n</li>\n<li>添加变量到path中<blockquote>\n<p> %SBT_HOME%bin;</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h2><p>修改文件<code>onf/sbtconfig.txt</code>，添加以下内容</p>\n<blockquote>\n<p>-Dfile.encoding=UTF8</p>\n</blockquote>\n<h2 id=\"sbt仓库设置\"><a href=\"#sbt仓库设置\" class=\"headerlink\" title=\"sbt仓库设置\"></a>sbt仓库设置</h2><h3 id=\"方案一\"><a href=\"#方案一\" class=\"headerlink\" title=\"方案一\"></a>方案一</h3><p>直接修改sbt的jar里面的配置文件。<code>windows</code>下可通过360压缩替换掉<code>jar</code>包里面的文件。</p>\n<ol>\n<li>找到sbt安装目录<code>D:\\ProgramFile\\sbt\\bin</code></li>\n<li>备份<code>sbt-launch.jar</code>为<code>sbt-launch.jar.bak</code></li>\n<li>解压<code>sbt-launch.jar.bak</code>,打开个<code>sbt.boot.properties</code>文件</li>\n<li><p>在<code>[repositories]</code>里面的<code>local</code>下面添加以下数据源</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">alirepo1:https://maven.aliyun.com/repository/central</span><br><span class=\"line\">alirepo2:https://maven.aliyun.com/repository/jcenter</span><br><span class=\"line\">alirepo3:https://maven.aliyun.com/repository/public</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>使用360压缩打开<code>sbt-launch.jar</code>,找到<code>sbt.boot.properties</code>文件并替换</p>\n</li>\n</ol>\n<h3 id=\"方案二\"><a href=\"#方案二\" class=\"headerlink\" title=\"方案二\"></a>方案二</h3><p>配置sbt的数据源，让其优先加载我们配置的数据源</p>\n<ol>\n<li>在<code>D:\\ProgramFile\\sbt\\conf</code>目录下，新建文件<code>repository.properties</code></li>\n<li><p>在<code>repository.properties</code>中添加以下内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">[repositories]</span><br><span class=\"line\">local</span><br><span class=\"line\">alirepo1:https://maven.aliyun.com/repository/central</span><br><span class=\"line\">alirepo2:https://maven.aliyun.com/repository/jcenter</span><br><span class=\"line\">alirepo3:https://maven.aliyun.com/repository/public</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在<code>conf/sbtconfig.txt</code>中添加<code>repository.properties</code>文件路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">-Dsbt.repository.config=D:/ProgramFile/sbt/conf/repository.properties</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"添加依赖build-sbt\"><a href=\"#添加依赖build-sbt\" class=\"headerlink\" title=\"添加依赖build.sbt\"></a>添加依赖build.sbt</h2><p>在项目中找到<code>build.sbt</code>文件，此类似于<code>maven</code>中的<code>pom</code>文件<br>添加<code>spark-core</code>和<code>spark-sql</code>等的依赖<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">name := &quot;sbt-test&quot; // 项目名称</span><br><span class=\"line\"></span><br><span class=\"line\">version := &quot;0.1&quot; // 项目版本号</span><br><span class=\"line\"></span><br><span class=\"line\">scalaVersion := &quot;2.11.12&quot; // scala版本号</span><br><span class=\"line\"></span><br><span class=\"line\">// 依赖</span><br><span class=\"line\">libraryDependencies ++= Seq(</span><br><span class=\"line\">  &quot;org.apache.spark&quot;  %%  &quot;spark-core&quot;    % &quot;2.2.0&quot;,</span><br><span class=\"line\">  &quot;org.apache.spark&quot;  %%  &quot;spark-sql&quot;     % &quot;2.2.0&quot;,</span><br><span class=\"line\">  &quot;com.typesafe.scala-logging&quot; % &quot;scala-logging-slf4j_2.11&quot; % &quot;latest.integration&quot;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"下载依赖\"><a href=\"#下载依赖\" class=\"headerlink\" title=\"下载依赖\"></a>下载依赖</h2><p>通过以下方式打开执行命令窗口</p>\n<ul>\n<li>在<code>build.sbt</code>同级目录下打开<code>cmd</code>窗口</li>\n<li>在<code>idea</code>中打开<code>Termail</code>窗口</li>\n</ul>\n<p>下载或更新jar</p>\n<blockquote>\n<p>sbt update</p>\n</blockquote>\n<p>编译文件</p>\n<blockquote>\n<p>sbt compile</p>\n</blockquote>\n<p>打包</p>\n<blockquote>\n<p>sbt package</p>\n</blockquote>\n<h1 id=\"问题处理\"><a href=\"#问题处理\" class=\"headerlink\" title=\"问题处理\"></a>问题处理</h1><h2 id=\"版本不兼容\"><a href=\"#版本不兼容\" class=\"headerlink\" title=\"版本不兼容\"></a>版本不兼容</h2><h3 id=\"jdk不兼容\"><a href=\"#jdk不兼容\" class=\"headerlink\" title=\"jdk不兼容\"></a>jdk不兼容</h3><ul>\n<li><code>idea2018</code>需要<code>jdk8</code>以上</li>\n<li><code>spark2.0</code>需要<code>jdk8</code>以上<h2 id=\"文件下载缓慢\"><a href=\"#文件下载缓慢\" class=\"headerlink\" title=\"文件下载缓慢\"></a>文件下载缓慢</h2><code>idea</code>控制台<code>build</code>界面一直在转圈,并提示<code>dump project structure from sbt</code></li>\n</ul>\n<p>这里需要注意，在<code>Intellij Idea</code>启动时，会执行<code>dump project structure from sbt</code>的操作，也就是把sbt所需要的项目结构从远程服务器拉取到本地，在本地会生成sbt所需要的项目结构。由于是从国外的远程服务器下载，所以，这个过程很慢，笔者电脑上运行了15分钟。这个过程没有结束之前，上图中的<code>File-&gt;New</code>弹出的子菜单是找不到<code>Scala Class</code>这个选项的。所以，一定要等<code>dump project structure from sbt</code>的操作全部执行结束以后，再去按照上图操作来新建<code>Scala Class</code>文件。</p>\n<h2 id=\"修改sbt数据源\"><a href=\"#修改sbt数据源\" class=\"headerlink\" title=\"修改sbt数据源\"></a>修改sbt数据源</h2><h3 id=\"不靠谱方案\"><a href=\"#不靠谱方案\" class=\"headerlink\" title=\"不靠谱方案\"></a>不靠谱方案</h3><ol>\n<li>将数据源改为<code>maven.oschina.com</code>。此数据源已经失效</li>\n<li>将<code>sbt.boot.properties</code>中的<code>https</code>改为<code>http</code>。未生效</li>\n<li>在<code>sbt</code>的<code>vm</code>中配置<code>-Dsbt.override.build.repos=true</code>。此方法效果和<code>-Dsbt.repository.config=D:/ProgramFile/sbt/conf/repository.properties</code>一致，前提是需要配置数据源</li>\n<li>最笨方案，下载jar包，放到本地仓库<code>C:\\Users\\sustcoder\\.ivy2\\cache</code></li>\n</ol>\n<h3 id=\"修改成阿里云数据源后依旧下载失败\"><a href=\"#修改成阿里云数据源后依旧下载失败\" class=\"headerlink\" title=\"修改成阿里云数据源后依旧下载失败\"></a>修改成阿里云数据源后依旧下载失败</h3><ul>\n<li>配置的<code>sbt</code>版本在阿里云的仓库中没有。排查办法：可以去<code>maven.aliyun.com</code>去查看对应版本pom文件是否存在</li>\n<li>在阿里云上找到了对应版本但依旧保持。注意查看日志信息中下载的jar包路径含有<code>_2.10</code>类似的字样，比如在<code>build.sbt</code>中配置的是<code>&quot;org.apache.spark&quot;  %%  &quot;spark-sql&quot;     % &quot;2.2.0&quot;</code>,但是日志里面是<code>[warn]  :: com.typesafe.scala-logging#scala-sql_2.10;2.1.2: not found</code>,这个是因为sbt里面的<code>%%</code>代表sbt默认会拼接上scala的版本号在pom文件上，下载最适合的jar包，可以将<code>%%</code>改为<code>%</code>，即改为<code>&quot;org.apache.spark&quot;  %  &quot;spark-sql&quot;     % &quot;2.2.0&quot;</code>,注意区别：仅仅是少了一个百分号。</li>\n<li>执行<code>sbt-shell</code>会走默认的仓库配置，需要在sbt的vm参数中配置<code>-Dsbt.override.build.repos=true</code> ????<h2 id=\"查看配置参数是否生效\"><a href=\"#查看配置参数是否生效\" class=\"headerlink\" title=\"查看配置参数是否生效\"></a>查看配置参数是否生效</h2>可在日志控制台查看第一行日志，查看配置参数是否生效，走的是自己安装的sbt还是idea的插件,如下日志，在<code>sbtconfig.txt</code>中配置信息会进行加载<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&quot;C:\\Program Files\\Java\\jdk8\\bin\\java.exe&quot; -agentlib:jdwp=transport=dt_socket,address=localhost:58502,suspend=n,server=y -Xdebug -server -Xmx1536M </span><br><span class=\"line\">-Dsbt.repository.config=D:/develop/sbt/conf/repository.properties -Didea.managed=true -Dfile.encoding=UTF-8 </span><br><span class=\"line\">-Didea.runid=2018.2 -jar D:\\ProgramFile\\sbt\\bin\\sbt-launch.jar idea-shell</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>参考链接</strong></p>\n<p><a href=\"https://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html\" target=\"_blank\" rel=\"noopener\">https://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html</a></p>\n<p><a href=\"https://segmentfault.com/a/1190000002484978\" target=\"_blank\" rel=\"noopener\">https://segmentfault.com/a/1190000002484978</a></p>\n","categories":["spark"],"tags":["spark","sbt"]},{"title":"Hello World","url":"https://sustcoder.github.io/2018/09/10/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","categories":[],"tags":[]},{"title":"category","url":"https://sustcoder.github.io/category/index.html","content":"","categories":[],"tags":[]},{"title":"about","url":"https://sustcoder.github.io/about/index.html","content":"","categories":[],"tags":[]},{"title":"","url":"https://sustcoder.github.io/life/index.html","content":"<html>\n\t<head>\n\t\t<title>胖斯世界</title>\n\t\t<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\n\t\t<style type=\"text/css\">\n\t\t\t\nbody {\n\tmargin: 0px 0px;\n\tpadding: 0px 0px;\n\toverflow: hidden;\n}\n\ndiv {\n\tcolor: #fff;\n}\n\n.space {\n\tbackground : #000;\n\twidth : 100%;\n\theight : 100%;\n}\n\n.space  div{\n\ttop:50%;\n\tleft:50%;\n\tposition:absolute;\n}\n\n.container {\n\n\tbackground : #888;\n\t\n    animation-play-state: running;\n    -webkit-animation-play-state: running;    \n}\n\n.container:hover {\n\n\tbackground : #eee;\n\t\n    animation-play-state: paused;\n    -webkit-animation-play-state: paused;\n\t\n\ttransition: 3s;\n}\n\n#all-container {\n\n\tbackground : #0f0f0f;\n\n    -moz-border-radius: 400px;\n    -webkit-border-radius: 400px;\n    border-radius: 400px;\n\t\n\twidth : 800px;\n\theight : 800px;\n\tmargin-left:-400px;\n\tmargin-top:-400px;\n\t\n\n    animation: spin 80s linear infinite;\n    -webkit-animation: spin 80s linear infinite;\n\t\n}\n\n#all-container:hover,\n#family-container:hover,\n#me-container:hover,\n#wife-container:hover,\n#child-container:hover {\n\n    animation-play-state: paused;\n    -webkit-animation-play-state: paused;\n\t\n}\n\n#family-container {\n\n    -moz-border-radius: 120px;\n    -webkit-border-radius: 120px;\n    border-radius: 120px;\n\t\n\twidth : 240px;\n\theight : 240px;\n\tmargin-left:-120px;\n\tmargin-top:-120px;\n\t\n    animation: spin 40s linear infinite;\n    -webkit-animation: spin 40s linear infinite;\n\t\n\tbackground-image:url(https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/life/myworld/wife.jpg);\n}\n\n#family {\n\n\twidth : 180px;\n\theight : 180px;\n\tmargin-left:-90px;\n\tmargin-top:-90px;\n\tfont-size: 50;\n\tfont-weight:300;\n\t\n}\n\n#family .words {\n\tdisplay:none;\n\tmargin-left:-90px;\n\tmargin-top:90px;\n}\n\n#family:hover .words {\n\tdisplay:block;\n}\n\n#wife-container {\n\n    -moz-border-radius: 40px;\n    -webkit-border-radius: 40px;\n    border-radius: 40px;\n\t\n\twidth : 70px;\n\theight : 70px;\n\tmargin-left:-330px;\n\tmargin-top:-30px;\n\t\n    animation: spin 10s linear infinite;\n    -webkit-animation: spin 15s linear infinite;\n\t\n\tbackground-image:url(https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/life/myworld/me.jpg);\n}\n\n#wife {\n\n\twidth : 40px;\n\theight : 40px;\n\tmargin-left:-30px;\n\tmargin-top:-30px;\n\tfont-size:20;\n\tfont-weight:300;\n}\n\n#wife .words {\n\tdisplay:none;\n\tmargin-left:-50px;\n\tmargin-top:50px;\n}\n\n#wife:hover .words {\n\tdisplay:block;\n}\n\n#child-container {\n\n    -moz-border-radius: 20px;\n    -webkit-border-radius: 20px;\n    border-radius: 20px;\n\t\n\twidth : 40px;\n\theight : 40px;\n\tmargin-left:-330px;\n\tmargin-top:15px;\n\t\n    animation: spin 5s linear infinite;\n    -webkit-animation: spin 5s linear infinite;\n\t\n\tbackground-image:url(https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/life/myworld/child.jpg);\n}\n\n#child {\n\n\twidth : 30px;\n\theight : 30px;\n\tmargin-left:-15px;\n\tmargin-top:-15px;\n\tfont-size:15;\n\tfont-weight:300;\n}\n\n#child .words {\n\tdisplay:none;\n\tmargin-left:-30px;\n\tmargin-top:30px;\n}\n\n#child:hover .words {\n\tdisplay:block;\n}\n\n@keyframes spin {\n\tto {\n\t\t-webkit-transform: rotate(1turn);\n\t\t-ms-transform: rotate(1turn);\n\t}\n}\n\n@-webkit-keyframes spin {\n\tto {\n\t\t-webkit-transform: rotate(1turn);\n\t\t-ms-transform: rotate(1turn);\n\t}\n}\n\n\n\n\t  </style>\n\t</head>\n\t<body>\n\t\t<div class=\"space\">\n\t\t\t<div class=\"container\" id=\"all-container\">\n\t\t\t\t<div class=\"container\" id=\"family-container\">\n\t\t\t\t\t<div id=\"family\" alt=\"菲菲\">\n\t\t\t\t\t\t<div class=\"words\">菲菲</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"container\" id=\"wife-container\">\n\t\t\t\t\t<div id=\"wife\" alt=\"胖斯\">\n\t\t\t\t\t\t<div class=\"words\">胖斯</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"container\" id=\"child-container\">\n\t\t\t\t\t<div id=\"child\" alt=\"小可爱\">\n\t\t\t\t\t\t<div class=\"words\">小可爱</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</body>\n</html>\n\n","categories":[],"tags":[]},{"title":"link","url":"https://sustcoder.github.io/link/index.html","content":"","categories":[],"tags":[]},{"title":"search","url":"https://sustcoder.github.io/search/index.html","content":"","categories":[],"tags":[]},{"title":"project","url":"https://sustcoder.github.io/project/index.html","content":"","categories":[],"tags":[]},{"title":"tag","url":"https://sustcoder.github.io/tag/index.html","content":"","categories":[],"tags":[]}]