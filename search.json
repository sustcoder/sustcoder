[{"title":"sparkCore源码解析之partition","url":"https://sustcoder.github.io/2019/01/14/2018-12-01-sparkCore-sorceCodeAnalysis-partition/","content":"<p> <img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/Partition.png\" alt=\"job\"></p>\n<h1 id=\"1-概念\"><a href=\"#1-概念\" class=\"headerlink\" title=\"1. 概念\"></a>1. <strong>概念</strong></h1><p>表示并行计算的一个计算单元</p>\n<p>RDD 内部的数据集合在逻辑上和物理上被划分成多个小子集合，这样的每一个子集合我们将其称为分区，分区的个数会决定并行计算的粒度，而每一个分区数值的计算都是在一个单独的任务中进行，因此并行任务的个数，也是由 RDD（实际上是一个阶段的末 RDD，调度章节会介绍）分区的个数决定的</p>\n<h1 id=\"2-获取分区\"><a href=\"#2-获取分区\" class=\"headerlink\" title=\"2. 获取分区\"></a>2. <strong>获取分区</strong></h1><p>RDD的分区数量可通过rdd.getPartitions获取。<br>getPartitions方法是在RDD类中定义的，由不同的子类进行具体的实现</p>\n<h2 id=\"2-1-接口\"><a href=\"#2-1-接口\" class=\"headerlink\" title=\"2.1. 接口\"></a>2.1. <strong>接口</strong></h2><p><strong>获取分区的定义</strong></p>\n<p>在RDD类中定义了getPartition方法，返回一个Partition列表，Partition对象只含有一个编码index字段，不同RDD的partition会继承Partition类，例如JdbcPartition、KafkaRDDPartition，HadoopPartition等。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RDD</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 获取分区定义</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>:<span class=\"type\">Array</span>[<span class=\"type\">Partition</span>]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// Partition类定义</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Partition</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">index</span></span>:<span class=\"type\">Int</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-2-实现\"><a href=\"#2-2-实现\" class=\"headerlink\" title=\"2.2. 实现\"></a>2.2. <strong>实现</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsC537.tmp.jpg\" alt=\"img\"> </p>\n<p>transformation类型的RDD分区数量和父RDD的保持一致，Action类型的RDD分区数量，不同的数据源默认值不一样，获取的方式也不同</p>\n<h3 id=\"2-2-1-KafkaRDD\"><a href=\"#2-2-1-KafkaRDD\" class=\"headerlink\" title=\"2.2.1. KafkaRDD\"></a>2.2.1. <strong>KafkaRDD</strong></h3><p>kafkaRDD的partition数量等于compute方法中生成OffsetRange的数量。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// DirectKafkaInputDStream类在接收到消息后通过compute方法计算得到OffsetRange</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">OffsetRange</span>(<span class=\"params\"> </span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tval topic:<span class=\"type\">String</span>, // <span class=\"type\">Kafka</span> topic name</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tval partition:<span class=\"type\">Int</span>, // <span class=\"type\">Kafka</span> partition id</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tval fromOffset:<span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tval untilOffset:<span class=\"type\">Long</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;...&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaRDD</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\t\tval offsetRages:<span class=\"type\">Array</span>[<span class=\"type\">OffsetRange</span>]</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\t</span>) <span class=\"keyword\">extends</span> <span class=\"title\">RDD</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 遍历OffsetRange数组，得到数组下标和偏移量等信息生成KafkaRDDPartition</span></span><br><span class=\"line\">\toffsetRanges.zipWithIndex.map&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span>(o,i)=&gt;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">KafkaRDDPartition</span>(</span><br><span class=\"line\">\t\t\t\ti,o.topic,o.partition,</span><br><span class=\"line\">\t\t\t\to.fromOffset,o.untilOffset</span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\">\t&#125;.toArray</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-2-HadoopRDD\"><a href=\"#2-2-2-HadoopRDD\" class=\"headerlink\" title=\"2.2.2. HadoopRDD\"></a>2.2.2. <strong>HadoopRDD</strong></h3><p>HadoopRDD的分区是基于hadoop的splits方法进行的。每个partition的大小默认等于hdfs的block的大小</p>\n<p>例如：一个txt文件12800M,则<br><code>val rdd1=sc.textFile(&quot;/data.txt&quot;);</code><br>rdd1默认会有12800/128=10个分区。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HadoopRDD</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 生成一个RDD的唯一ID</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> id=<span class=\"type\">Int</span>=sc.newRddId()</span><br><span class=\"line\"> </span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>:<span class=\"type\">Array</span>[<span class=\"type\">Partition</span>]=&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 调用hadoop的splits方法进行切割</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> inputSplits=inputFormat.getSplits(</span><br><span class=\"line\">\t\t\tjobConf,minPartitions</span><br><span class=\"line\">\t\t)</span><br><span class=\"line\">\t\t<span class=\"comment\">// 组成spark的partition</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> array=<span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>](inputSplits.size)</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until inputSplits.size)&#123;</span><br><span class=\"line\">\t\t\tarray(i)=<span class=\"keyword\">new</span> <span class=\"type\">HadoopPartition</span>(id,i,</span><br><span class=\"line\">\t\t\t\tinputSplits(i))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>hadoop的FileInputFormat类：<br>texfile的分区大小时指定的分区数和block树中取较大值，所以当指定numPartitions，小于block数时无效，大于则生效</p>\n<h3 id=\"2-2-3-JdbcRDD\"><a href=\"#2-2-3-JdbcRDD\" class=\"headerlink\" title=\"2.2.3. JdbcRDD\"></a>2.2.3. <strong>JdbcRDD</strong></h3><p>JDBC的partition划分是指定开始行和结束行，然后将查询到的结果分为3个（默认值）partition。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JdbcRDD</span>(<span class=\"params\">numPartitions:<span class=\"type\">Int</span></span>)</span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>:<span class=\"type\">Array</span>[<span class=\"type\">Partition</span>]=&#123;</span><br><span class=\"line\">\t\t(<span class=\"number\">0</span> until numPartitions).map&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">JdbcPartition</span>(i,start,end)</span><br><span class=\"line\">\t\t&#125;.toArray</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-4-MapPartitionsRDD\"><a href=\"#2-2-4-MapPartitionsRDD\" class=\"headerlink\" title=\"2.2.4. MapPartitionsRDD\"></a>2.2.4. <strong>MapPartitionsRDD</strong></h3><p>转换类的RDD分区数量是由其父类的分区数决定的</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 获取父RDD列表的第一个RDD</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RDD</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">firstParent</span></span>:<span class=\"type\">RDD</span>=&#123;</span><br><span class=\"line\">\t\tdependencies.head.rdd.asInstanceOf[<span class=\"type\">RDD</span>]</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapPartitionsRDD</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 获取父RDD的partitions数量</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = \t\t\tfirstParent[<span class=\"type\">T</span>].partitions</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"3-分区数量\"><a href=\"#3-分区数量\" class=\"headerlink\" title=\"3. 分区数量\"></a>3. <strong>分区数量</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsC557.tmp.jpg\" alt=\"img\"> </p>\n<p>分区数量的原则：尽可能的选择大的分区值</p>\n<h2 id=\"3-1-RDD初始化相关\"><a href=\"#3-1-RDD初始化相关\" class=\"headerlink\" title=\"3.1. RDD初始化相关\"></a>3.1. <strong>RDD初始化相关</strong></h2><table>\n<thead>\n<tr>\n<th>Spark API</th>\n<th>partition数量</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sc.parallelize(…)</td>\n<td>sc.defaultParallelism</td>\n</tr>\n<tr>\n<td>sc.textFile(…)</td>\n<td>max(传参, block数)</td>\n</tr>\n<tr>\n<td>sc.newAPIHadoopRDD(…)</td>\n<td>max(传参, block数)</td>\n</tr>\n<tr>\n<td>new JdbcRDD(…)</td>\n<td>传参</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"3-2-通用transformation\"><a href=\"#3-2-通用transformation\" class=\"headerlink\" title=\"3.2. 通用transformation\"></a>3.2. <strong>通用transformation</strong></h2><ul>\n<li>filter(),map(),flatMap(),distinct()：和父RDD相同</li>\n<li>union： 两个RDD的和rdd.union(otherRDD)：rdd.partitions.size + otherRDD. partitions.size</li>\n<li>intersection：取较大的rdd.intersection(otherRDD)：max(rdd.partitions.size, otherRDD. partitions.size)</li>\n<li>rdd.subtract(otherRDD)    ：rdd.partitions.size</li>\n<li>cartesian：两个RDD数量的乘积rdd.cartesian(otherRDD)：<br>rdd.partitions.size * otherRDD. partitions.size</li>\n</ul>\n<h2 id=\"3-3-Key-based-Transformations\"><a href=\"#3-3-Key-based-Transformations\" class=\"headerlink\" title=\"3.3. Key-based Transformations\"></a>3.3. <strong>Key-based Transformations</strong></h2><p>reduceByKey(),foldByKey(),combineByKey(), groupByKey(),sortByKey(),mapValues(),flatMapValues()    和父RDD相同</p>\n<p>cogroup(), join(), ,leftOuterJoin(), rightOuterJoin():<br>所有父RDD按照其partition数降序排列，从partition数最大的RDD开始查找是否存在partitioner，存在则partition数由此partitioner确定，否则，所有RDD不存在partitioner，由spark.default.parallelism确定，若还没设置，最后partition数为所有RDD中partition数的最大值</p>\n<h1 id=\"4-分区器\"><a href=\"#4-分区器\" class=\"headerlink\" title=\"4. 分区器\"></a>4. <strong>分区器</strong></h1><p><strong>注意：只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None的</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Partitioner</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">numPartitions</span></span>: <span class=\"type\">Int</span> <span class=\"comment\">// 分区数量</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span> <span class=\"comment\">// 分区编号</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-1-作用\"><a href=\"#4-1-作用\" class=\"headerlink\" title=\"4.1. 作用\"></a>4.1. <strong>作用</strong></h2><p>partitioner分区器作用：</p>\n<ol>\n<li>决定Shuffle过程中Reducer个数（实际上是子RDD的分区个数）以及Map端一条数据记录应该分配给那几个Reducer</li>\n<li>决定RDD的分区数量，例如执行groupByKey(new HashPartitioner(2))所生成的ShuffledRDD中，分区数目等于2</li>\n<li>决定CoGroupedRDD与父RDD之间的依赖关系</li>\n</ol>\n<h2 id=\"4-2-种类\"><a href=\"#4-2-种类\" class=\"headerlink\" title=\"4.2. 种类\"></a>4.2. <strong>种类</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsC559.tmp.jpg\" alt=\"img\"> </p>\n<p>分区器的选择：</p>\n<ol>\n<li>如果RDD已经有了分区器，则在已有分区器里面挑选分区数量最多的一个分区器。</li>\n<li>如果RDD没有指定分区器，则默认使用HashPartitioner分区器。</li>\n<li>用户可以自己声明RangePartitioner分区器</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Partitioner</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">defaultPartitioner</span></span>(rdd):<span class=\"type\">Partitioner</span>=&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> hasPartitioner=</span><br><span class=\"line\">\t\t\trdds.filter(</span><br><span class=\"line\">\t\t\t\t_.partitioner.exists(_numPartitions&gt;<span class=\"number\">0</span>))</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 如果RDD已经有分区则选取其分区数最多的</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(hasPartitioner.nonEmpty)&#123;</span><br><span class=\"line\">\t\t\thasPartitioner.maxBy(_.partitions.length).</span><br><span class=\"line\">\t\t\t\tpartitioner.get</span><br><span class=\"line\">\t\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(rdd.context.conf.contains(</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"spark.default.parallelism\"</span></span><br><span class=\"line\">\t\t\t))&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 如果在conf中配置了分区数则用之</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">HashPartitioner</span>(</span><br><span class=\"line\">\t\t\t\t\trdd.context.defaultParallelism</span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\">\t\t\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 如果没有配置parallelism则和父RDD中最大的保持一致</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">HashPartitioner</span>(rdds.map(</span><br><span class=\"line\">\t\t\t\t\t_.partitions.length</span><br><span class=\"line\">\t\t\t\t).max)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-1-HashPartitioner\"><a href=\"#4-2-1-HashPartitioner\" class=\"headerlink\" title=\"4.2.1. HashPartitioner\"></a>4.2.1. <strong>HashPartitioner</strong></h3><p>HashPartitioner分区的原理很简单，对于给定的key，计算其hashCode，并除于分区的个数取余，<strong>如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HashPartitioner</span>(<span class=\"params\">partitions:<span class=\"type\">Int</span></span>) </span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key:<span class=\"type\">Any</span>):<span class=\"type\">Int</span>=key <span class=\"keyword\">match</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"literal\">null</span>=&gt; <span class=\"number\">0</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> _=&gt; nonNegativeMod(key.hashCode, numPartitions)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nonNegativeMod</span></span>(x: <span class=\"type\">Int</span>, mod: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> rawMod = x % mod</span><br><span class=\"line\">    rawMod + (<span class=\"keyword\">if</span> (rawMod &lt; <span class=\"number\">0</span>) mod <span class=\"keyword\">else</span> <span class=\"number\">0</span>)</span><br><span class=\"line\">  &#125;\t</span><br><span class=\"line\">\t<span class=\"comment\">// 判断两个RDD分区方式是否一样</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equals</span></span>(other:<span class=\"type\">Any</span>):<span class=\"type\">Boolean</span>= other <span class=\"keyword\">match</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> h:<span class=\"type\">HashPartitioner</span> =&gt; </span><br><span class=\"line\">\t\t\th.numPartitions==numPartitions</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span>  _ =&gt; <span class=\"literal\">false</span></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-2-RangePartitioner\"><a href=\"#4-2-2-RangePartitioner\" class=\"headerlink\" title=\"4.2.2. RangePartitioner\"></a>4.2.2. <strong>RangePartitioner</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsC55A.tmp.jpg\" alt=\"img\"> </p>\n<p>HashPartitioner分区可能导致每个分区中数据量的不均匀。而RangePartitioner分区则尽量保证每个分区中数据量的均匀，将一定范围内的数映射到某一个分区内。分区与分区之间数据是有序的，但分区内的元素是不能保证顺序的。</p>\n<p>  RangePartitioner分区执行原理：</p>\n<ol>\n<li>计算总体的数据抽样大小sampleSize，计算规则是：至少每个分区抽取20个数据或者最多1M的数据量。</li>\n<li>根据sampleSize和分区数量计算每个分区的数据抽样样本数量最大值sampleSizePrePartition</li>\n<li>根据以上两个值进行水塘抽样，返回RDD的总数据量，分区ID和每个分区的采样数据。</li>\n<li>计算出数据量较大的分区通过RDD.sample进行重新抽样。</li>\n<li>通过抽样数组 candidates: ArrayBuffer[(K, wiegth)]计算出分区边界的数组BoundsArray</li>\n<li>在取数据时，如果分区数小于128则直接获取，如果大于128则通过二分法，获取当前Key属于那个区间，返回对应的BoundsArray下标即为partitionsID</li>\n</ol>\n<p>一句话概括：<strong>就是遍历每个paritiion，对里面的数据进行抽样，把抽样的数据进行排序，并按照对应的权重确定边界</strong></p>\n<h4 id=\"4-2-2-1-获取区间数组\"><a href=\"#4-2-2-1-获取区间数组\" class=\"headerlink\" title=\"4.2.2.1. 获取区间数组\"></a>4.2.2.1. <strong>获取区间数组</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsC56B.tmp.jpg\" alt=\"img\"> </p>\n<h5 id=\"4-2-2-1-1-给定样本总数\"><a href=\"#4-2-2-1-1-给定样本总数\" class=\"headerlink\" title=\"4.2.2.1.1. 给定样本总数\"></a>4.2.2.1.1. <strong>给定样本总数</strong></h5><p>给定总的数据抽样大小，最多1M的数据量(10^6)，最少20倍的RDD分区数量，也就是每个RDD分区至少抽取20条数据<br>​<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RangePartitioner</span>(<span class=\"params\">partitions,rdd</span>) </span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// 1. 计算样本大小</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> sampleSize =math.min(<span class=\"number\">20.0</span> * partitions, <span class=\"number\">1e6</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-2-2-1-2-计算样本最大值\"><a href=\"#4-2-2-1-2-计算样本最大值\" class=\"headerlink\" title=\"4.2.2.1.2. 计算样本最大值\"></a>4.2.2.1.2. <strong>计算样本最大值</strong></h5><p>RDD各分区中的数据量可能会出现倾斜的情况，乘于3的目的就是保证数据量小的分区能够采样到足够的数据，而对于数据量大的分区会进行第二次采样</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RangePartitioner</span>(<span class=\"params\">partitions,rdd</span>) </span>&#123;</span><br><span class=\"line\">​\t</span><br><span class=\"line\"><span class=\"comment\">// 1. 计算样本大小</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> sampleSize = </span><br><span class=\"line\">​\t\t\tmath.min(<span class=\"number\">20.0</span> * partitions, <span class=\"number\">1e6</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 2. 计算样本最大值</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sampleSizePerPartition = </span><br><span class=\"line\">​\tmath.ceil(</span><br><span class=\"line\">​\t\t<span class=\"number\">3.0</span> * sampleSize / rdd.partitions.length</span><br><span class=\"line\">​\t).toInt</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"4-2-2-1-3-水塘抽样\"><a href=\"#4-2-2-1-3-水塘抽样\" class=\"headerlink\" title=\"4.2.2.1.3. 水塘抽样\"></a>4.2.2.1.3. <strong>水塘抽样</strong></h5><p>根据以上两个值进行水塘抽样，返回RDD的总数据量，分区ID和每个分区的采样数据。其中总数据量通过遍历RDD所有partition的key累加得到的，不是通过rdd.count计算得到的</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RangePartitioner</span>(<span class=\"params\">partitions,rdd</span>) </span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"comment\">// 1. 计算样本大小</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> sampleSize =math.min(<span class=\"number\">20.0</span> * partitions, <span class=\"number\">1e6</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 2. 计算样本最大值</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sampleSizePerPartition = </span><br><span class=\"line\">\tmath.ceil(</span><br><span class=\"line\">\t\t<span class=\"number\">3.0</span> * sampleSize / rdd.partitions.length</span><br><span class=\"line\">\t).toInt</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 3. 进行抽样，返回总数据量，分区ID和样本数据</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> (numItems, sketched) = <span class=\"type\">RangePartitioner</span>.sketch(</span><br><span class=\"line\">\t\trdd.map(_._1), sampleSizePerPartition)</span><br></pre></td></tr></table></figure>\n<h5 id=\"4-2-2-1-4-是否需要二次采样\"><a href=\"#4-2-2-1-4-是否需要二次采样\" class=\"headerlink\" title=\"4.2.2.1.4. 是否需要二次采样\"></a>4.2.2.1.4. <strong>是否需要二次采样</strong></h5><p>如果有较大RDD存在，则按照平均值去采样的话数据量太少，容易造成数据倾斜，所以需要进行二次采样</p>\n<p>判断是否需要重新采样方法：<br>样本数量占比乘以当前RDD的总行数大于预设的每个RDD最大抽取数量，说明这个RDD的数据量比较大，需要采样更多的数据：eg: 0.2<em>100=20&lt;60;0.2</em>20000=2000&gt;60</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RangePartitioner</span>(<span class=\"params\">partitions,rdd</span>) </span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"comment\">// 1. 计算样本大小</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> sampleSize =math.min(<span class=\"number\">20.0</span> * partitions, <span class=\"number\">1e6</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 2. 计算样本最大值</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sampleSizePerPartition = </span><br><span class=\"line\">\tmath.ceil(</span><br><span class=\"line\">\t\t<span class=\"number\">3.0</span> * sampleSize / rdd.partitions.length</span><br><span class=\"line\">\t).toInt</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 3. 进行抽样，返回总数据量，分区ID和样本数据</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> (numItems, sketched) = <span class=\"type\">RangePartitioner</span>.sketch(</span><br><span class=\"line\">\t\trdd.map(_._1), sampleSizePerPartition)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 4. 是否需要二次采样</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> imbalancedPartitions = \tmutable.<span class=\"type\">Set</span>.empty[<span class=\"type\">Int</span>]</span><br><span class=\"line\"> <span class=\"keyword\">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 记录需要重新采样的RDD的ID</span></span><br><span class=\"line\">\timbalancedPartitions += idx </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"4-2-2-1-5-计算样本权重\"><a href=\"#4-2-2-1-5-计算样本权重\" class=\"headerlink\" title=\"4.2.2.1.5. 计算样本权重\"></a>4.2.2.1.5. <strong>计算样本权重</strong></h5><p>计算每个采样数据的权重占比，根据采样数据的ID和权重生成出RDD分区边界数组</p>\n<p>权重计算方法：总数据量/当前RDD的采样数据量</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RangePartitioner</span>(<span class=\"params\">partitions,rdd</span>) </span>&#123;</span><br><span class=\"line\">​\t</span><br><span class=\"line\"><span class=\"comment\">// 1. 计算样本大小</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> sampleSize = </span><br><span class=\"line\">​\t\t\tmath.min(<span class=\"number\">20.0</span> * partitions, <span class=\"number\">1e6</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 2. 计算样本最大值</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sampleSizePerPartition = </span><br><span class=\"line\">​\tmath.ceil(</span><br><span class=\"line\">​\t\t<span class=\"number\">3.0</span> * sampleSize / rdd.partitions.length</span><br><span class=\"line\">​\t).toInt</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 3. 进行抽样，返回总数据量，分区ID和样本数据</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> (numItems, sketched) = \t\t<span class=\"type\">RangePartitioner</span>.sketch(</span><br><span class=\"line\">​\t\trdd.map(_._1), sampleSizePerPartition)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 4. 是否需要二次采样</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> imbalancedPartitions = \tmutable.<span class=\"type\">Set</span>.empty[<span class=\"type\">Int</span>]</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">//  5. 保存样本数据的集合buffer:包含数据和权重</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> candidates = <span class=\"type\">ArrayBuffer</span>.empty[(<span class=\"type\">K</span>, <span class=\"type\">Float</span>)]</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"keyword\">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">// 记录需要重新采样的RDD的ID</span></span><br><span class=\"line\">​\timbalancedPartitions += idx </span><br><span class=\"line\">&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// 5. 计算样本权重</span></span><br><span class=\"line\">​\t<span class=\"keyword\">val</span> weight = (</span><br><span class=\"line\">​\t  <span class=\"comment\">// 采样数据的占比</span></span><br><span class=\"line\">​\t\tn.toDouble / sample.length).toFloat </span><br><span class=\"line\">​            <span class=\"keyword\">for</span> (key &lt;- sample) &#123;</span><br><span class=\"line\">​\t\t\t<span class=\"comment\">// 记录采样数据key和权重</span></span><br><span class=\"line\">​              candidates += ((key, weight))</span><br><span class=\"line\">​            &#125;</span><br><span class=\"line\">​\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"4-2-2-1-6-二次抽样\"><a href=\"#4-2-2-1-6-二次抽样\" class=\"headerlink\" title=\"4.2.2.1.6. 二次抽样\"></a>4.2.2.1.6. <strong>二次抽样</strong></h5><p> 对于数据分布不均衡的RDD分区，重新进行二次抽样。<br>二次抽样采用的是RDD的采样方法：RDD.sample</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RangePartitioner</span>(<span class=\"params\">partitions,rdd</span>) </span>&#123;</span><br><span class=\"line\">​\t</span><br><span class=\"line\"><span class=\"comment\">// 1. 计算样本大小</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> sampleSize = </span><br><span class=\"line\">​\t\t\tmath.min(<span class=\"number\">20.0</span> * partitions, <span class=\"number\">1e6</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 2. 计算样本最大值</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sampleSizePerPartition = </span><br><span class=\"line\">​\tmath.ceil(</span><br><span class=\"line\">​\t\t<span class=\"number\">3.0</span> * sampleSize / rdd.partitions.length</span><br><span class=\"line\">​\t).toInt</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 3. 进行抽样，返回总数据量，分区ID和样本数据</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> (numItems, sketched) = \t\t<span class=\"type\">RangePartitioner</span>.sketch(</span><br><span class=\"line\">​\t\trdd.map(_._1), sampleSizePerPartition)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 4. 是否需要二次采样</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> imbalancedPartitions = \tmutable.<span class=\"type\">Set</span>.empty[<span class=\"type\">Int</span>]</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">//  5. 保存样本数据的集合buffer:包含数据和权重</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> candidates = <span class=\"type\">ArrayBuffer</span>.empty[(<span class=\"type\">K</span>, <span class=\"type\">Float</span>)]</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"keyword\">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">// 记录需要重新采样的RDD的ID</span></span><br><span class=\"line\">​\timbalancedPartitions += idx </span><br><span class=\"line\">&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// 5. 计算样本权重</span></span><br><span class=\"line\">​\t<span class=\"keyword\">val</span> weight = (</span><br><span class=\"line\">​\t  <span class=\"comment\">// 采样数据的占比</span></span><br><span class=\"line\">​\t\tn.toDouble / sample.length).toFloat </span><br><span class=\"line\">​            <span class=\"keyword\">for</span> (key &lt;- sample) &#123;</span><br><span class=\"line\">​\t\t\t<span class=\"comment\">// 记录采样数据key和权重</span></span><br><span class=\"line\">​              candidates += ((key, weight))</span><br><span class=\"line\">​            &#125;</span><br><span class=\"line\">​\t&#125;</span><br><span class=\"line\"><span class=\"comment\">// 6. 对于数据分布不均衡的RDD分区，重新数据抽样</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (imbalancedPartitions.nonEmpty) &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">// 利用rdd的sample抽样函数API进行数据抽样</span></span><br><span class=\"line\">​          <span class=\"keyword\">val</span> reSampled = imbalanced.sample(</span><br><span class=\"line\">​\t\t\t\twithReplacement = </span><br><span class=\"line\">​\t\t\t\t\t<span class=\"literal\">false</span>, fraction, seed).collect()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"4-2-2-1-7-生成边界数组\"><a href=\"#4-2-2-1-7-生成边界数组\" class=\"headerlink\" title=\"4.2.2.1.7. 生成边界数组\"></a>4.2.2.1.7. <strong>生成边界数组</strong></h5><p>将最终的抽样数据计算出分区边界数组返回，边界数组里面存放的是RDD里面数据的key值，<br>比如最终返回的数组是：array[0,10,20,30..]<br>其中0,10,20,30是采样数据中的key值，对于每一条数据都会判断其在此数组的那个区间中间，例如有一条数据key值是3则其在0到10之间，属于第一个分区，同理Key值为15的数据在第二个分区</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RangePartitioner</span>(<span class=\"params\">partitions,rdd</span>) </span>&#123;</span><br><span class=\"line\">​\t</span><br><span class=\"line\"><span class=\"comment\">// 1. 计算样本大小</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> sampleSize = </span><br><span class=\"line\">​\t\t\tmath.min(<span class=\"number\">20.0</span> * partitions, <span class=\"number\">1e6</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 2. 计算样本最大值</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sampleSizePerPartition = </span><br><span class=\"line\">​\tmath.ceil(</span><br><span class=\"line\">​\t\t<span class=\"number\">3.0</span> * sampleSize / rdd.partitions.length</span><br><span class=\"line\">​\t).toInt</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 3. 进行抽样，返回总数据量，分区ID和样本数据</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> (numItems, sketched) = \t\t<span class=\"type\">RangePartitioner</span>.sketch(</span><br><span class=\"line\">​\t\trdd.map(_._1), sampleSizePerPartition)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 4. 是否需要二次采样</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> imbalancedPartitions = \tmutable.<span class=\"type\">Set</span>.empty[<span class=\"type\">Int</span>]</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">//  5. 保存样本数据的集合buffer:包含数据和权重</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> candidates = <span class=\"type\">ArrayBuffer</span>.empty[(<span class=\"type\">K</span>, <span class=\"type\">Float</span>)]</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"keyword\">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">// 记录需要重新采样的RDD的ID</span></span><br><span class=\"line\">​\timbalancedPartitions += idx </span><br><span class=\"line\">&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// 5. 计算样本权重</span></span><br><span class=\"line\">​\t<span class=\"keyword\">val</span> weight = (</span><br><span class=\"line\">​\t  <span class=\"comment\">// 采样数据的占比</span></span><br><span class=\"line\">​\t\tn.toDouble / sample.length).toFloat </span><br><span class=\"line\">​            <span class=\"keyword\">for</span> (key &lt;- sample) &#123;</span><br><span class=\"line\">​\t\t\t<span class=\"comment\">// 记录采样数据key和权重</span></span><br><span class=\"line\">​              candidates += ((key, weight))</span><br><span class=\"line\">​            &#125;</span><br><span class=\"line\">​\t&#125;</span><br><span class=\"line\"><span class=\"comment\">// 6. 对于数据分布不均衡的RDD分区，重新数据抽样</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (imbalancedPartitions.nonEmpty) &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">// 利用rdd的sample抽样函数API进行数据抽样</span></span><br><span class=\"line\">​          <span class=\"keyword\">val</span> reSampled = imbalanced.sample(</span><br><span class=\"line\">​\t\t\t\twithReplacement = </span><br><span class=\"line\">​\t\t\t\t\t<span class=\"literal\">false</span>, fraction, seed).collect()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 7. 生成边界数组</span></span><br><span class=\"line\"><span class=\"type\">RangePartitioner</span>.determineBounds(</span><br><span class=\"line\">​\tcandidates, partitions)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2-2-2-水塘抽样算法\"><a href=\"#4-2-2-2-水塘抽样算法\" class=\"headerlink\" title=\"4.2.2.2. 水塘抽样算法\"></a>4.2.2.2. <strong>水塘抽样算法</strong></h4><p>水塘抽样概念：<br>它是一系列的随机算法，其目的在于从包含n个项目的集合S中选取k个样本，使得每条数据抽中的概率是k/n。其中n为一很大或未知的数量，尤其适用于不能把所有n个项目都存放到主内存的情况</p>\n<p>我们可以：定义取出的行号为choice，第一次直接以第一行作为取出行 choice ，而后第二次以二分之一概率决定是否用第二行替换 choice ，第三次以三分之一的概率决定是否以第三行替换 choice ……，以此类推。由上面的分析我们可以得出结论，在取第n个数据的时候，我们生成一个0到1的随机数p，如果p小于1/n，保留第n个数。大于1/n，继续保留前面的数。直到数据流结束，返回此数，算法结束。</p>\n<p>详见：<br><a href=\"https://www.iteblog.com/archives/1525.html\" target=\"_blank\" rel=\"noopener\">https://www.iteblog.com/archives/1525.html</a><br><a href=\"https://my.oschina.net/freelili/blog/2987667\" target=\"_blank\" rel=\"noopener\">https://my.oschina.net/freelili/blog/2987667</a></p>\n<p> 实现：</p>\n<ol>\n<li>获取到需要抽样RDD分区的样本大小k和分区的所有KEY数组input</li>\n<li>初始化抽样结果集reservoir为分区前K个KEY值</li>\n<li>如果分区的总数小于预计样本大小k,则将当前分区的所有数据作为样本数据，否则到第四步</li>\n<li>遍历分区里所有Key组成的数组input</li>\n<li>生成随机需要替换input数组的下标，如果下标小于K则替换</li>\n<li>返回抽取的key值数组和当前分区的总数据量： (reservoir, l)</li>\n</ol>\n<p>难点：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\">// 计算出需要替换的数组下标</span></span><br><span class=\"line\"><span class=\"comment\">// 选取第n个数的概率是：n/l; 如果随机替换数组值的概率是p=rand.nextDouble，</span></span><br><span class=\"line\"><span class=\"comment\">// 则如果p&lt;k/l;则替换池中任意一个数，即： p*l &lt; k 则进行替换，用p*l作为随机替换的下标</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> replacementIndex = (rand.nextDouble() * l).toLong</span><br><span class=\"line\"><span class=\"keyword\">if</span> (replacementIndex &lt; k) &#123;</span><br><span class=\"line\"><span class=\"comment\">// 替换reservoir[随机抽取的下标]的值为input[l]的值item</span></span><br><span class=\"line\">          reservoir(replacementIndex.toInt) = item</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-2-2-3-定位分区ID\"><a href=\"#4-2-2-3-定位分区ID\" class=\"headerlink\" title=\"4.2.2.3. 定位分区ID\"></a>4.2.2.3. <strong>定位分区ID</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsC56C.tmp.jpg\" alt=\"img\"> </p>\n<p>如果分区边界数组的大小小于或等于128的时候直接变量数组，否则采用二分查找法确定key属于某个分区。</p>\n<h5 id=\"4-2-2-3-1-数组直接获取\"><a href=\"#4-2-2-3-1-数组直接获取\" class=\"headerlink\" title=\"4.2.2.3.1. 数组直接获取\"></a>4.2.2.3.1. <strong>数组直接获取</strong></h5><p>遍历数组，判断当前key值是否属于当前区间</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 根据RDD的key值返回对应的分区id。从0开始</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">​    <span class=\"comment\">// 强制转换key类型为RDD中原本的数据类型</span></span><br><span class=\"line\">​    <span class=\"keyword\">val</span> k = key.asInstanceOf[<span class=\"type\">K</span>]</span><br><span class=\"line\">​    <span class=\"keyword\">var</span> partition = <span class=\"number\">0</span></span><br><span class=\"line\">​    <span class=\"keyword\">if</span> (rangeBounds.length &lt;= <span class=\"number\">128</span>) &#123;</span><br><span class=\"line\">​      <span class=\"comment\">// 如果分区数据小于等于128个，那么直接本地循环寻找当前k所属的分区下标</span></span><br><span class=\"line\">​      <span class=\"comment\">// ordering.gt(x,y):如果x&gt;y,则返回true</span></span><br><span class=\"line\">​      <span class=\"keyword\">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class=\"line\">​        partition += <span class=\"number\">1</span></span><br><span class=\"line\">​      &#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"4-2-2-3-2-二分法查找\"><a href=\"#4-2-2-3-2-二分法查找\" class=\"headerlink\" title=\"4.2.2.3.2. 二分法查找\"></a>4.2.2.3.2. <strong>二分法查找</strong></h5><p>对于分区数大于128的情况，采样二分法查找<br> <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\">// 根据RDD的key值返回对应的分区id。从0开始</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\"> <span class=\"comment\">// 如果分区数量大于128个，那么使用二分查找方法寻找对应k所属的下标;</span></span><br><span class=\"line\">​      <span class=\"comment\">// 但是如果k在rangeBounds中没有出现，实质上返回的是一个负数(范围)或者是一个超过rangeBounds大小的数(最后一个分区，比所有数据都大)</span></span><br><span class=\"line\">​      <span class=\"comment\">// Determine which binary search method to use only once.</span></span><br><span class=\"line\">​      partition = binarySearch(rangeBounds, k)</span><br><span class=\"line\">​      <span class=\"comment\">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class=\"line\">​      <span class=\"keyword\">if</span> (partition &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">​        partition = -partition<span class=\"number\">-1</span></span><br><span class=\"line\">​      &#125;</span><br><span class=\"line\">​      <span class=\"keyword\">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class=\"line\">​        partition = rangeBounds.length</span><br><span class=\"line\">​      &#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"5-自定义分区器\"><a href=\"#5-自定义分区器\" class=\"headerlink\" title=\"5. 自定义分区器\"></a>5. <strong>自定义分区器</strong></h1><p>自定义：</p>\n<ol>\n<li>继承Partitioner方法，</li>\n<li>重写getPartition、numPartitions、equals等方法。<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">public <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyPartioner</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span> </span>&#123; </span><br><span class=\"line\">    <span class=\"meta\">@Override</span> </span><br><span class=\"line\">    public int numPartitions() &#123; </span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1000</span>; </span><br><span class=\"line\">    &#125; </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span> </span><br><span class=\"line\">    public int getPartition(<span class=\"type\">Object</span> key) &#123; </span><br><span class=\"line\">        <span class=\"type\">String</span> k = (<span class=\"type\">String</span>) key; </span><br><span class=\"line\">        int code = k.hashCode() % <span class=\"number\">1000</span>; </span><br><span class=\"line\">        <span class=\"type\">System</span>.out.println(k+<span class=\"string\">\":\"</span>+code); </span><br><span class=\"line\">        <span class=\"keyword\">return</span>  code &lt; <span class=\"number\">0</span>?code+<span class=\"number\">1000</span>:code; </span><br><span class=\"line\">    &#125; </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span> </span><br><span class=\"line\">    public boolean equals(<span class=\"type\">Object</span> obj) &#123; </span><br><span class=\"line\">        <span class=\"keyword\">if</span>(obj instanceof <span class=\"type\">MyPartioner</span>)&#123; </span><br><span class=\"line\">            <span class=\"keyword\">if</span>(<span class=\"keyword\">this</span>.numPartitions()==((<span class=\"type\">MyPartioner</span>) obj).numPartitions())&#123; </span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">true</span>; </span><br><span class=\"line\">            &#125; </span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; </span><br><span class=\"line\">        &#125; </span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.equals(obj); </span><br><span class=\"line\">    &#125; </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>调用：<code>pairRdd.groupbykey(new MyPartitioner())</code></p>\n<p>参考链接：<a href=\"https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html\" target=\"_blank\" rel=\"noopener\">https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkCore源码解析之shuffle","url":"https://sustcoder.github.io/2019/01/14/2018-12-01-sparkCore-sorceCodeAnalysis-shuffle/","content":"<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/shuffle.png\" alt=\"shuffle\"></p>\n<p>Shuffle Map的过程,即Shuffle Stage的ShuffleTask按照一定的规则将数据写到相应的文件中,并把写的文件”位置信息” 以MapOutput返回给DAGScheduler ,MapOutput将它更新到特定位置就完成了整个Shuffle Map过程.<br>在Spark中,Shuffle reduce过程抽象化为ShuffledRDD,即这个RDD的compute方法计算每一个分片即每一个reduce的数据是通过拉取ShuffleMap输出的文件并返回Iterator来实现的</p>\n<h1 id=\"1-对比MapReduce\"><a href=\"#1-对比MapReduce\" class=\"headerlink\" title=\"1. 对比MapReduce\"></a>1. <strong>对比MapReduce</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6F6F.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"1-1-宏观比较\"><a href=\"#1-1-宏观比较\" class=\"headerlink\" title=\"1.1. 宏观比较\"></a>1.1. <strong>宏观比较</strong></h2><p><strong>两者差别不大，度分为map和reduce两个阶段</strong>。</p>\n<p>从 high-level 的角度来看，两者并没有大的差别。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。</p>\n<h2 id=\"1-2-微观比较\"><a href=\"#1-2-微观比较\" class=\"headerlink\" title=\"1.2. 微观比较\"></a>1.2. <strong>微观比较</strong></h2><p>差别较大，Hadoop在Map和reduce阶段都有排序操作，而spark默认使用hash进行聚合，不会提前进行排序操作。</p>\n<p>从 low-level 的角度来看，两者差别不小。 Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作</p>\n<h2 id=\"1-3-实现方式\"><a href=\"#1-3-实现方式\" class=\"headerlink\" title=\"1.3. 实现方式\"></a>1.3. <strong>实现方式</strong></h2><p>mapreduce将处理流程进行细化出map,shuffle,sort,reduce等几个阶段，而spark只有一个stage和一系列的transformation()</p>\n<p>Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。</p>\n<h1 id=\"2-Map\"><a href=\"#2-Map\" class=\"headerlink\" title=\"2. Map\"></a>2. <strong>Map</strong></h1><p>为了分析方便，假定每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p>\n<p>shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。参见下面HashShuffleManager图示。</p>\n<h2 id=\"2-1-MapStatus的注册和获取\"><a href=\"#2-1-MapStatus的注册和获取\" class=\"headerlink\" title=\"2.1. MapStatus的注册和获取\"></a>2.1. <strong>MapStatus的注册和获取</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6F81.tmp.jpg\" alt=\"img\"><br><strong>参见:</strong> <a href=\"#_ShuffleMapStage\">ShuffleMapStage</a></p>\n<p>MapOutputTracker :是为MapOutput提供一个访问入口，提供了注册和获取MapStatus的接口。</p>\n<p>MapOutputTracker可以把每个Map输出的MapStatus注册到Tracker,同时Tracker也提供了访问接口,可以从该Tracker中读取指定每个ShuffleID所对应的map输出的位置;</p>\n<p>同时MapOutputTracker也是主从结构,其中Master提供了将Map输出注册到Tracker的入口, slave运行在每个Executor上,提供读取入口, 但是这个读取过程需要和Master进行交互,将指定的 ShuffleID所对应的MapStatus信息从Master中fetch过来;</p>\n<h3 id=\"2-1-1-MapOutputTrackerMaster\"><a href=\"#2-1-1-MapOutputTrackerMaster\" class=\"headerlink\" title=\"2.1.1. MapOutputTrackerMaster\"></a>2.1.1. <strong>MapOutputTrackerMaster</strong></h3><p><strong>参见:</strong> <a href=\"#___stage\">提交stage</a></p>\n<p><strong>driver端，记录shuffle信息</strong></p>\n<p><strong>MapStatus数据记录的格式：{shuffleId,mapId,MapStatus}</strong></p>\n<p>每个Shuffle都对应一个ShuffleID,该ShuffleID下面对应多个MapID,每个MapID都会输出一个MapStatus,通过该MapStatus,可以定位每个 MapID所对应的ShuffleMapTask运行过程中所对应的机器</p>\n<p>通过shuffleID进行索引,存储了所有注册到tracker的Shuffle, 通过registerShuffle可以进行注册Shuffle, 通过registerMapOutput可以在每次ShuffleMapTask结束以后,将Map的输出注册到Track中; 同时提供了getSerializedMapOutputStatuses接口 将一个Shuffle所有的MapStatus进行序列化并进行返回;</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapOutputTrackerMaster</span></span>&#123;</span><br><span class=\"line\"><span class=\"keyword\">val</span> shuffleStatuses = <span class=\"keyword\">new</span> <span class=\"type\">ConcurrentHashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ShuffleStatus</span>]().asScala</span><br><span class=\"line\"><span class=\"keyword\">val</span> mapStatuses = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">MapStatus</span>](numPartitions)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在创建stage时，初始化ShuffleStatus</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">registerShuffle</span></span>(shuffleId: <span class=\"type\">Int</span>, numMaps: <span class=\"type\">Int</span>) &#123;</span><br><span class=\"line\">    shuffleStatuses.put(</span><br><span class=\"line\">\t\tshuffleId,<span class=\"keyword\">new</span> <span class=\"type\">ShuffleStatus</span>(numMaps)) </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 将MapTask的输出注册到Track中</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">registerMapOutput</span></span>(shuffleId: <span class=\"type\">Int</span>, mapId: <span class=\"type\">Int</span>, status: <span class=\"type\">MapStatus</span>) &#123;</span><br><span class=\"line\">    **<span class=\"comment\">// &#123;shuffleId,mapId,MapStatus&#125;**</span></span><br><span class=\"line\">\tshuffleStatuses(shuffleId).addMapOutput(mapId, status)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addMapOutput</span></span>(mapId: <span class=\"type\">Int</span>, status: <span class=\"type\">MapStatus</span>): <span class=\"type\">Unit</span> = synchronized &#123;</span><br><span class=\"line\">    mapStatuses(mapId) = status</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// mapStatus中包含了task运行位置，partitions数量等信息</span></span><br><span class=\"line\"><span class=\"type\">MapStatus</span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">location</span></span>: <span class=\"type\">BlockManagerId</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getSizeForBlock</span></span>(reduceId: <span class=\"type\">Int</span>): <span class=\"type\">Long</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-1-2-MapOutputTrackerWorker\"><a href=\"#2-1-2-MapOutputTrackerWorker\" class=\"headerlink\" title=\"2.1.2. MapOutputTrackerWorker\"></a>2.1.2. <strong>MapOutputTrackerWorker</strong></h3><p>excutor端获取shuffle信息，注意：local模式下是直接从trackerMaster获取信息的（worker和master拥有相同的父类，local模式下直接获取不用再走RPC调用）</p>\n<p>MapOutputTrackerWorker的实现很简单,核心功能就是getServerStatuses, 它获取指定Shuffle的每个reduce所对应的MapStatus信息</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapOutputWorker</span></span>&#123;</span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMapSizesByExecutorId</span></span>(shuffleId: <span class=\"type\">Int</span>, startPartition: <span class=\"type\">Int</span>, endPartition: <span class=\"type\">Int</span>)</span><br><span class=\"line\">​      : <span class=\"type\">Seq</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\"><span class=\"comment\">// 根据shuffleId获取MapStatus集合</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> statuses = getStatuses(shuffleId)</span><br><span class=\"line\"><span class=\"comment\">// 根据shuffleId和起始分区，从mapStatus获取响应的blockManager信息</span></span><br><span class=\"line\"><span class=\"type\">MapOutputTracker</span>.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 发送消息给trackerMaster，获取mapOutPut信息</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">askTracker</span></span>&#123;</span><br><span class=\"line\">​\t<span class=\"keyword\">var</span> trackerEndpoint: <span class=\"type\">RpcEndpointRef</span> = _</span><br><span class=\"line\">​\ttrackerEndpoint.askSync[<span class=\"type\">T</span>](message)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-2-写数据\"><a href=\"#2-2-写数据\" class=\"headerlink\" title=\"2.2. 写数据\"></a>2.2. <strong>写数据</strong></h2><p>ShuffleMapTask负责写数据操作，最后会生成.data和.index文件，在执行完毕后返回一个MapStatus对象。</p>\n<p>ShuffleMapTask在excutor上获取到具体的writer后进行实际的写操作</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShuffleMapTask</span> <span class=\"keyword\">extends</span> <span class=\"title\">Task</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">def runTask(context: <span class=\"type\">TaskContext</span></span>)</span>: <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 反序列化接收到的数据</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> (rdd, dep) = closureSerializer.deserialize(</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 调用ShuffleManager的getWriter方法获取一组writer</span></span><br><span class=\"line\"> writer = manager.getWriter(dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">\t <span class=\"comment\">// 遍历RDD进行write</span></span><br><span class=\"line\">    writer.write(）</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-1-writer\"><a href=\"#2-2-1-writer\" class=\"headerlink\" title=\"2.2.1. writer\"></a>2.2.1. <strong>writer</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6F83.tmp.jpg\" alt=\"img\"><br><strong>参见:</strong> <a href=\"#_writer_2\">writer</a></p>\n<p> Get a writer for a given partition. Called on executors by map tasks.</p>\n<p>因为Shuffle过程中需要将Map结果数据输出到文件，所以需要通过注册一个ShuffleHandle来获取到一个ShuffleWriter对象，通过它来控制Map阶段记录数据输出的行为。其中，ShuffleHandle包含了如下基本信息：</p>\n<p>shuffleId：标识Shuffle过程的唯一ID<br>numMaps：RDD对应的Partitioner指定的Partition的个数，也就是ShuffleMapTask输出的Partition个数<br>dependency：RDD对应的依赖ShuffleDependency</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SortShuffleManager</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getWriter</span></span>()&#123;</span><br><span class=\"line\">​\thandle <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">​      <span class=\"keyword\">case</span> <span class=\"type\">SerializedShuffleHandle</span>=&gt;</span><br><span class=\"line\">​\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">UnsafeShuffleWriter</span>()</span><br><span class=\"line\">​\t  <span class=\"keyword\">case</span> <span class=\"type\">BypassMergeSortShuffleHandle</span>=&gt;</span><br><span class=\"line\">​\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">BypassMergeSortShuffleWriter</span>()</span><br><span class=\"line\">​\t  <span class=\"keyword\">case</span> <span class=\"type\">BaseShuffleHandle</span>=&gt;</span><br><span class=\"line\">​\t\t\t <span class=\"keyword\">new</span> <span class=\"type\">SortShuffleWriter</span>()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-2-1-1-BypassMergeSortShuffleWriter\"><a href=\"#2-2-1-1-BypassMergeSortShuffleWriter\" class=\"headerlink\" title=\"2.2.1.1. BypassMergeSortShuffleWriter\"></a>2.2.1.1. <strong>BypassMergeSortShuffleWriter</strong></h4><ol>\n<li>按照hash方式排序</li>\n<li>每个partition产生一个file,然后将相同task产生的文件进行合并。blocks的偏移量被单独存放在一个索引文件中</li>\n<li>通过IndexShuffleBlockResolver对写入的数据进行缓存</li>\n<li>使用场景：<ol>\n<li>不用排序</li>\n<li>没有聚合函数</li>\n<li>分区数量少于设置的阈值<br>spark.shuffle.sort.bypassMergeThreshold默认值是200<h4 id=\"2-2-1-2-UnsafeShuffleWriter\"><a href=\"#2-2-1-2-UnsafeShuffleWriter\" class=\"headerlink\" title=\"2.2.1.2. UnsafeShuffleWriter\"></a>2.2.1.2. <strong>UnsafeShuffleWriter</strong></h4></li>\n</ol>\n</li>\n</ol>\n<p>如果ShuffleDependency中的Serializer，允许对将要输出数据对象进行排序后，再执行序列化写入到文件，则会选择创建一个SerializedShuffleHandle，生成一个UnsafeShuffleWriter</p>\n<h4 id=\"2-2-1-3-SortShuffleWriter\"><a href=\"#2-2-1-3-SortShuffleWriter\" class=\"headerlink\" title=\"2.2.1.3. SortShuffleWriter\"></a>2.2.1.3. <strong>SortShuffleWriter</strong></h4><p>除了上面两种ShuffleHandle以后，其他情况都会创建一个BaseShuffleHandle对象，它会以反序列化的格式处理Shuffle输出数据。</p>\n<p>数据记录格式:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// shuffle_shuffleId_mapId_reducId</span></span><br><span class=\"line\">shuffle_2901_11825_0.data</span><br><span class=\"line\">shuffle_2901_11825_0.index</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-3-1-写文件\"><a href=\"#2-2-1-3-1-写文件\" class=\"headerlink\" title=\"2.2.1.3.1. 写文件\"></a>2.2.1.3.1. <strong>写文件</strong></h5><h6 id=\"2-2-1-3-1-1-数据格式\"><a href=\"#2-2-1-3-1-1-数据格式\" class=\"headerlink\" title=\"2.2.1.3.1.1. 数据格式\"></a>2.2.1.3.1.1. <strong>数据格式</strong></h6><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6F96.tmp.jpg\" alt=\"img\"> </p>\n<p>数据格式有两种，如果不需要合并则使用buffer，如果需要合并使用map</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ExternalSorter</span></span>&#123;</span><br><span class=\"line\">map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>&#123;</span><br><span class=\"line\">​\t<span class=\"keyword\">if</span>(shouldCombine)&#123;</span><br><span class=\"line\">​\t\tmap.changeValue()</span><br><span class=\"line\">​\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">​\t\t buffer.insert()</span><br><span class=\"line\">​\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>2.2.1.3.1.1.1. map</p>\n<p>在Map阶段会执行Combine操作，在Map阶段进行Combine操作能够降低Map阶段数据记录的总数，从而降低Shuffle过程中数据的跨网络拷贝传输。这时，RDD对应的ShuffleDependency需要设置一个Aggregator用来执行Combine操作</p>\n<p>map是内存数据结构，最重要的是update函数和map的changeValue方法（这里的map对应的实现类是PartitionedAppendOnlyMap）。update函数所做的工作，其实就是对createCombiner和mergeValue这两个函数的使用，第一次遇到一个Key调用createCombiner函数处理，非首次遇到同一个Key对应新的Value调用mergeValue函数进行合并处理。map的changeValue方法主要是将Key和Value在map中存储或者进行修改（对出现的同一个Key的多个Value进行合并，并将合并后的新Value替换旧Value）。<br>PartitionedAppendOnlyMap是一个经过优化的哈希表，它支持向map中追加数据，以及修改Key对应的Value，但是不支持删除某个Key及其对应的Value。它能够支持的存储容量是0.7 * 2 ^ 29 = 375809638。当达到指定存储容量或者指定限制，就会将map中记录数据Spill到磁盘文件，这个过程和前面的类似</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ExternalSorter</span></span>&#123;</span><br><span class=\"line\">map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>&#123;</span><br><span class=\"line\">​\t<span class=\"keyword\">if</span>(shouldCombine)&#123;</span><br><span class=\"line\"><span class=\"comment\">// 定义一个aggtregator函数</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> mergeValue = aggregator.get.mergeValue</span><br><span class=\"line\">​      <span class=\"keyword\">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class=\"line\">​      <span class=\"keyword\">var</span> kv: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">​      <span class=\"keyword\">val</span> update = (hadValue: <span class=\"type\">Boolean</span>, oldValue: <span class=\"type\">C</span>) =&gt; &#123;</span><br><span class=\"line\">​        <span class=\"keyword\">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class=\"keyword\">else</span> createCombiner(kv._2)</span><br><span class=\"line\">​      &#125;</span><br><span class=\"line\"><span class=\"comment\">// 使用update函数实现对新增元素的合并操作</span></span><br><span class=\"line\">map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class=\"line\">​\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">​\t\t buffer.insert()</span><br><span class=\"line\">​\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>2.2.1.3.1.1.2. buffer</p>\n<p>map端不需要排序时使用的数据存储格式</p>\n<p>Map阶段不进行Combine操作，在内存中缓存记录数据会使用PartitionedPairBuffer这种数据结构来缓存、排序记录数据，它是一个Append-only Buffer，仅支持向Buffer中追加数据键值对记录</p>\n<ol>\n<li>buffer大小：默认64，最大2 ^ 30 - 1</li>\n</ol>\n<h6 id=\"2-2-1-3-1-2-spill\"><a href=\"#2-2-1-3-1-2-spill\" class=\"headerlink\" title=\"2.2.1.3.1.2. spill\"></a>2.2.1.3.1.2. <strong>spill</strong></h6><p>组装完数据后写磁盘</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ExternalSorter</span></span>&#123;</span><br><span class=\"line\">map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>&#123;</span><br><span class=\"line\">​\t<span class=\"keyword\">if</span>(shouldCombine)&#123;</span><br><span class=\"line\">​\t\tmaybeSpillCollection(usingMap = <span class=\"literal\">true</span>)</span><br><span class=\"line\">​\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">​\t\t maybeSpillCollection(usingMap = <span class=\"literal\">false</span>)</span><br><span class=\"line\">​\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-3-2-建索引\"><a href=\"#2-2-1-3-2-建索引\" class=\"headerlink\" title=\"2.2.1.3.2. 建索引\"></a>2.2.1.3.2. <strong>建索引</strong></h5><h3 id=\"2-2-2-写顺序\"><a href=\"#2-2-2-写顺序\" class=\"headerlink\" title=\"2.2.2. 写顺序\"></a>2.2.2. <strong>写顺序</strong></h3><h1 id=\"3-reduce\"><a href=\"#3-reduce\" class=\"headerlink\" title=\"3. reduce\"></a>3. <strong>reduce</strong></h1><p>shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task为下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p>\n<p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果</p>\n<h2 id=\"3-1-读数据\"><a href=\"#3-1-读数据\" class=\"headerlink\" title=\"3.1. 读数据\"></a>3.1. <strong>读数据</strong></h2><h2 id=\"3-2-reduce端获取\"><a href=\"#3-2-reduce端获取\" class=\"headerlink\" title=\"3.2. reduce端获取\"></a>3.2. <strong>reduce端获取</strong></h2><p><strong>参见:</strong> <a href=\"#_ResultTask\">ResultTask</a></p>\n<p>调用ShuffleManager通过getReader方法获取具体的Reader，去读数据。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShuffledRDD</span> </span>&#123;</span><br><span class=\"line\">​\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>()&#123;</span><br><span class=\"line\">​\t\t <span class=\"keyword\">val</span> dep = dependencies.head.asInstanceOf[<span class=\"type\">ShuffleDependency</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">​    <span class=\"type\">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class=\"number\">1</span>, context)</span><br><span class=\"line\">​      .read()</span><br><span class=\"line\">​      .asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">​\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"4-shuffle管理入口\"><a href=\"#4-shuffle管理入口\" class=\"headerlink\" title=\"4. shuffle管理入口\"></a>4. <strong>shuffle管理入口</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6F98.tmp.jpg\" alt=\"img\"> </p>\n<p>ShuffleManager是shuffle整个过程的管理入口，对外提供读写等接口。</p>\n<p>ShuffleManager在driver中创建，driver用ShuffleManager进行注册shuffle，执行读写操作等</p>\n<p>对Shuffle做了什么优化来提供Spark的性能,本质上就是对ShuffleManager进行优化和提供新的实现</p>\n<p>spark2.2.0中已取消对HashShuffleManager的支持<br>新增了tungsten-sort。</p>\n<p>ShuffleManager有两种实现HashShuffleManager和SorShuffleManager,1.1一会的版本默认是SortShuffleManger,可通过<br>conf.get(“spark.shuffle.manager”, “sort”) 修改默认的shuffle实现方式</p>\n<p>SortShuffleManager和HashShuffleManager有一个本质的差别,即同一个map的多个reduce的数据都写入到同一个文件中;那么SortShuffleManager产生的Shuffle 文件个数为2*Map个数</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// shuffleManger提供的功能</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">ShuffleManager</span> </span>&#123;</span><br><span class=\"line\"> <span class=\"comment\">// shuffle注册</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">registerShuffle</span></span>(shuffleId: <span class=\"type\">Int</span>, numMaps: <span class=\"type\">Int</span>,dependency: <span class=\"type\">ShuffleDependency</span>): <span class=\"type\">ShuffleHandle</span></span><br><span class=\"line\"><span class=\"comment\">// shuffle注销</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unregisterShuffle</span></span>(shuffleId: <span class=\"type\">Int</span>): <span class=\"type\">Boolean</span></span><br><span class=\"line\"><span class=\"comment\">// mapTask返回一组Writer</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getWriter</span></span>(handle: <span class=\"type\">ShuffleHandle</span>, mapId: <span class=\"type\">Int</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">ShuffleWriter</span></span><br><span class=\"line\"><span class=\"comment\">// 提供Start分区编号和end分区编号;当然一般情况如果每个reduce单独运行,那么start-end区间也只对应一个reduce</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getReader</span></span>(handle: <span class=\"type\">ShuffleHandle</span>,startPartition: <span class=\"type\">Int</span>,endPartition: <span class=\"type\">Int</span>,context: <span class=\"type\">TaskContext</span>): <span class=\"type\">ShuffleReader</span></span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">shuffleBlockManager</span></span>: <span class=\"type\">ShuffleBlockManager</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">stop</span></span>(): <span class=\"type\">Unit</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-1-HashShuffleManager\"><a href=\"#4-1-HashShuffleManager\" class=\"headerlink\" title=\"4.1. HashShuffleManager\"></a>4.1. <strong>HashShuffleManager</strong></h2><p>spark2.2.0中已取消对HashShuffleManager的支持<br>(SPARK-14667)。参考：<a href=\"http://lxw1234.com/archives/2016/05/666.htm\" target=\"_blank\" rel=\"noopener\">http://lxw1234.com/archives/2016/05/666.htm</a></p>\n<p>HashShuffleManager是Spark最早版本的ShuffleManager，该ShuffleManager的严重缺点是会产生太多小文件，特别是reduce个数很多时候，存在很大的性能瓶颈。</p>\n<p>最初版本：ShuffleMapTask个数×reduce个数<br>后期版本：<br>并发的ShuffleMapTask的个数为M<br>xreduce个数</p>\n<h2 id=\"4-2-SortShuffleManager\"><a href=\"#4-2-SortShuffleManager\" class=\"headerlink\" title=\"4.2. SortShuffleManager\"></a>4.2. <strong>SortShuffleManager</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6F99.tmp.jpg\" alt=\"img\"> </p>\n<p>参考：<a href=\"http://shiyanjun.cn/archives/1655.html\" target=\"_blank\" rel=\"noopener\">http://shiyanjun.cn/archives/1655.html</a></p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6F9A.tmp.png\" alt=\"img\"> </p>\n<h3 id=\"4-2-1-reader\"><a href=\"#4-2-1-reader\" class=\"headerlink\" title=\"4.2.1. reader\"></a>4.2.1. <strong>reader</strong></h3><h4 id=\"4-2-1-1-BlockStoreShuffleReader\"><a href=\"#4-2-1-1-BlockStoreShuffleReader\" class=\"headerlink\" title=\"4.2.1.1. BlockStoreShuffleReader\"></a>4.2.1.1. <strong>BlockStoreShuffleReader</strong></h4><p>根据partition的起止位置，从别的节点获取blockURL,node信息组成reader，</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockStoreShuffleReader</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    handle: <span class=\"type\">BaseShuffleHandle</span>[<span class=\"type\">K</span>, _, <span class=\"type\">C</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    startPartition: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    endPartition: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    context: <span class=\"type\">TaskContext</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    serializerManager: <span class=\"type\">SerializerManager</span> = \t\t<span class=\"type\">SparkEnv</span>.get.serializerManager,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    blockManager: <span class=\"type\">BlockManager</span> = \t\t<span class=\"type\">SparkEnv</span>.get.blockManager,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    mapOutputTracker: <span class=\"type\">MapOutputTracker</span> = \t\t<span class=\"type\">SparkEnv</span>.get.mapOutputTracker</span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">ShuffleReader</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] <span class=\"keyword\">with</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> blocksByAddress=</span><br><span class=\"line\">\tmapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 根据获取到的block信息，给trackerMaster发送消息，获取RDD数据</span></span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">ShuffleBlockFetcherIterator</span>(</span><br><span class=\"line\">\t\tblocksByAddress</span><br><span class=\"line\">\t)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapOutputTracker</span></span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// excutor在计算ShuffleRDD时调用，返回&#123;blocak地址，Seq&#123;blockID,和输出数据大小&#125;&#125;等</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMapSizesByExecutorId</span></span>(shuffleId: <span class=\"type\">Int</span>, startPartition: <span class=\"type\">Int</span>, endPartition: <span class=\"type\">Int</span>)</span><br><span class=\"line\">      : <span class=\"type\">Seq</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])]</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShuffleBlockFetcherIterator</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> results = </span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">LinkedBlockingQueue</span>[<span class=\"type\">FetchResult</span>]</span><br><span class=\"line\">\t<span class=\"comment\">// 负责发送请求和接收数据</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sendRequest</span></span>(req: <span class=\"type\">FetchRequest</span>)&#123;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t<span class=\"comment\">// 将接收到的数据放入到队列中</span></span><br><span class=\"line\">\tresults.put(<span class=\"keyword\">new</span> <span class=\"type\">SuccessFetchResult</span>(</span><br><span class=\"line\">\t  blockId: <span class=\"type\">BlockId</span>,</span><br><span class=\"line\">      address: <span class=\"type\">BlockManagerId</span>,</span><br><span class=\"line\">      size: <span class=\"type\">Long</span>,</span><br><span class=\"line\">      **buf: <span class=\"type\">ManagedBuffer</span>,**</span><br><span class=\"line\">      isNetworkReqDone: <span class=\"type\">Boolean</span></span><br><span class=\"line\">\t))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-2-writer\"><a href=\"#4-2-2-writer\" class=\"headerlink\" title=\"4.2.2. writer\"></a>4.2.2. <strong>writer</strong></h3><p><strong>参见:</strong> <a href=\"#_writer\">writer</a></p>\n<p>Shuffle过程中需要将Map结果数据输出到文件，所以需要通过注册一个ShuffleHandle来获取到一个ShuffleWriter对象，通过它来控制Map阶段记录数据输出的行为</p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkCore源码解析之block","url":"https://sustcoder.github.io/2019/01/14/2018-12-01-sparkCore-sorceCodeAnalysis-block/","content":"<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/Block.png\" alt=\"block\"></p>\n<h1 id=\"1-标记\"><a href=\"#1-标记\" class=\"headerlink\" title=\"1. 标记\"></a>1. <strong>标记</strong></h1><p>Apache Spark中，对Block的查询、存储管理，是通过唯一的Block ID来进行区分的。</p>\n<p>同一个Spark Application，以及多个运行的Application之间，对应的Block都具有唯一的ID</p>\n<h2 id=\"1-1-种类\"><a href=\"#1-1-种类\" class=\"headerlink\" title=\"1.1. 种类\"></a>1.1. <strong>种类</strong></h2><p>需要在worker和driver间共享数据时，就需要对这个数据进行唯一的标识，常用的需要传输的block信息有以下几类<br>RDDBlockId、ShuffleBlockId、ShuffleDataBlockId、ShuffleIndexBlockId、BroadcastBlockId、TaskResultBlockId、TempLocalBlockId、TempShuffleBlockId</p>\n<h2 id=\"1-2-生成规则\"><a href=\"#1-2-生成规则\" class=\"headerlink\" title=\"1.2. 生成规则\"></a>1.2. <strong>生成规则</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">RDDBlockId</span> : <span class=\"string\">\"rdd_\"</span> + rddId + <span class=\"string\">\"_\"</span> + splitIndex</span><br><span class=\"line\"><span class=\"type\">ShuffleBlockId</span> : <span class=\"string\">\"shuffle_\"</span> + shuffleId + <span class=\"string\">\"_\"</span> + mapId + <span class=\"string\">\"_\"</span> + reduceId</span><br><span class=\"line\"><span class=\"type\">ShuffleDataBlockId</span>:<span class=\"string\">\"shuffle_\"</span> + shuffleId + <span class=\"string\">\"_\"</span> + mapId + <span class=\"string\">\"_\"</span> + reduceId + <span class=\"string\">\".data\"</span></span><br><span class=\"line\"><span class=\"type\">ShuffleIndexBlockId</span>:<span class=\"string\">\"shuffle_\"</span> + shuffleId + <span class=\"string\">\"_\"</span> + mapId + <span class=\"string\">\"_\"</span> + reduceId + <span class=\"string\">\".index\"</span></span><br><span class=\"line\"><span class=\"type\">TaskResultBlockId</span>:<span class=\"string\">\"taskresult_\"</span> + taskId</span><br><span class=\"line\"><span class=\"type\">StreamBlockId</span>:<span class=\"string\">\"input-\"</span> + streamId + <span class=\"string\">\"-\"</span> + uniqueId</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<h1 id=\"2-存储\"><a href=\"#2-存储\" class=\"headerlink\" title=\"2. 存储\"></a>2. <strong>存储</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6E0A.tmp.jpg\" alt=\"img\"> </p>\n<p>DiskStore是通过DiskBlockManager进行管理存储到磁盘上的Block数据文件的，在同一个节点上的多个Executor共享相同的磁盘文件路径，相同的Block数据文件也就会被同一个节点上的多个Executor所共享。而对应MemoryStore，因为每个Executor对应独立的JVM实例，从而具有独立的Storage/Execution内存管理，所以使用MemoryStore不能共享同一个Block数据，但是同一个节点上的多个Executor之间的MemoryStore之间拷贝数据，比跨网络传输要高效的多</p>\n<h2 id=\"2-1-MemoryStore\"><a href=\"#2-1-MemoryStore\" class=\"headerlink\" title=\"2.1. MemoryStore\"></a>2.1. <strong>MemoryStore</strong></h2><p>数据在内存中存储的形式</p>\n<ol>\n<li>以序列化格式</li>\n<li>以反序列化的形式<br> ​    2.1 Block数据记录能够完全放到内存中<br> ​    2.2 Block数据记录只能部分放到内存中：申请Unroll内存（预占内存）</li>\n<li>以序列化二进制格式保存Block数据</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">MEMORY_ONLY</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_ONLY_2</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_ONLY_SER</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_ONLY_SER_2</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK_2</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK_SER</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK_SER_2</span></span><br><span class=\"line\"><span class=\"type\">OFF_HEAP</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-2-DiskStore\"><a href=\"#2-2-DiskStore\" class=\"headerlink\" title=\"2.2. DiskStore\"></a>2.2. <strong>DiskStore</strong></h2><p>数据罗盘的几种形式：</p>\n<ol>\n<li>通过文件流写Block数据</li>\n<li>将二进制Block数据写入文件</li>\n<li><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">DISK_ONLY</span></span><br><span class=\"line\"><span class=\"type\">DISK_ONLY_2</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK_2</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK_SER</span></span><br><span class=\"line\"><span class=\"type\">MEMORY_AND_DISK_SER_2</span></span><br><span class=\"line\"><span class=\"type\">OFF_HEAP</span></span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"2-2-1-DiskBlockManager\"><a href=\"#2-2-1-DiskBlockManager\" class=\"headerlink\" title=\"2.2.1. DiskBlockManager\"></a>2.2.1. <strong>DiskBlockManager</strong></h3><p>DiskStore即基于文件来存储Block. 基于Disk来存储,首先必须要解决一个问题就是磁盘文件的管理:磁盘目录结构的组成,目录的清理等,在Spark对磁盘文件的管理是通过 DiskBlockManager来进行管理的</p>\n<p>DiskBlockManager管理了每个Block数据存储位置的信息，包括从Block ID到磁盘上文件的映射关系。DiskBlockManager主要有如下几个功能：</p>\n<ol>\n<li>负责创建一个本地节点上的指定磁盘目录，用来存储Block数据到指定文件中</li>\n<li>如果Block数据想要落盘，需要通过调用getFile方法来分配一个唯一的文件路径</li>\n<li>如果想要查询一个Block是否在磁盘上，通过调用containsBlock方法来查询</li>\n<li>查询当前节点上管理的全部Block文件<br>通过调用createTempLocalBlock方法，生成一个唯一Block ID，并创建一个唯一的临时文件，用来存储中间结果数据</li>\n<li>通过调用createTempShuffleBlock方法，生成一个唯一Block ID，并创建一个唯一的临时文件，用来存储Shuffle过程的中间结果数据<h2 id=\"2-3-offHeap\"><a href=\"#2-3-offHeap\" class=\"headerlink\" title=\"2.3. offHeap\"></a>2.3. <strong>offHeap</strong></h2></li>\n</ol>\n<p>堆外存储不支持序列化和副本</p>\n<p>Spark中实现的OffHeap是基于Tachyon:分布式内存文件系统来实现的</p>\n<h1 id=\"3-内存管理模型\"><a href=\"#3-内存管理模型\" class=\"headerlink\" title=\"3. 内存管理模型\"></a>3. <strong>内存管理模型</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6E0C.tmp.jpg\" alt=\"img\"> </p>\n<p>在Spark Application提交以后，最终会在Worker上启动独立的Executor JVM，Task就运行在Executor里面。在一个Executor JVM内部，内存管理模型就是管理excutor运行所需要的内存</p>\n<p><a href=\"http://shiyanjun.cn/archives/1585.html\" target=\"_blank\" rel=\"noopener\">http://shiyanjun.cn/archives/1585.html</a></p>\n<h2 id=\"3-1-StaticMemoryManager\"><a href=\"#3-1-StaticMemoryManager\" class=\"headerlink\" title=\"3.1. StaticMemoryManager\"></a>3.1. <strong>StaticMemoryManager</strong></h2><p>1.5之前版本使用<br>缺点：</p>\n<ol>\n<li>没有一个合理的默认值能够适应不同计算场景下的Workload</li>\n<li>内存调优困难，需要对Spark内部原理非常熟悉才能做好</li>\n<li>对不需要Cache的Application的计算场景，只能使用很少一部分内存<h2 id=\"3-2-UnifiedMemoryManager\"><a href=\"#3-2-UnifiedMemoryManager\" class=\"headerlink\" title=\"3.2. UnifiedMemoryManager\"></a>3.2. <strong>UnifiedMemoryManager</strong></h2><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6E0D.tmp.jpg\" alt=\"img\"> </li>\n</ol>\n<p>统一内存分配管理模型：</p>\n<ol>\n<li>可以动态的分配excution和storage的内存大小</li>\n<li>不仅可以分配堆内内存，也可以分配堆外内存</li>\n<li>堆外内存和分配比例都可以通过参数配置</li>\n<li>内存的分配和回收是通过MemoryPool控制</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MemoryManager</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">​    conf: <span class=\"type\">SparkConf</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">​    numCores: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">​    onHeapStorageMemory: <span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">​    onHeapExecutionMemory: <span class=\"type\">Long</span></span>)</span>&#123;</span><br><span class=\"line\">​\t</span><br><span class=\"line\"> <span class=\"comment\">// storage堆内内存</span></span><br><span class=\"line\">  <span class=\"meta\">@GuardedBy</span>(<span class=\"string\">\"this\"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"keyword\">val</span> onHeapStorageMemoryPool = \t\t<span class=\"keyword\">new</span> <span class=\"type\">StorageMemoryPool</span>(<span class=\"keyword\">this</span>, \t\t<span class=\"type\">MemoryMode</span>.<span class=\"type\">ON_HEAP</span>)</span><br><span class=\"line\"> <span class=\"comment\">// storage堆外内存</span></span><br><span class=\"line\">  <span class=\"meta\">@GuardedBy</span>(<span class=\"string\">\"this\"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"keyword\">val</span> offHeapStorageMemoryPool = \t\t<span class=\"keyword\">new</span> <span class=\"type\">StorageMemoryPool</span>(<span class=\"keyword\">this</span>, \t\t<span class=\"type\">MemoryMode</span>.<span class=\"type\">OFF_HEAP</span>)</span><br><span class=\"line\"><span class=\"comment\">// execution堆内内存</span></span><br><span class=\"line\">  <span class=\"meta\">@GuardedBy</span>(<span class=\"string\">\"this\"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"keyword\">val</span> onHeapExecutionMemoryPool = \t\t<span class=\"keyword\">new</span> <span class=\"type\">ExecutionMemoryPool</span>(<span class=\"keyword\">this</span>, \t\t<span class=\"type\">MemoryMode</span>.<span class=\"type\">ON_HEAP</span>)</span><br><span class=\"line\"><span class=\"comment\">// excution堆外内存</span></span><br><span class=\"line\">  <span class=\"meta\">@GuardedBy</span>(<span class=\"string\">\"this\"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"keyword\">val</span> offHeapExecutionMemoryPool = \t\t<span class=\"keyword\">new</span> <span class=\"type\">ExecutionMemoryPool</span>(<span class=\"keyword\">this</span>, \t\t<span class=\"type\">MemoryMode</span>.<span class=\"type\">OFF_HEAP</span>)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 默认最大堆内存</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> maxOffHeapMemory = conf.getSizeAsBytes(<span class=\"string\">\"spark.memory.offHeap.size\"</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 默认storage和excution的内存大小各占50%</span></span><br><span class=\"line\">offHeapStorageMemory =</span><br><span class=\"line\">​    (maxOffHeapMemory * conf.getDouble(<span class=\"string\">\"spark.memory.storageFraction\"</span>, <span class=\"number\">0.5</span>)).toLong</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-1-内存划分\"><a href=\"#3-2-1-内存划分\" class=\"headerlink\" title=\"3.2.1. 内存划分\"></a>3.2.1. <strong>内存划分</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6E1E.tmp.jpg\" alt=\"img\"> </p>\n<p>在统一内存管理模型中，storage和excution内存大小可以动态调整，在一定程度上减少了OOM发生概率</p>\n<p>默认内存划分：</p>\n<p>预留内存reservedMemory=300M<br>管理内存maxHeapMemory = (systemMemory - reservedMemory) <em> 0.6<br>storageMemory=excutionMemory=maxHeapMemory</em>0.5</p>\n<p>非堆内存默认值0，可通过spark.memory.offHeap.size参数调整，其中storage和excution的内存占比也均为50%</p>\n<h4 id=\"3-2-1-1-Storage内存区\"><a href=\"#3-2-1-1-Storage内存区\" class=\"headerlink\" title=\"3.2.1.1. Storage内存区\"></a>3.2.1.1. <strong>Storage内存区</strong></h4><p>Storage内存，用来缓存Task数据、在Spark集群中传输（Propagation）内部数据</p>\n<h4 id=\"3-2-1-2-Execution内存区\"><a href=\"#3-2-1-2-Execution内存区\" class=\"headerlink\" title=\"3.2.1.2. Execution内存区\"></a>3.2.1.2. <strong>Execution内存区</strong></h4><p>Execution内存，用于满足Shuffle、Join、Sort、Aggregation计算过程中对内存的需求</p>\n<h4 id=\"3-2-1-3-预留内存\"><a href=\"#3-2-1-3-预留内存\" class=\"headerlink\" title=\"3.2.1.3. 预留内存\"></a>3.2.1.3. <strong>预留内存</strong></h4><h4 id=\"3-2-1-4-非堆内存\"><a href=\"#3-2-1-4-非堆内存\" class=\"headerlink\" title=\"3.2.1.4. 非堆内存\"></a>3.2.1.4. <strong>非堆内存</strong></h4><h3 id=\"3-2-2-内存调控\"><a href=\"#3-2-2-内存调控\" class=\"headerlink\" title=\"3.2.2. 内存调控\"></a>3.2.2. <strong>内存调控</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wps6E2E.tmp.jpg\" alt=\"img\"> </p>\n<h4 id=\"3-2-2-1-Storage内存\"><a href=\"#3-2-2-1-Storage内存\" class=\"headerlink\" title=\"3.2.2.1. Storage内存\"></a>3.2.2.1. <strong>Storage内存</strong></h4><h5 id=\"3-2-2-1-1-申请\"><a href=\"#3-2-2-1-1-申请\" class=\"headerlink\" title=\"3.2.2.1.1. 申请\"></a>3.2.2.1.1. <strong>申请</strong></h5><ol>\n<li>判断申请内存类型：堆内还是堆外</li>\n<li>如果申请内存大于剩余内存总量则申请失败</li>\n<li>如果申请内存大小在storage内存范围内则直接分配</li>\n<li>如果申请内存大于storage剩余内存则借用excution内存</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">// 为blockId申请numBytes字节大小的内存</span></span><br><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">acquireStorageMemory</span> </span>()synchronized &#123; </span><br><span class=\"line\">  <span class=\"keyword\">val</span> (executionPool, storagePool, maxMemory) = memoryMode <span class=\"keyword\">match</span> &#123; </span><br><span class=\"line\"><span class=\"comment\">// 根据memoryMode值，返回对应的StorageMemoryPool与ExecutionMemoryPool</span></span><br><span class=\"line\">​    <span class=\"keyword\">case</span> <span class=\"type\">MemoryMode</span>.<span class=\"type\">ON_HEAP</span> =&gt;</span><br><span class=\"line\">​    <span class=\"keyword\">case</span> <span class=\"type\">MemoryMode</span>.<span class=\"type\">OFF_HEAP</span> =&gt; </span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (numBytes &gt; maxMemory) &#123;</span><br><span class=\"line\"> <span class=\"comment\">// 申请的内存大于剩余内存总理则申请失败</span></span><br><span class=\"line\">​      <span class=\"string\">s\"memory limit (<span class=\"subst\">$maxMemory</span> bytes)\"</span>)</span><br><span class=\"line\">​    <span class=\"keyword\">return</span> <span class=\"literal\">false</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (numBytes &gt; storagePool.memoryFree) &#123; </span><br><span class=\"line\"><span class=\"comment\">// 如果Storage内存块中没有足够可用内存给blockId使用，则计算当前Storage内存区缺少多少内存，然后从Execution内存区中借用</span></span><br><span class=\"line\">​    <span class=\"keyword\">val</span> memoryBorrowedFromExecution = <span class=\"type\">Math</span>.min(executionPool.memoryFree, numBytes)</span><br><span class=\"line\"><span class=\"comment\">// Execution内存区减掉借用内存量</span></span><br><span class=\"line\">executionPool.decrementPoolSize(memoryBorrowedFromExecution) </span><br><span class=\"line\"><span class=\"comment\">// Storage内存区增加借用内存量    storagePool.incrementPoolSize(memoryBorrowedFromExecution) </span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"><span class=\"comment\">// 如果Storage内存区可以为blockId分配内存，直接成功分配；否则，如果从Execution内存区中借用的内存能够满足blockId，则分配成功，不能满足则分配失败。</span></span><br><span class=\"line\">  storagePool.acquireMemory(blockId, numBytes) </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-2-2-1-2-释放\"><a href=\"#3-2-2-1-2-释放\" class=\"headerlink\" title=\"3.2.2.1.2. 释放\"></a>3.2.2.1.2. <strong>释放</strong></h5><p>释放Storage内存比较简单，只需要更新Storage内存计量变量即可</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">releaseMemory</span></span>(size: <span class=\"type\">Long</span>): <span class=\"type\">Unit</span> = lock.synchronized &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (size &gt; _memoryUsed) &#123;</span><br><span class=\"line\">​    <span class=\"comment\">// 需要释放内存大于已使用内存，则直接清零</span></span><br><span class=\"line\">​    _memoryUsed = <span class=\"number\">0</span></span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">// 从已使用内存中减去释放内存大小</span></span><br><span class=\"line\">​    _memoryUsed -= size</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-2-2-2-Excution内存\"><a href=\"#3-2-2-2-Excution内存\" class=\"headerlink\" title=\"3.2.2.2. Excution内存\"></a>3.2.2.2. <strong>Excution内存</strong></h4><p>excution内存的获取和释放都是线程安全的，而且分配给每个task的内存大小是均等的，每当有task运行完毕后，都会触发内存的回收操作。</p>\n<h5 id=\"3-2-2-2-1-申请\"><a href=\"#3-2-2-2-1-申请\" class=\"headerlink\" title=\"3.2.2.2.1. 申请\"></a>3.2.2.2.1. <strong>申请</strong></h5><p>如果从storage申请内存大小比storage剩余内存大，则申请线程会阻塞，并对storage内存发起缩小操作。直到storage释放足够内存。</p>\n<p>Execution内存区内存分配的基本原则：<br>如果有N个活跃（Active）的Task在运行，ExecutionMemoryPool需要保证每个Task在将中间结果数据Spill到磁盘之前，至少能够申请到当前Execution内存区对应的Pool中1/2N大小的内存量，至多是1/N大小的内存。</p>\n<p>这里N是动态变化的，因为可能有新的Task被启动，也有可能Task运行完成释放资源，所以ExecutionMemoryPool会持续跟踪ExecutionMemoryPool内部Task集合memoryForTask的变化，并不断地重新计算分配给每个Task的这两个内存量的值：1/2N和1/N。</p>\n<h5 id=\"3-2-2-2-2-释放\"><a href=\"#3-2-2-2-2-释放\" class=\"headerlink\" title=\"3.2.2.2.2. 释放\"></a>3.2.2.2.2. <strong>释放</strong></h5><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 同步的释放内存</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">releaseMemory</span></span>(numBytes: <span class=\"type\">Long</span>, taskAttemptId: <span class=\"type\">Long</span>): <span class=\"type\">Unit</span> = lock.synchronized &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> curMem = memoryForTask.getOrElse(taskAttemptId, <span class=\"number\">0</span>L)</span><br><span class=\"line\"><span class=\"comment\">// 计算释放内存大小</span></span><br><span class=\"line\"> <span class=\"keyword\">var</span> memoryToFree = <span class=\"keyword\">if</span> (curMem &lt; numBytes) &#123; </span><br><span class=\"line\">​\t<span class=\"comment\">// 没有足够内存需要释放，则释放掉当前task所有使用内存</span></span><br><span class=\"line\">​    curMem</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">​    numBytes</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (memoryForTask.contains(taskAttemptId)) &#123; <span class=\"comment\">// Task执行完成，从内部维护的memoryForTask中移除</span></span><br><span class=\"line\">​    memoryForTask(taskAttemptId) -= memoryToFree</span><br><span class=\"line\">​    <span class=\"keyword\">if</span> (memoryForTask(taskAttemptId) &lt;= <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">​      memoryForTask.remove(taskAttemptId)</span><br><span class=\"line\">​    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> <span class=\"comment\">// 通知调用acquireMemory()方法申请内存的Task内存已经释放</span></span><br><span class=\"line\">  lock.notifyAll()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"4-BlockManager\"><a href=\"#4-BlockManager\" class=\"headerlink\" title=\"4. BlockManager\"></a>4. <strong>BlockManager</strong></h1><p>BlockManagerMaster管理BlockManager.<br>BlockManager在每个Dirver和Executor上都有，用来管理Block数据，包括数据的获取和保存等</p>\n<p>谈到Spark中的Block数据存储，我们很容易能够想到BlockManager，他负责管理在每个Dirver和Executor上的Block数据，可能是本地或者远程的。具体操作包括查询Block、将Block保存在指定的存储中，如内存、磁盘、堆外（Off-heap）。而BlockManager依赖的后端，对Block数据进行内存、磁盘存储访问，都是基于前面讲到的MemoryStore、DiskStore。<br>在Spark集群中，当提交一个Application执行时，该Application对应的Driver以及所有的Executor上，都存在一个BlockManager、BlockManagerMaster，而BlockManagerMaster是负责管理各个BlockManager之间通信，这个BlockManager管理集群</p>\n<h2 id=\"4-1-读数据\"><a href=\"#4-1-读数据\" class=\"headerlink\" title=\"4.1. 读数据\"></a>4.1. <strong>读数据</strong></h2><p>每个Executor上都有一个BlockManager实例，负责管理用户提交的该Application计算过程中产生的Block。</p>\n<p>很有可能当前Executor上存储在RDD对应Partition的经过处理后得到的Block数据，也有可能当前Executor上没有，但是其他Executor上已经处理过并缓存了Block数据，所以对应着本地获取、远程获取两种可能</p>\n<h2 id=\"4-2-BlockManager集群\"><a href=\"#4-2-BlockManager集群\" class=\"headerlink\" title=\"4.2. BlockManager集群\"></a>4.2. <strong>BlockManager集群</strong></h2><p>关于一个Application运行过程中Block的管理，主要是基于该Application所关联的一个Driver和多个Executor构建了一个Block管理集群：Driver上的(BlockManagerMaster, BlockManagerMasterEndpoint)是集群的Master角色，所有Executor上的(BlockManagerMaster, RpcEndpointRef)作为集群的Slave角色。当Executor上的Task运行时，会查询对应的RDD的某个Partition对应的Block数据是否处理过，这个过程中会触发多个BlockManager之间的通信交互</p>\n<h2 id=\"4-3-状态管理\"><a href=\"#4-3-状态管理\" class=\"headerlink\" title=\"4.3. 状态管理\"></a>4.3. <strong>状态管理</strong></h2><p>BlockManager在进行put操作后，通过blockInfoManager来控制当前put等操作是否完成以及是否成功。</p>\n<p>对于BlockManager中的存储的每个Block,不一定是对应的数据都PUT成功了,不一定可以立即提供对外的读取,因为PUT是一个过程,有成功还是有失败的状态. ,拿ShuffleBlock来说,在shuffleMapTask需要Put一个Block到BlockManager中,在Put完成之前,该Block将处于Pending状态,等待Put完成了不代表Block就可以被读取, 因为Block还可能Put”fail”了.</p>\n<p>因此BlockManager通过BlockInfo来维护每个Block状态,在BlockManager的代码中就是通过一个TimeStampedHashMap来维护BlockID和BlockInfo之间的map.</p>\n<p>private val blockInfo = new TimeStampedHashMap[BlockId, BlockInfo]<br>注： 2.2中此处是通过线程安全的hashMap和一个计数器实现的 </p>\n<h1 id=\"5-读写控制\"><a href=\"#5-读写控制\" class=\"headerlink\" title=\"5. 读写控制\"></a>5. <strong>读写控制</strong></h1><p>BlockInfoManager通过同步机制防止多个task处理同一个block数据块</p>\n<p>用户提交一个Spark Application程序，如果程序对应的DAG图相对复杂，其中很多Task计算的结果Block数据都有可能被重复使用，这种情况下如何去控制某个Executor上的Task线程去读写Block数据呢？其实，BlockInfoManager就是用来控制Block数据读写操作，并且跟踪Task读写了哪些Block数据的映射关系，这样如果两个Task都想去处理同一个RDD的同一个Partition数据，如果没有锁来控制，很可能两个Task都会计算并写同一个Block数据，从而造成混乱</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockInfoManager</span></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> infos = </span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">BlockId</span>, <span class=\"type\">BlockInfo</span>]</span><br><span class=\"line\">\t<span class=\"comment\">// 存放被锁定任务列表</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> writeLocksByTask =</span><br><span class=\"line\">    \t<span class=\"keyword\">new</span> mutable.<span class=\"type\">HashMap</span>[</span><br><span class=\"line\">\t\t\t<span class=\"type\">TaskAttemptId</span>, mutable.<span class=\"type\">Set</span>[<span class=\"type\">BlockId</span>]]</span><br><span class=\"line\">\t <span class=\"keyword\">val</span> readLocksByTask =</span><br><span class=\"line\">    \t<span class=\"keyword\">new</span> mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">TaskAttemptId</span>, \t\t<span class=\"type\">ConcurrentHashMultiset</span>[<span class=\"type\">BlockId</span>]]</span><br><span class=\"line\"> </span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">lockForReading</span></span>(）&#123;</span><br><span class=\"line\">\t\tinfos.get(blockId) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">Some</span>(info) =&gt;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 没有写任务</span></span><br><span class=\"line\">          \t\t<span class=\"keyword\">if</span> (info.writerTask == \t\t\t\t\t<span class=\"type\">BlockInfo</span>.<span class=\"type\">NO_WRITER</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"comment\">// 读task数量加一</span></span><br><span class=\"line\">            \t\tinfo.readerCount += <span class=\"number\">1</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"comment\">//\t放入读多锁定队列</span></span><br><span class=\"line\"> \t\t\t\t\treadLocksByTask(</span><br><span class=\"line\">\t\t\t\t\tcurrentTaskAttemptId).</span><br><span class=\"line\">\t\t\t\t\tadd(blockId)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">lockForWriting</span></span>(）&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">Some</span>(info) =&gt;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (info.writerTask == \t\t\t\t<span class=\"type\">BlockInfo</span>.<span class=\"type\">NO_WRITER</span> &amp;&amp; \t\t\t\tinfo.readerCount == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            info.writerTask = currentTaskAttemptId</span><br><span class=\"line\"> \t\t\twriteLocksByTask.addBinding(</span><br><span class=\"line\">\t\t\tcurrentTaskAttemptId, blockId)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkCore源码解析之Job","url":"https://sustcoder.github.io/2019/01/14/2018-12-01-sparkCore-sorceCodeAnalysis-Job/","content":"<p> <img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/Job.png\" alt=\"job\"></p>\n<h1 id=\"1-概念\"><a href=\"#1-概念\" class=\"headerlink\" title=\"1. 概念\"></a>1. <strong>概念</strong></h1><p>站在不同的角度看job</p>\n<ol>\n<li><p>transaction: Job是由一组RDD上转换和动作组成。</p>\n</li>\n<li><p>stage: Job是由ResultStage和多个ShuffleMapState组成</p>\n</li>\n<li><p>init:由action操作触发提交执行的一个函数<br> action操作会触发调用sc.runJob方法，</p>\n</li>\n</ol>\n<p>Job是一组rdd的转换以及最后动作的操作集合，它是Spark里面计算最大最虚的概念，甚至在spark的任务页面中都无法看到job这个单位。 但是不管怎么样，在spark用户的角度，job是我们计算目标的单位，每次在一个rdd上做一个动作操作时，都会触发一个job，完成计算并返回我们想要的数据。<br><strong>Job是由一组RDD上转换和动作组成</strong>，这组RDD之间的转换关系表现为一个有向无环图(DAG)，每个RDD的生成依赖于前面1个或多个RDD。<br>在Spark中，两个RDD之间的依赖关系是Spark的核心。站在RDD的角度，两者依赖表现为点对点依赖， 但是在Spark中，RDD存在分区（partition）的概念，两个RDD之间的转换会被细化为两个RDD分区之间的转换。<br>Stage的划分是对一个Job里面一系列RDD转换和动作进行划分。<br>首先job是因动作而产生，因此每个job肯定都有一个ResultStage，否则job就不会启动。<br>其次，如果Job内部RDD之间存在宽依赖，Spark会针对它产生一个中间Stage，即为ShuffleStage，严格来说应该是ShuffleMapStage，这个stage是针对父RDD而产生的， 相当于在父RDD上做一个父rdd.map().collect()的操作。ShuffleMapStage生成的map输入，对于子RDD，如果检测到所自己所“宽依赖”的stage完成计算，就可以启动一个shuffleFectch， 从而将父RDD输出的数据拉取过程，进行后续的计算。<br>  因此<strong>一个Job由一个ResultStage和多个ShuffleMapStage组成</strong>。</p>\n<h1 id=\"2-job处理流程\"><a href=\"#2-job处理流程\" class=\"headerlink\" title=\"2. job处理流程\"></a>2. <strong>job处理流程</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsA9AB.tmp.jpg\" alt=\"img\"><br><a href=\"https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-study.md\" target=\"_blank\" rel=\"noopener\">https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-study.md</a></p>\n<h2 id=\"2-1-job生成过程\"><a href=\"#2-1-job生成过程\" class=\"headerlink\" title=\"2.1. job生成过程\"></a>2.1. <strong>job生成过程</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsA9AC.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"2-1-1-job重载函数\"><a href=\"#2-1-1-job重载函数\" class=\"headerlink\" title=\"2.1.1. job重载函数\"></a>2.1.1. <strong>job重载函数</strong></h3><p>调用SparkContext里面的函数重载，将分区数量，需要计算的分区下标等参数设置好<br>以rdd.count为例：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">rdd.count</span><br><span class=\"line\"><span class=\"comment\">// 获取分区数</span></span><br><span class=\"line\">sc.runJob(<span class=\"keyword\">this</span>, <span class=\"type\">Utils</span>.getIteratorSize _).sum</span><br><span class=\"line\"><span class=\"comment\">// 设置需要计算的分区</span></span><br><span class=\"line\">runJob(rdd, func, <span class=\"number\">0</span> until rdd.partitions.length)</span><br><span class=\"line\"><span class=\"comment\">// 设置需要在每个partition上执行的函数</span></span><br><span class=\"line\">runJob(rdd, (ctx: <span class=\"type\">TaskContext</span>, it: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; cleanedFunc(it), partitions)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-1-2-设置回调函数\"><a href=\"#2-1-2-设置回调函数\" class=\"headerlink\" title=\"2.1.2. 设置回调函数\"></a>2.1.2. <strong>设置回调函数</strong></h3><p>定义一个接收计算结果的对象数组并将其返回<br>构造一个Array,并构造一个函数对象”(index, res) =&gt; results(index) = res”继续传递给runJob函数,然后等待runJob函数运行结束,将results返回; 对这里的解释相当在runJob添加一个回调函数,将runJob的运行结果保存到Array到, 回调函数,index表示mapindex, res为单个map的运行结果<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">     rdd: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">     func: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">     partitions: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>]): <span class=\"type\">Array</span>[<span class=\"type\">U</span>] = &#123;</span><br><span class=\"line\"><span class=\"comment\">// 定义返回的结果集</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> results = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">U</span>](partitions.size)</span><br><span class=\"line\"><span class=\"comment\">// 定义resulthandler</span></span><br><span class=\"line\">   runJob[<span class=\"type\">T</span>, <span class=\"type\">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)</span><br><span class=\"line\"><span class=\"comment\">// 返回计算结果</span></span><br><span class=\"line\">   results</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-1-3-获取需要执行的excutor\"><a href=\"#2-1-3-获取需要执行的excutor\" class=\"headerlink\" title=\"2.1.3. 获取需要执行的excutor\"></a>2.1.3. <strong>获取需要执行的excutor</strong></h3><p>将需要执行excutor的地址和回调函数等传给DAG调度器，由DAG调度器进行具体的submitJob操作。<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">      rdd: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">      func: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      partitions: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>],</span><br><span class=\"line\">      resultHandler: (<span class=\"type\">Int</span>, <span class=\"type\">U</span>) =&gt; <span class=\"type\">Unit</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 获取需要发送的excutor地址</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> callSite = getCallSite</span><br><span class=\"line\">\t<span class=\"comment\">// 闭包封装，防止序列化错误</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> cleanedFunc = clean(func)</span><br><span class=\"line\">\t<span class=\"comment\">// 提交给dag调度器,</span></span><br><span class=\"line\">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class=\"line\">    <span class=\"comment\">// docheckpoint</span></span><br><span class=\"line\">    rdd.doCheckpoint()</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<p>注意：<strong>dagScheduler.runJob是堵塞的操作,即直到Spark完成Job的运行之前,rdd.doCheckpoint()是不会执行的</strong><br>上异步的runJob回调用下面这个方法，里面设置了JobWaiter，用来等待job执行完毕。<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>&#123;</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"comment\">// job提交后会返回一个jobwaiter对象</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br><span class=\"line\"><span class=\"type\">ThreadUtils</span>.awaitReady(waiter.completionFuture, <span class=\"type\">Duration</span>.<span class=\"type\">Inf</span>)</span><br><span class=\"line\"> waiter.completionFuture.value.get <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">​      <span class=\"keyword\">case</span> scala.util.<span class=\"type\">Success</span>(_) =&gt;</span><br><span class=\"line\">​\t\t...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-1-4-将Job放入队列\"><a href=\"#2-1-4-将Job放入队列\" class=\"headerlink\" title=\"2.1.4. 将Job放入队列\"></a>2.1.4. <strong>将Job放入队列</strong></h3><p>给JOB分配一个ID，并将其放入队列，返回一个阻塞器，等待当前job执行完毕。将结果数据传送给handler function<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitJob</span></span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// 生成JOB的ID</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class=\"line\"><span class=\"comment\">// 生成阻塞器</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> waiter = <span class=\"keyword\">new</span> <span class=\"type\">JobWaiter</span>(<span class=\"keyword\">this</span>, jobId, partitions.size, resultHandler)</span><br><span class=\"line\"><span class=\"comment\">// post方法的实现：eventQueue.put(event),实际上是将此job提交到了一个LinkedBlockingDeque</span></span><br><span class=\"line\">eventProcessLoop.post(<span class=\"type\">JobSubmitted</span>(</span><br><span class=\"line\">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class=\"line\">      <span class=\"type\">SerializationUtils</span>.clone(properties)))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">waiter</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-2-job监听\"><a href=\"#2-2-job监听\" class=\"headerlink\" title=\"2.2. job监听\"></a>2.2. <strong>job监听</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsA9AD.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"2-2-1-监听触发\"><a href=\"#2-2-1-监听触发\" class=\"headerlink\" title=\"2.2.1. 监听触发\"></a>2.2.1. <strong>监听触发</strong></h3><p>在提交job时，我们将job放到了一个LinkedBlockingDeque队列，然后由EventLoop<br>负责接收处理请求，触发job的提交，产生一个finalStage.<br>EventLoop是在jobScheduler中启动的时候在JobGenerator中启动的<br>当从队列中拉去job时，开创建ResultStage:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">EventLoop</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">override</span> <span class=\"title\">def</span> <span class=\"title\">run</span>(<span class=\"params\"></span>)</span>: <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">​      <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">​        <span class=\"keyword\">while</span> (!stopped.get) &#123;</span><br><span class=\"line\">​\t\t <span class=\"comment\">// 拉去job</span></span><br><span class=\"line\">​          <span class=\"keyword\">val</span> event = eventQueue.take()</span><br><span class=\"line\">​          <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">​\t\t\t<span class=\"comment\">// 触发创建stage</span></span><br><span class=\"line\">​            onReceive(event)</span><br><span class=\"line\">​\t...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doOnReceive</span></span>&#123;</span><br><span class=\"line\">​\t<span class=\"keyword\">case</span> <span class=\"type\">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class=\"line\">​      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-2-2-初始化job和stage\"><a href=\"#2-2-2-初始化job和stage\" class=\"headerlink\" title=\"2.2.2. 初始化job和stage\"></a>2.2.2. <strong>初始化job和stage</strong></h3><p>创建job：根据JobId，finalStage,excutor地址,job状态监听的JobListener,task的属性properties等生成job,并把job放入Map中记录。<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// class DAGScheduler</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">handleJobSubmitted</span></span>() &#123;</span><br><span class=\"line\"><span class=\"comment\">// 以不同形式的hashMap存放job</span></span><br><span class=\"line\"> jobIdToStageIds = <span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">HashSet</span>[<span class=\"type\">Int</span>]]</span><br><span class=\"line\">stageIdToStage = <span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">Stage</span>]  </span><br><span class=\"line\">jobIdToActiveJob = <span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ActiveJob</span>] </span><br><span class=\"line\"><span class=\"comment\">// 初始化finalStage</span></span><br><span class=\"line\"><span class=\"keyword\">var</span> finalStage: <span class=\"type\">ResultStage</span> = \tcreateResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class=\"line\">   <span class=\"comment\">// 初始化job</span></span><br><span class=\"line\">​    <span class=\"keyword\">val</span> job = <span class=\"keyword\">new</span> <span class=\"type\">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class=\"line\">​    clearCacheLocs()</span><br><span class=\"line\">​    jobIdToActiveJob(jobId) = job</span><br><span class=\"line\">​    activeJobs += job</span><br><span class=\"line\">​    finalStage.setActiveJob(job)</span><br><span class=\"line\">​    <span class=\"keyword\">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class=\"line\">​    <span class=\"keyword\">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class=\"line\">​    **listenerBus.post(**</span><br><span class=\"line\">​      **<span class=\"type\">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))**</span><br><span class=\"line\">   <span class=\"comment\">// 提交finalStage，计算时会判断其之前是否存在shuffleStage，如果存在会优先计算shuffleStage，最后再计算finalStage</span></span><br><span class=\"line\">​    **submitStage(finalStage)**</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-2-3-提交stage\"><a href=\"#2-2-3-提交stage\" class=\"headerlink\" title=\"2.2.3. 提交stage\"></a>2.2.3. <strong>提交stage</strong></h3><p><strong>参见:</strong> <a href=\"#_MapOutputTrackerMaster\">MapOutputTrackerMaster</a><br>stage的状态分为三类：计算失败，计算完成和未计算完成，迭代的去计算完成父stage后，就可以到下一步，将stage转换到具体的task进行执行。<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DAGScheduler</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">private</span>[scheduler] <span class=\"title\">def</span> <span class=\"title\">handleJobSubmitted</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\"> <span class=\"keyword\">var</span> finalStage: <span class=\"type\">ResultStage</span> = \tcreateResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class=\"line\">​    ...</span><br><span class=\"line\">​    submitStage(finalStage)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"><span class=\"comment\">// 迭代的去判断父stage是否全部计算完成</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitStage</span></span>(stage: <span class=\"type\">Stage</span>) &#123;</span><br><span class=\"line\"><span class=\"keyword\">if</span>(jobId.isDefined)&#123;</span><br><span class=\"line\"><span class=\"keyword\">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class=\"line\"><span class=\"keyword\">if</span> (missing.isEmpty) &#123;</span><br><span class=\"line\">   <span class=\"comment\">// 父stage已经计算完成，可以开始当前计算</span></span><br><span class=\"line\">​\tsubmitMissingTasks(stage, jobId.get)</span><br><span class=\"line\">​    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">//  父stage的map操作未完成，继续进行迭代</span></span><br><span class=\"line\">​          <span class=\"keyword\">for</span> (parent &lt;- missing) &#123;</span><br><span class=\"line\">​            submitStage(parent)</span><br><span class=\"line\">​          &#125;</span><br><span class=\"line\">​          waitingStages += stage</span><br><span class=\"line\">​     &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 获取未计算完成的stage</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMissingParentStages</span></span>(stage: <span class=\"type\">Stage</span>): <span class=\"type\">List</span>[<span class=\"type\">Stage</span>] = &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span> (dep &lt;- rdd.dependencies) &#123;</span><br><span class=\"line\">   dep <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">​      <span class=\"keyword\">case</span> shufDep: <span class=\"type\">ShuffleDependency</span>=&gt;</span><br><span class=\"line\"><span class=\"comment\">// 判断当前stage是否计算完成</span></span><br><span class=\"line\">​\t <span class=\"keyword\">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)</span><br><span class=\"line\">​         <span class=\"keyword\">if</span> (!mapStage.isAvailable) &#123;</span><br><span class=\"line\">​                  missing += mapStage</span><br><span class=\"line\">​          &#125;           </span><br><span class=\"line\">​       <span class=\"keyword\">case</span> narrowDep: <span class=\"type\">NarrowDependency</span>[_] =&gt;</span><br><span class=\"line\">​             waitingForVisit.push(narrowDep.rdd)</span><br><span class=\"line\">​      &#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-3-stage转task\"><a href=\"#2-3-stage转task\" class=\"headerlink\" title=\"2.3. stage转task\"></a>2.3. <strong>stage转task</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsA9BE.tmp.jpg\" alt=\"img\"><br>首先利于上面说到的Stage知识获取所需要进行计算的task的分片;因为该Stage有些分片可能已经计算完成了;然后将Task运行依赖的RDD,Func,shuffleDep 进行序列化,通过broadcast发布出去; 然后创建Task对象,提交给taskScheduler调度器进行运行</p>\n<h3 id=\"2-3-1-过滤需要执行的分片\"><a href=\"#2-3-1-过滤需要执行的分片\" class=\"headerlink\" title=\"2.3.1. 过滤需要执行的分片\"></a>2.3.1. <strong>过滤需要执行的分片</strong></h3><p><strong>参见:</strong> <a href=\"#___task__\">获取task分片</a><br>对Stage进行遍历所有需要运行的Task分片;<br>原因：存在部分task失败之类的情况,或者task运行结果所在的BlockManager被删除了,就需要针对特定分片进行重新计算;即所谓的恢复和重算机制;<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DAGScheduler</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitMissingTasks</span></span>(stage, jobId)&#123;</span><br><span class=\"line\">​\t<span class=\"keyword\">val</span> partitionsToCompute: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>] = stage.findMissingPartitions()</span><br><span class=\"line\">​\t<span class=\"keyword\">val</span> properties = jobIdToActiveJob(jobId).properties</span><br><span class=\"line\">​\trunningStages += stage</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-3-2-序列化和广播\"><a href=\"#2-3-2-序列化和广播\" class=\"headerlink\" title=\"2.3.2. 序列化和广播\"></a>2.3.2. <strong>序列化和广播</strong></h3><p>对Stage的运行依赖进行序列化并broadcast给excutors(如果不序列化在数据传输过程中可能出错)<br>对ShuffleStage和FinalStage所序列化的内容有所不同：<strong>对于ShuffleStage序列化的是RDD和shuffleDep;而对FinalStage序列化的是RDD和Func</strong><br>对于FinalStage我们知道,每个Task运行过程中,需要知道RDD和运行的函数,比如我们这里讨论的Count实现的Func;而对于ShuffleStage,ShuffleDependency记录了父RDD，排序方式，聚合器等，reduce端需要获取这些参数进行初始化和计算。<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DAGScheduler</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitMissingTasks</span></span>(stage, jobId)&#123;</span><br><span class=\"line\">​\t...</span><br><span class=\"line\">​      <span class=\"comment\">// consistent view of both variables.</span></span><br><span class=\"line\"><span class=\"type\">RDDCheckpointData</span>.synchronized &#123;</span><br><span class=\"line\">​        taskBinaryBytes = stage <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">​          <span class=\"keyword\">case</span> stage: <span class=\"type\">ShuffleMapStage</span> =&gt;</span><br><span class=\"line\">​            <span class=\"type\">JavaUtils</span>.bufferToArray(</span><br><span class=\"line\">​              closureSerializer.serialize((stage.rdd, stage.shuffleDep): <span class=\"type\">AnyRef</span>))</span><br><span class=\"line\">​          <span class=\"keyword\">case</span> stage: <span class=\"type\">ResultStage</span> =&gt;           <span class=\"type\">JavaUtils</span>.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): <span class=\"type\">AnyRef</span>))</span><br><span class=\"line\">​        &#125;</span><br><span class=\"line\">​        partitions = stage.rdd.partitions</span><br><span class=\"line\">​      &#125;</span><br><span class=\"line\">​      taskBinary = sc.broadcast(taskBinaryBytes)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-3-3-构造task对象\"><a href=\"#2-3-3-构造task对象\" class=\"headerlink\" title=\"2.3.3. 构造task对象\"></a>2.3.3. <strong>构造task对象</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsA9BF.tmp.jpg\" alt=\"img\"><br>针对每个需要计算的分片构造一个Task对象，<br>对于ResultTask就是在分片上调用我们的Func,而ShuffleMapTask按照ShuffleDep进行 MapOut</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DAGScheduler</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitMissingTasks</span></span>(stage, jobId)&#123;</span><br><span class=\"line\">​\t...</span><br><span class=\"line\"> <span class=\"keyword\">val</span> tasks: <span class=\"type\">Seq</span>[<span class=\"type\">Task</span>[_]] = <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">​      <span class=\"keyword\">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class=\"line\">​      stage <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">​\t\t<span class=\"comment\">// 一个stage会产生多个task任务</span></span><br><span class=\"line\">​        <span class=\"keyword\">case</span> stage: <span class=\"type\">ShuffleMapStage</span> =&gt;</span><br><span class=\"line\">​          partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">​            <span class=\"keyword\">new</span> <span class=\"type\">ShuffleMapTask</span>()</span><br><span class=\"line\">​          &#125;</span><br><span class=\"line\">​        <span class=\"keyword\">case</span> stage: <span class=\"type\">ResultStage</span> =&gt;</span><br><span class=\"line\">​          partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">​            <span class=\"keyword\">new</span> <span class=\"type\">ResultTask</span>()</span><br><span class=\"line\">​          &#125;</span><br><span class=\"line\">​      &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-3-3-1-ShuffleMapTask\"><a href=\"#2-3-3-1-ShuffleMapTask\" class=\"headerlink\" title=\"2.3.3.1. ShuffleMapTask\"></a>2.3.3.1. <strong>ShuffleMapTask</strong></h4><h4 id=\"2-3-3-2-ResultTask\"><a href=\"#2-3-3-2-ResultTask\" class=\"headerlink\" title=\"2.3.3.2. ResultTask\"></a>2.3.3.2. <strong>ResultTask</strong></h4><h3 id=\"2-3-4-taskScheduler调度task\"><a href=\"#2-3-4-taskScheduler调度task\" class=\"headerlink\" title=\"2.3.4. taskScheduler调度task\"></a>2.3.4. <strong>taskScheduler调度task</strong></h3><p>调用taskScheduler将task提交给Spark进行调度<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DAGScheduler</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitMissingTasks</span></span>(stage, jobId)&#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"><span class=\"keyword\">if</span> (tasks.size &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">   <span class=\"comment\">// 将taskSet发送给 taskScheduler</span></span><br><span class=\"line\">\ttaskScheduler.submitTasks(<span class=\"keyword\">new</span> <span class=\"type\">TaskSet</span>(</span><br><span class=\"line\">        tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))</span><br><span class=\"line\">      stage.latestInfo.submissionTime = <span class=\"type\">Some</span>(clock.getTimeMillis())</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     markStageAsFinished(stage, <span class=\"type\">None</span>)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-4-获取运行结果\"><a href=\"#2-4-获取运行结果\" class=\"headerlink\" title=\"2.4. 获取运行结果\"></a>2.4. <strong>获取运行结果</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsA9C0.tmp.jpg\" alt=\"img\"><br>DAGScheduler接收到DAGSchedulerEvent后判断其类型是TaskCompletion，不同的stage的实现方式不一样，shuffle的实现更复杂一点</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doOnReceive</span></span>(event: <span class=\"type\">DAGSchedulerEvent</span>): <span class=\"type\">Unit</span> = event <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> completion: <span class=\"type\">CompletionEvent</span> =&gt;</span><br><span class=\"line\">dagScheduler.handleTaskCompletion(completion)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">handleTaskCompletion</span></span>(event: <span class=\"type\">CompletionEvent</span>) &#123;</span><br><span class=\"line\">event.reason <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Success</span> =&gt;</span><br><span class=\"line\">​      task <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">​      <span class=\"keyword\">case</span> rt: <span class=\"type\">ResultTask</span>[_, _] =&gt;</span><br><span class=\"line\">​\t\t\t<span class=\"comment\">// 调用jobWaiter的taskSucced通知结果</span></span><br><span class=\"line\">​\t\t\tjob.listener.taskSucceeded</span><br><span class=\"line\">​\t <span class=\"keyword\">case</span> smt: <span class=\"type\">ShuffleMapTask</span> =&gt;</span><br><span class=\"line\">​\t\t\t<span class=\"comment\">// 调用outputTracker</span></span><br><span class=\"line\">​\t\tmapOutputTracker.registerMapOutput</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-4-1-ResultStage\"><a href=\"#2-4-1-ResultStage\" class=\"headerlink\" title=\"2.4.1. ResultStage\"></a>2.4.1. <strong>ResultStage</strong></h3><p>当计算完毕后，JobWaiter同步调用resultHandler处理task返回的结果。<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">handleTaskCompletion</span></span>(event: <span class=\"type\">CompletionEvent</span>) &#123;</span><br><span class=\"line\">event.reason <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Success</span> =&gt;</span><br><span class=\"line\">      task <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> rt: <span class=\"type\">ResultTask</span>[_, _] =&gt;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 调用jobWaiter的taskSucced通知结果</span></span><br><span class=\"line\">\t\t\tjob.listener.taskSucceeded(</span><br><span class=\"line\">\t\t\trt.outputId, event.result)</span><br><span class=\"line\">\t <span class=\"keyword\">case</span> smt: <span class=\"type\">ShuffleMapTask</span> =&gt;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// jobWaiter是JobListner的子类</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobWaiter</span> <span class=\"keyword\">extends</span> <span class=\"title\">JobListener</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">taskSucceeded</span></span>(index: <span class=\"type\">Int</span>, result: <span class=\"type\">Any</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    synchronized &#123;</span><br><span class=\"line\">      resultHandler(index, result.asInstanceOf[<span class=\"type\">T</span>])</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (finishedTasks.incrementAndGet() == totalTasks) &#123;</span><br><span class=\"line\">      jobPromise.success(())</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-4-2-ShuffleMapStage\"><a href=\"#2-4-2-ShuffleMapStage\" class=\"headerlink\" title=\"2.4.2. ShuffleMapStage\"></a>2.4.2. <strong>ShuffleMapStage</strong></h3><p><strong>参见:</strong> <a href=\"#_MapStatus______\">MapStatus的注册和获取</a><br>将运行结果(mapStatus)传送给outputTrancker<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">handleTaskCompletion</span></span>(event: <span class=\"type\">CompletionEvent</span>) &#123;</span><br><span class=\"line\">event.reason <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Success</span> =&gt;</span><br><span class=\"line\">​      task <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">​      <span class=\"keyword\">case</span> rt: <span class=\"type\">ResultTask</span>[_, _] =&gt;</span><br><span class=\"line\">​\t <span class=\"keyword\">case</span> smt: <span class=\"type\">ShuffleMapTask</span> =&gt;</span><br><span class=\"line\">​\t\t\t<span class=\"comment\">// </span></span><br><span class=\"line\">​\t\tmapOutputTracker.registerMapOutput(</span><br><span class=\"line\">​                shuffleStage.shuffleDep.shuffleId, \t\t\t\tsmt.partitionId, status)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-5-doCheckPoint\"><a href=\"#2-5-doCheckPoint\" class=\"headerlink\" title=\"2.5. doCheckPoint\"></a>2.5. <strong>doCheckPoint</strong></h2><p>job执行完毕后执行</p>\n<h1 id=\"3-stage\"><a href=\"#3-stage\" class=\"headerlink\" title=\"3. stage\"></a>3. <strong>stage</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/sparkcore/wpsA9C1.tmp.jpg\" alt=\"img\"> </p>\n<ol>\n<li>一组含有相同计算函数的任务集合，这些任务组合成了一个完整的job</li>\n<li>stage分为两种：FinalStage和shuffleStage</li>\n<li>stage中包含了jobId,对于FIFO规则，jobId越小的优先级越高</li>\n<li>为了保证容错性，一个stage可以被重复执行，所以在web UI上有可能看见多个stage的信息，取最新更新时间的即可</li>\n<li>组成：<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Stage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val id: <span class=\"type\">Int</span>, // stageId</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val rdd: <span class=\"type\">RDD</span>[_],// <span class=\"type\">RDD</span> that this stage runs on</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val numTasks: <span class=\"type\">Int</span>,// task数量</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val parents: <span class=\"type\">List</span>[<span class=\"type\">Stage</span>],// 父stage</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val firstJobId: <span class=\"type\">Int</span>,//当前stage上<span class=\"type\">JobId</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val callSite: <span class=\"type\">CallSite</span>// 生成<span class=\"type\">RDD</span>存放位置</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)  <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"3-1-ShuffleMapStage\"><a href=\"#3-1-ShuffleMapStage\" class=\"headerlink\" title=\"3.1. ShuffleMapStage\"></a>3.1. <strong>ShuffleMapStage</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShuffleMapStage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val shuffleDep: <span class=\"type\">ShuffleDependency</span>[_, _, _],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    mapOutputTrackerMaster: <span class=\"type\">MapOutputTrackerMaster</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Stage</span>(<span class=\"params\">id, rdd, numTasks, parents, firstJobId, callSite</span>)</span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// 判断当前stage是否可用</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isAvailable</span></span>: <span class=\"type\">Boolean</span> = numAvailableOutputs == numPartitions</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-1-ShuffleMapTask\"><a href=\"#3-1-1-ShuffleMapTask\" class=\"headerlink\" title=\"3.1.1. ShuffleMapTask\"></a>3.1.1. <strong>ShuffleMapTask</strong></h3><p>每个运行在Executor上的Task, 通过SparkEnv获取shuffleManager对象, 然后调用getWriter来当前MapID=partitionId的一组Writer. 然后将rdd的迭代器传递给writer.write函数, 由每个Writer的实现去实现具体的write操作;<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShuffleMapTask</span> <span class=\"keyword\">extends</span> <span class=\"title\">Task</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">def runTask(context: <span class=\"type\">TaskContext</span></span>)</span>: <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">​\t<span class=\"comment\">// 反序列化接收到的数据</span></span><br><span class=\"line\">​    <span class=\"keyword\">val</span> (rdd, dep) = closureSerializer.deserialize(</span><br><span class=\"line\">​      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value))</span><br><span class=\"line\"><span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\"><span class=\"comment\">// 调用ShuffleManager的getWriter方法获取一组writer</span></span><br><span class=\"line\"> writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\"><span class=\"comment\">// 遍历RDD进行write</span></span><br><span class=\"line\"> writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上面代码中，在调用rdd的iterator()方法时，会根据RDD实现类的compute方法指定的处理逻辑对数据进行处理，当然，如果该Partition对应的数据已经处理过并存储在MemoryStore或DiskStore，直接通过BlockManager获取到对应的Block数据，而无需每次需要时重新计算。然后，write()方法会将已经处理过的Partition数据输出到磁盘文件。<br>在Spark Shuffle过程中，每个ShuffleMapTask会通过配置的ShuffleManager实现类对应的ShuffleManager对象（实际上是在SparkEnv中创建），根据已经注册的ShuffleHandle，获取到对应的ShuffleWriter对象，然后通过ShuffleWriter对象将Partition数据写入内存或文件。</p>\n<h3 id=\"3-1-2-获取task分片\"><a href=\"#3-1-2-获取task分片\" class=\"headerlink\" title=\"3.1.2. 获取task分片\"></a>3.1.2. <strong>获取task分片</strong></h3><p><strong>参见:</strong> <a href=\"#__________\">过滤需要执行的分片</a><br>返回需要计算的partition信息<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShuffleMapStage</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">findMissingPartitions</span></span>(): <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>] = &#123;</span><br><span class=\"line\">​    mapOutputTrackerMaster</span><br><span class=\"line\">​      .findMissingPartitions(shuffleDep.shuffleId)</span><br><span class=\"line\">​      .getOrElse(<span class=\"number\">0</span> until numPartitions)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3-2-ResultStage\"><a href=\"#3-2-ResultStage\" class=\"headerlink\" title=\"3.2. ResultStage\"></a>3.2. <strong>ResultStage</strong></h2><h3 id=\"3-2-1-ResultTask\"><a href=\"#3-2-1-ResultTask\" class=\"headerlink\" title=\"3.2.1. ResultTask\"></a>3.2.1. <strong>ResultTask</strong></h3><p><strong>参见:</strong> <a href=\"#_reduce___\">reduce端获取</a><br>ResultTask不需要进行写操作。直接将计算结果返回。<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ResultTask</span> <span class=\"keyword\">extends</span> <span class=\"title\">Task</span> </span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 对RDD和函数进行反序列化</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> (rdd, func) = ser.deserialize(</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value)</span><br><span class=\"line\">\t<span class=\"comment\">// 调用函数进行计算</span></span><br><span class=\"line\">\tfunc(context, rdd.iterator(partition, context))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// RDD的iterator函数，</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RDD</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;</span><br><span class=\"line\">      getOrCompute(split, context)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-2-2-获取task分片\"><a href=\"#3-2-2-获取task分片\" class=\"headerlink\" title=\"3.2.2. 获取task分片\"></a>3.2.2. <strong>获取task分片</strong></h3><p>返回需要计算的partition信息,不需要经过tracker,在提交Job的时候会将其保存在ResultStage<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DAGScheduler</span></span>&#123;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">handleJobSubmitted</span></span>()&#123;</span><br><span class=\"line\"><span class=\"comment\">// 定义resultStage</span></span><br><span class=\"line\">finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class=\"line\"><span class=\"comment\">// 将job传递给resultStage</span></span><br><span class=\"line\">finalStage.setActiveJob(job)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ResultStage</span></span>&#123;</span><br><span class=\"line\"><span class=\"comment\">// 过滤掉已经完成的</span></span><br><span class=\"line\">findMissingPartitions(): <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> job = activeJob.get</span><br><span class=\"line\">    (<span class=\"number\">0</span> until job.numPartitions).filter(id =&gt; !job.finished(id))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"布隆过滤器之Counting Bloom Filter","url":"https://sustcoder.github.io/2019/01/11/2019-01-11-bloom-filter/","content":"<p>Bloom Filter是一种空间效率很高的概率型数据结构，由位数组和一组哈希函数组成。特点是高效地插入和查询，可以用来告诉你 <strong>“某样东西一定不存在或者可能存在”</strong>。总体规则是：位数组中存放的是：集合中每个元素的哈希转二进制后对应的值，对于新加入的元素根据其哈希值去判断是否已经存在于集合中。</p>\n<h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><p>Bloom Filter使用<a href=\"https://my.oschina.net/freelili/blog/2885263\" target=\"_blank\" rel=\"noopener\">位数组</a>来实现过滤，初始状态下位数组每一位都为0，如下图所示：</p>\n<p><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/algorithm/20170410095728_20163.png\" alt=\"img\"></p>\n<p>假如此时有一个集合S = {x1, x2, … xn}，Bloom Filter使用k个独立的hash函数，分别将集合中的每一个元素映射到｛1,…,m｝的范围。对于任何一个元素，被映射到的数字作为对应的位数组的索引，该位会被置为1。比如元素x1被hash函数映射到数字8，那么位数组的第8位就会被置为1。下图中集合S只有两个元素x和y，分别被3个hash函数进行映射，映射到的位置分别为（0，3，6）和（4，7，10），对应的位会被置为1:</p>\n<p><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/algorithm/20170410095821_31707.png\" alt=\"529239.png\"></p>\n<p>现在假如要判断另一个元素是否是在此集合中，只需要<strong>被这3个hash函数进行映射，查看对应的位置是否有0存在，如果有的话，表示此元素肯定不存在于这个集合，否则有可能存在</strong>。下图所示就表示z肯定不在集合｛x，y｝中：</p>\n<p><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/algorithm/20170410095844_44900.png\" alt=\"img\"></p>\n<p><strong>对有可能存在的解释</strong></p>\n<p>假设X进行三次hash后对应的bit位是(0,3,6),Y被三次hash后对应的bit位是(0,3,7)，则Y在bit的0和3的值会覆盖掉X的值，假设还有一个新元素Z的hash结果是(1,3,6)，则意味着X的hash结果被全部覆盖，也就是说即使没有X，(0,3,6)位置的值也是1，所以只能是有可能存在。</p>\n<h3 id=\"CBF\"><a href=\"#CBF\" class=\"headerlink\" title=\"CBF\"></a>CBF</h3><p>由于判断某个元素在bloom filter是否存在时个概率问题，所以就不能随便删除一个元素了，如上如果我们删除了X元素的哈希结果，那么Y元素和Z元素也会被误判为不存在集合中。</p>\n<p>删除元素问题可以通过布隆过滤器的变体<code>CBF(Counting bloomfilter)</code>解决。CBF将基本Bloom Filter每一个Bit改为一个计数器，这样就可以实现删除字符串的功能了。</p>\n<p>它将标准Bloom Filter位数组的每一位扩展为一个小的计数器（Counter），在插入元素时给对应的k（k为哈希函数个数）个Counter的值分别加1，删除元素时给对应的k个Counter的值分别减1。Counting Bloom Filter通过多占用几倍的存储空间的代价，给Bloom Filter增加了删除操作。</p>\n<p>其他bloomfilter的升级版：</p>\n<ul>\n<li><p><code>SBF（Spectral Bloom Filter）</code>:在判断元素是否存在的基础上还可以查询集合元素的出现频率。</p>\n</li>\n<li><p><code>dlCBF（d-Left Counting Bloom Filter）</code>:利用 d-left hashing 的方法存储 fingerprint，解决哈希表的负载平衡问题；</p>\n</li>\n<li><code>ACBF（Accurate Counting Bloom Filter）</code>:通过 offset indexing 的方式将 Counter 数组划分成多个层级</li>\n</ul>\n<h2 id=\"应用场景\"><a href=\"#应用场景\" class=\"headerlink\" title=\"应用场景\"></a>应用场景</h2><ol>\n<li>网页爬虫对URL的去重，避免爬取相同的URL地址；</li>\n<li>反垃圾邮件，从数十亿个垃圾邮件列表中判断某邮箱是否垃圾邮箱（同理，垃圾短信）；</li>\n<li>缓存击穿，将已存在的缓存放到布隆中，当黑客访问不存在的缓存时迅速返回避免缓存及DB挂掉。</li>\n<li>spark的jion操作中<code>Runtime Filter</code>的过滤原理实现。</li>\n</ol>\n<h2 id=\"数学结论\"><a href=\"#数学结论\" class=\"headerlink\" title=\"数学结论\"></a>数学结论</h2><p><a href=\"http://blog.csdn.net/jiaomeng/article/details/1495500\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/jiaomeng/article/details/1495500</a>该文中从数学的角度阐述了布隆过滤器的原理，以及一系列的数学结论。</p>\n<p>首先，与布隆过滤器准确率有关的参数有：</p>\n<ul>\n<li>哈希函数的个数k；</li>\n<li>布隆过滤器位数组的容量m;</li>\n<li>布隆过滤器插入的数据数量n;</li>\n</ul>\n<p>主要的数学结论有：</p>\n<ol>\n<li>为了获得<strong>最优的准确率</strong>，当k = ln2 * (m/n)时，布隆过滤器获得最优的准确性；</li>\n<li>在哈希函数的个数取到最优时，要让错误率不超过є，m至少需要取到最小值的1.44倍；</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://zhuanlan.zhihu.com/p/43263751\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/43263751</a></p>\n<p><a href=\"https://blog.csdn.net/tianyaleixiaowu/article/details/74721877\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/tianyaleixiaowu/article/details/74721877</a></p>\n","categories":["algorithm"],"tags":["algorithm","bloom"]},{"title":"Spark Streaming基于kafka获取数据","url":"https://sustcoder.github.io/2019/01/08/2019-01-08-Spark-Streaming-kafka-direct/","content":"<h2 id=\"Receiver方式\"><a href=\"#Receiver方式\" class=\"headerlink\" title=\"Receiver方式\"></a>Receiver方式</h2><h3 id=\"处理流程\"><a href=\"#处理流程\" class=\"headerlink\" title=\"处理流程\"></a>处理流程</h3><p>实际上做kafka receiver的时候，通过receiver来获取数据，这个时候，kafka receiver是使用的kafka高层次的comsumer api来实现的。receiver会从kafka中获取数据，然后把它存储到我们具体的Executor内存中。然后Spark streaming也就是driver中，会根据这获取到的数据，启动job去处理。</p>\n<p><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/kafkareceiver.png\" alt=\"kafkareceiver\"></p>\n<h3 id=\"receiver缺点\"><a href=\"#receiver缺点\" class=\"headerlink\" title=\"receiver缺点\"></a>receiver缺点</h3><ol>\n<li>已经拉取的数据消费失败后，会导致数据丢失。此问题虽然可以通过WAL方式或者Memory_and_Disc2解决，但是存在耗时等问题</li>\n<li>使用了kafka consumer的高阶API，KafkaInputDStream的实现和我们常用的consumer实现类似，需要zk额外的记录偏移量</li>\n</ol>\n<h2 id=\"Direct方式\"><a href=\"#Direct方式\" class=\"headerlink\" title=\"Direct方式\"></a>Direct方式</h2><h3 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h3><p>在使用kafka接收消息时，都是调用了KafkaUtils里面createStream的不同实现。</p>\n<p>receiver方式的实现方式如下。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 创建一个inputStream，从kafkaBrokers上拉去消息，需要传入zk集群信息，默认会复制到另一个excutor</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">createStream</span></span>(</span><br><span class=\"line\">    ssc: <span class=\"type\">StreamingContext</span>,<span class=\"comment\">// spark上下文</span></span><br><span class=\"line\">    zkQuorum: <span class=\"type\">String</span>,<span class=\"comment\">// zk集群信息（hostname:port,hostname:port...）</span></span><br><span class=\"line\">    groupId: <span class=\"type\">String</span>,<span class=\"comment\">// 当前consumer所属分组</span></span><br><span class=\"line\">    topics: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">Int</span>],<span class=\"comment\">// Map[topic_name,numPartitions],topic消费对应的分区</span></span><br><span class=\"line\">    storageLevel: <span class=\"type\">StorageLevel</span> = <span class=\"type\">StorageLevel</span>.<span class=\"type\">MEMORY_AND_DISK_SER_2</span></span><br><span class=\"line\">  ): <span class=\"type\">ReceiverInputDStream</span>[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> kafkaParams = <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>](</span><br><span class=\"line\">    <span class=\"string\">\"zookeeper.connect\"</span> -&gt; zkQuorum, <span class=\"string\">\"group.id\"</span> -&gt; groupId,</span><br><span class=\"line\">    <span class=\"string\">\"zookeeper.connection.timeout.ms\"</span> -&gt; <span class=\"string\">\"10000\"</span>)</span><br><span class=\"line\">    <span class=\"comment\">// 写日志</span></span><br><span class=\"line\"> <span class=\"keyword\">val</span> walEnabled = <span class=\"type\">WriteAheadLogUtils</span>.enableReceiverLog(ssc.conf)</span><br><span class=\"line\">    <span class=\"comment\">// 组装成KafkaInputDStream</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">KafkaInputDStream</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">U</span>, <span class=\"type\">T</span>](</span><br><span class=\"line\">          ssc, kafkaParams, topics, walEnabled, \tstorageLevel)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>direct方式实现消费</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">   * 摒弃了高阶的kafkaConsumerAPI直接从kafkaBrokers获取信息，可以保证每条消息只被消费一次</span></span><br><span class=\"line\"><span class=\"comment\">   * 特点：</span></span><br><span class=\"line\"><span class=\"comment\">   * - No receivers：没有receivers,直接从kafka拉取数据</span></span><br><span class=\"line\"><span class=\"comment\">   * - Offsets：不用zookeeper存储offsets,偏移量是通过stream自己跟踪记录的，可以通过HasOffsetRanges获取offset</span></span><br><span class=\"line\"><span class=\"comment\">   * - Failure Recovery故障恢复：需要开启sparkContext的checkpoint功能</span></span><br><span class=\"line\"><span class=\"comment\">   * - End-to-end semantics最终一致性：保证消息被消费且只消费一次</span></span><br><span class=\"line\"><span class=\"comment\">   * @return DStream of (Kafka message key, Kafka message value)</span></span><br><span class=\"line\"><span class=\"comment\">   */</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">createDirectStream</span></span>[</span><br><span class=\"line\">    <span class=\"type\">K</span>: <span class=\"type\">ClassTag</span>,</span><br><span class=\"line\">    <span class=\"type\">V</span>: <span class=\"type\">ClassTag</span>,</span><br><span class=\"line\">    <span class=\"type\">KD</span> &lt;: <span class=\"type\">Decoder</span>[<span class=\"type\">K</span>]: <span class=\"type\">ClassTag</span>,</span><br><span class=\"line\">    <span class=\"type\">VD</span> &lt;: <span class=\"type\">Decoder</span>[<span class=\"type\">V</span>]: <span class=\"type\">ClassTag</span>] (</span><br><span class=\"line\">      ssc: <span class=\"type\">StreamingContext</span>,</span><br><span class=\"line\">      <span class=\"comment\">// brokers列表，Map(\"metadata.broker.list\" -&gt; brokers)</span></span><br><span class=\"line\">      kafkaParams: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>],</span><br><span class=\"line\">      topics: <span class=\"type\">Set</span>[<span class=\"type\">String</span>]</span><br><span class=\"line\">  ): <span class=\"type\">InputDStream</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> messageHandler = (mmd: <span class=\"type\">MessageAndMetadata</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]) =&gt; (mmd.key, mmd.message)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> kc = <span class=\"keyword\">new</span> <span class=\"type\">KafkaCluster</span>(kafkaParams)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> fromOffsets = getFromOffsets(kc, kafkaParams, topics)</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">DirectKafkaInputDStream</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">KD</span>, <span class=\"type\">VD</span>, (<span class=\"type\">K</span>, <span class=\"type\">V</span>)](</span><br><span class=\"line\">      ssc, kafkaParams, fromOffsets, messageHandler)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a>特点</h3><ol>\n<li>Direct的方式是会直接操作kafka底层的元数据信息，这样如果计算失败了，可以把数据重新读一下，重新处理。即数据一定会被处理。拉数据，是RDD在执行的时候直接去拉数据。</li>\n<li>由于直接操作的是kafka，kafka就相当于你底层的文件系统。这个时候能保证严格的事务一致性，即一定会被处理，而且只会被处理一次。而Receiver的方式则不能保证，因为Receiver和ZK中的数据可能不同步，Spark Streaming可能会重复消费数据。而Direct api直接是操作kafka的，spark streaming自己负责追踪消费这个数据的偏移量或者offset，并且自己保存到checkpoint，所以它的数据一定是同步的，一定不会被重复。</li>\n<li>底层是直接读数据，没有所谓的Receiver，直接是周期性(Batch Intervel)的查询kafka，处理数据的时候，我们会使用基于kafka原生的Consumer api来获取kafka中特定范围(offset范围)中的数据，这个时候，Direct Api访问kafka带来的一个显而易见的性能上的好处就是，如果你要读取多个partition，Spark也会创建RDD的partition，这个时候RDD的partition和kafka的partition是一致的。所以增加kafka中的topic的partition数量可以提高并行度。</li>\n<li>偏移量：默认从最新偏移量(<code>largest</code>)开始消费。如果设置了<code>auto.offset.reset</code>参数值为<code>smallest</code>将从最小偏移处开始消费。</li>\n</ol>\n<p>checkpoint恢复后，如果数据累积太多处理不过来，怎么办?</p>\n<p>1）限速，通过<code>spark.streaming.kafka.maxRatePerPartition</code>参数配置</p>\n<p>2）增强机器的处理能力</p>\n<p>3）放到数据缓冲池中。</p>\n<h2 id=\"模式比对\"><a href=\"#模式比对\" class=\"headerlink\" title=\"模式比对\"></a>模式比对</h2><ol>\n<li>.简化并行性：无需创建多个输入Kafka流并且结合它们。 使用directStream，Spark Streaming将创建与要消费的Kafkatopic中partition分区一样多的RDD分区，这将从Kafka并行读取数据。 因此，在Kafka和RDD分区之间存在一对一映射，这更容易理解和调整。</li>\n<li>效率：在第一种方法中实现零数据丢失需要将数据存储在预写日志中，该日志进一步复制数据。 这实际上是低效的，因为数据有效地被复制两次 - 一次是Kafka，另一次是写入提前日志。 第二种方法消除了问题，因为没有接收器(zookeeper)，因此不需要预写日志。 将元数据信息直接保存在kafka中，可以从Kafka恢复消息。</li>\n<li>Exactly-once语义：第一种方法使用Kafka的高级API在Zookeeper中存储消耗的偏移量。这是传统上消费Kafka数据的方式。虽然这种方法（与预写日志结合）可以确保零数据丢失（即至少一次语义），但是一些记录在一些故障下可能被消耗两次。这是因为Spark Streaming可靠接收的数据与Zookeeper跟踪的偏移之间存在不一致。因此，在第二种方法中，我们使用简单的Kafka API,不使用Zookeeper的。偏移由Spark Streaming在其检查点内跟踪。这消除了Spark Streaming和Zookeeper / Kafka之间的不一致，所以每个记录被Spark Streaming有效地精确接收一次，尽管失败了。为了实现输出结果的一次性语义，将数据保存到外部数据存储的输出操作必须是幂等的，或者是保存结果和偏移量的原子事务。</li>\n</ol>\n<h2 id=\"direct源码\"><a href=\"#direct源码\" class=\"headerlink\" title=\"direct源码\"></a>direct源码</h2><p>获取offset集合，然后创建<code>DirectKafkaInputDStream</code>对象</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  class KafkaUtils  </span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[kafka] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getFromOffsets</span></span>(</span><br><span class=\"line\">      kc: <span class=\"type\">KafkaCluster</span>,</span><br><span class=\"line\">      kafkaParams: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>],</span><br><span class=\"line\">      topics: <span class=\"type\">Set</span>[<span class=\"type\">String</span>]</span><br><span class=\"line\">    ): <span class=\"type\">Map</span>[<span class=\"type\">TopicAndPartition</span>, <span class=\"type\">Long</span>] = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// createDirectStream方法kafkaParams入参：消费起始位置</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> reset = kafkaParams.get(<span class=\"string\">\"auto.offset.reset\"</span>).map(_.toLowerCase(<span class=\"type\">Locale</span>.<span class=\"type\">ROOT</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> result = <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">      topicPartitions &lt;- kc.getPartitions(topics).right</span><br><span class=\"line\">      leaderOffsets &lt;- (<span class=\"keyword\">if</span> (reset == <span class=\"type\">Some</span>(<span class=\"string\">\"smallest\"</span>)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// smallest表示最小offset,即从topic的开始位置消费所有消息.</span></span><br><span class=\"line\">        kc.getEarliestLeaderOffsets(topicPartitions)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// largest表示接受接收最大的offset(即最新消息),</span></span><br><span class=\"line\">        kc.getLatestLeaderOffsets(topicPartitions)</span><br><span class=\"line\">      &#125;).right</span><br><span class=\"line\">      <span class=\"comment\">// for循环中的 yield 会把当前的元素记下来，保存在集合中，循环结束后将返回该集合。Scala中for循环是有返回值的。如果被循环的是Map，返回的就是Map，被循环的是List，返回的就是List，以此类推。</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">yield</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// 存放for循环的计算结果：map[TopicAndPartition, LeaderOffset]</span></span><br><span class=\"line\">      leaderOffsets.map &#123; <span class=\"keyword\">case</span> (tp, lo) =&gt;</span><br><span class=\"line\">          (tp, lo.offset)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"type\">KafkaCluster</span>.checkErrors(result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">createDirectStream</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">DirectKafkaInputDStream</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">KD</span>, <span class=\"type\">VD</span>, (<span class=\"type\">K</span>, <span class=\"type\">V</span>)](</span><br><span class=\"line\">      ssc, kafkaParams, fromOffsets, messageHandler)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>DirectKafkaInputDStream.compute中创建KafkaRDD，并将offsets信息发送给inputStreamTracker.</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(validTime: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">KafkaRDD</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">U</span>, <span class=\"type\">T</span>, <span class=\"type\">R</span>]] = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Map[TopicAndPartition, LeaderOffset] topic的partiton对应偏移量集合</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> untilOffsets = clamp(latestLeaderOffsets(maxRetries))</span><br><span class=\"line\">    <span class=\"comment\">// 消息处理函数val messageHandler = (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message)</span></span><br><span class=\"line\">    <span class=\"comment\">// 创建KafkaRDD</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> rdd = <span class=\"type\">KafkaRDD</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">U</span>, <span class=\"type\">T</span>, <span class=\"type\">R</span>](</span><br><span class=\"line\">      context.sparkContext, kafkaParams, currentOffsets, untilOffsets, messageHandler)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将topic和partition信息包装成OffsetRange对象中</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> offsetRanges = currentOffsets.map &#123; <span class=\"keyword\">case</span> (tp, fo) =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> uo = untilOffsets(tp)</span><br><span class=\"line\">      <span class=\"type\">OffsetRange</span>(tp.topic, tp.partition, fo, uo.offset)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">   </span><br><span class=\"line\"> \t<span class=\"comment\">// 将OffsetRange报告给InputInfoTracker记录</span></span><br><span class=\"line\">    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)</span><br><span class=\"line\">    currentOffsets = untilOffsets.map(kv =&gt; kv._1 -&gt; kv._2.offset)</span><br><span class=\"line\">    <span class=\"type\">Some</span>(rdd)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>KafkaRDD计算时直接从kafka上拉取数据</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(thePart: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">R</span>] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> part = thePart.asInstanceOf[<span class=\"type\">KafkaRDDPartition</span>]</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">KafkaRDDIterator</span>(part, context)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaRDDIterator</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">      part: <span class=\"type\">KafkaRDDPartition</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">      context: <span class=\"type\">TaskContext</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">NextIterator</span>[<span class=\"type\">R</span>] </span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 根据metadata.broker.list初始化KafkaCluster，用来连接到kafka</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> kc = <span class=\"keyword\">new</span> <span class=\"type\">KafkaCluster</span>(kafkaParams)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">var</span> requestOffset = part.fromOffset</span><br><span class=\"line\">    <span class=\"keyword\">var</span> iter: <span class=\"type\">Iterator</span>[<span class=\"type\">MessageAndOffset</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 提供一个最优的host优先访问，最大化的减少重试次数</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> consumer:<span class=\"type\">SimpleConsumer</span> = &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 重试次数大于0</span></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (context.attemptNumber &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        kc.connectLeader(part.topic, part.partition).fold(</span><br><span class=\"line\">          errs =&gt; <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">SparkException</span>(</span><br><span class=\"line\">            <span class=\"string\">s\"Couldn't connect to leader for topic <span class=\"subst\">$&#123;part.topic&#125;</span> <span class=\"subst\">$&#123;part.partition&#125;</span>: \"</span> +</span><br><span class=\"line\">              errs.mkString(<span class=\"string\">\"\\n\"</span>)),</span><br><span class=\"line\">          consumer =&gt; consumer</span><br><span class=\"line\">        )</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          <span class=\"comment\">// 不用重试，直接连接</span></span><br><span class=\"line\">        kc.connect(part.host, part.port)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建请求拉取数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fetchBatch</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">MessageAndOffset</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> req = <span class=\"keyword\">new</span> <span class=\"type\">FetchRequestBuilder</span>()</span><br><span class=\"line\">        .addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes)</span><br><span class=\"line\">        .build()</span><br><span class=\"line\">      <span class=\"keyword\">val</span> resp = consumer.fetch(req)</span><br><span class=\"line\">      <span class=\"comment\">// 失败重试 </span></span><br><span class=\"line\">      handleFetchErr(resp)</span><br><span class=\"line\">      <span class=\"comment\">// kafka may return a batch that starts before the requested offset</span></span><br><span class=\"line\">      resp.messageSet(part.topic, part.partition)</span><br><span class=\"line\">        .iterator</span><br><span class=\"line\">        .dropWhile(_.offset &lt; requestOffset)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 拉取失败，通知另一个rdd重新尝试</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">handleFetchErr</span></span>(resp: <span class=\"type\">FetchResponse</span>) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (resp.hasError) &#123;</span><br><span class=\"line\">   \t\t<span class=\"comment\">// Let normal rdd retry sort out reconnect attempts</span></span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"type\">ErrorMapping</span>.exceptionFor(err)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getNext</span></span>(): <span class=\"type\">R</span> = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (iter == <span class=\"literal\">null</span> || !iter.hasNext) &#123;</span><br><span class=\"line\">          <span class=\"comment\">// 拉取数据</span></span><br><span class=\"line\">        iter = fetchBatch</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!iter.hasNext) &#123;</span><br><span class=\"line\">        assert(requestOffset == part.untilOffset, errRanOutBeforeEnd(part))</span><br><span class=\"line\">        finished = <span class=\"literal\">true</span></span><br><span class=\"line\">        <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">R</span>]</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          <span class=\"comment\">// 遍历拉取到的数据</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> item = iter.next()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (item.offset &gt;= part.untilOffset) &#123;</span><br><span class=\"line\">\t\t  <span class=\"comment\">// 如果当前item的偏移量大于需要拉取的最大偏移量则结束</span></span><br><span class=\"line\">          finished = <span class=\"literal\">true</span></span><br><span class=\"line\">          <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">R</span>]</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          requestOffset = item.nextOffset</span><br><span class=\"line\">            <span class=\"comment\">// 将拉取到的数据交由messageHandler处理</span></span><br><span class=\"line\">          messageHandler(<span class=\"keyword\">new</span> <span class=\"type\">MessageAndMetadata</span>(</span><br><span class=\"line\">            part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过chekpoint的方式保存offset</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// DStream中定义checkpoint的实现类</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DirectKafkaInputDStream</span> <span class=\"keyword\">extends</span> <span class=\"title\">InputDStream</span></span>&#123;</span><br><span class=\"line\">   <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> checkpointData =<span class=\"keyword\">new</span> <span class=\"type\">DirectKafkaInputDStreamCheckpointData</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DirectKafkaInputDStreamCheckpointData</span> <span class=\"keyword\">extends</span> <span class=\"title\">DStreamCheckpointData</span>(<span class=\"params\">this</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">batchForTime</span></span>: mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">Time</span>, <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>, <span class=\"type\">Long</span>, <span class=\"type\">Long</span>)]] = &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 定义一个不可变数组保存offset信息</span></span><br><span class=\"line\">      data.asInstanceOf[mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">Time</span>, <span class=\"type\">Array</span>[<span class=\"type\">OffsetRange</span>.<span class=\"type\">OffsetRangeTuple</span>]]]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","categories":["spark"],"tags":["spark","kafka"]},{"title":"spark源码分析环境搭建","url":"https://sustcoder.github.io/2018/12/20/2018-12-20-spark-sourceCode-run/","content":"<p>在学习spark的过程中发现很多博客对概念和原理的讲解存在矛盾或者理解不透彻，所以开始对照源码学习，发现根据概念总结去寻找对应源码，能更好理解，但随之而来的问题是好多源码看不懂，只跑example的话好多地方跑不到，但是结合测试类理解起来就方便多了。</p>\n<p>forck一份源码，在未修改源码的情况下（修改源码后，比如加注释等，在编译阶段容易报错）,使用gitbash进入项目的根目录下，执行下面2条命令使用mvn进行编译：</p>\n<ol>\n<li>设置编译时mven内存大小</li>\n</ol>\n<blockquote>\n<p>export MAVEN_OPTS=”-Xmx2g  -XX:ReservedCodeCacheSize=512m”</p>\n</blockquote>\n<ol start=\"2\">\n<li><code>-T -4</code> :启动四个线程 , <code>- P</code>：将hadoop编译进去 </li>\n</ol>\n<blockquote>\n<p>./build/mvn  -T 4 -Pyarn -Phadoop-2.6  -Dhadoop.version=2.6.0  -DskipTests clean package</p>\n</blockquote>\n<ol start=\"3\">\n<li>通过运行一个test测试是否编译完成</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">&gt; build/sbt</span><br><span class=\"line\">&gt; project core</span><br><span class=\"line\">&gt; testOnly *<span class=\"type\">RDDSuite</span> -- -z <span class=\"string\">\"serialization\"</span></span><br></pre></td></tr></table></figure>\n<p><strong>远程调试</strong></p>\n<p>根据spark的<a href=\"https://www.zhihu.com/question/24869894\" target=\"_blank\" rel=\"noopener\">pmc建议</a>使用远程调试运行test更方便，以下是使用远程调试步骤：</p>\n<ol>\n<li><p>使用idea通过maven导入导入项目：<code>File / Open，选定 Spark 根目录下的 pom.xml</code>,点击确定即可</p>\n</li>\n<li><p>建立remote debugger</p>\n<ol>\n<li>选取菜单项 Run &gt; Edit Configurations… 点击左上角加号，选取 Remote 建立一套远程调试配置，并命名为“Remote Debugging (listen)”：</li>\n<li>选择Debugger mode: <code>Listen to remote JVM</code></li>\n<li>选择Transport:<code>Socket</code></li>\n<li>配置完成后点击Debug，开始启动remote debugger的监听</li>\n</ol>\n<p>配置完成后如下图：</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog%2F2018%2Fspark%2Fsrccode%2F1546052559018.png\" alt=\"1546052559018\"></p>\n</li>\n<li><p>在SBT中启动远程调试，打开idea中的<code>sbt shell</code></p>\n<ol>\n<li><p>SBT 中的 settings key 是跟着当前 sub-project 走的，所以调试时需要切换到对应的模块，例如：切换到core模块，可以在sbt shell中执行以下命令</p>\n<blockquote>\n<p>project core</p>\n</blockquote>\n</li>\n<li><p>设置监听模式远程调试选项：</p>\n<blockquote>\n<p>set javaOptions in Test += “-agentlib:jdwp=transport=dt_socket,server=n,suspend=n,address=localhost:5005”</p>\n</blockquote>\n</li>\n<li><p>启动我们的 test case，<a href=\"http://www.scalatest.org/user_guide/using_the_runner\" target=\"_blank\" rel=\"noopener\">sbt test教程</a></p>\n<blockquote>\n<p>testOnly *RDDSuite – -z “basic operations”</p>\n</blockquote>\n</li>\n</ol>\n<p>如果我们在test调用的代码里面打断点后，就可以进行调试了，结果如图：</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/1546053639151.png\" alt=\"1546053639151\"></p>\n</li>\n</ol>\n<p><strong>报错信息</strong>1</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">[<span class=\"type\">INFO</span>] <span class=\"type\">BUILD</span> <span class=\"type\">FAILURE</span></span><br><span class=\"line\">[<span class=\"type\">INFO</span>] ------------------------------------------------------------------------</span><br><span class=\"line\">[<span class=\"type\">INFO</span>] <span class=\"type\">Total</span> time: <span class=\"number\">05</span>:<span class=\"number\">31</span> min</span><br><span class=\"line\">[<span class=\"type\">INFO</span>] <span class=\"type\">Finished</span> at: <span class=\"number\">2018</span><span class=\"number\">-12</span><span class=\"number\">-25</span>T17:<span class=\"number\">04</span>:<span class=\"number\">22</span>+<span class=\"number\">08</span>:<span class=\"number\">00</span></span><br><span class=\"line\">[<span class=\"type\">INFO</span>] ------------------------------------------------------------------------</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] <span class=\"type\">Failed</span> to execute goal org.apache.maven.plugins:maven-antrun-plugin:<span class=\"number\">1.8</span>:run (<span class=\"keyword\">default</span>) on project spark-core_2<span class=\"number\">.11</span>: <span class=\"type\">An</span> <span class=\"type\">Ant</span> <span class=\"type\">BuildException</span> has occured: <span class=\"type\">Execute</span> failed: java.io.<span class=\"type\">IOException</span>: <span class=\"type\">Cannot</span> run program <span class=\"string\">\"bash\"</span> (in directory <span class=\"string\">\"E:\\data\\gitee\\fork-spark2.2.0\\spark\\core\"</span>): <span class=\"type\">CreateProcess</span> error=<span class=\"number\">2</span>, 系统找不到指定的文件。</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] around <span class=\"type\">Ant</span> part ...&lt;exec executable=<span class=\"string\">\"bash\"</span>&gt;... @ <span class=\"number\">4</span>:<span class=\"number\">27</span> in <span class=\"type\">E</span>:\\data\\gitee\\fork-spark2<span class=\"number\">.2</span><span class=\"number\">.0</span>\\spark\\core\\target\\antrun\\build-main.xml</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] -&gt; [<span class=\"type\">Help</span> <span class=\"number\">1</span>]</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>]</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] <span class=\"type\">To</span> see the full stack trace of the errors, re-run <span class=\"type\">Maven</span> <span class=\"keyword\">with</span> the -e switch.</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] <span class=\"type\">Re</span>-run <span class=\"type\">Maven</span> using the -<span class=\"type\">X</span> switch to enable full debug logging.</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>]</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] <span class=\"type\">For</span> more information about the errors and possible solutions, please read the following articles:</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] [<span class=\"type\">Help</span> <span class=\"number\">1</span>] http:<span class=\"comment\">//cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException</span></span><br><span class=\"line\">[<span class=\"type\">ERROR</span>]</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>] <span class=\"type\">After</span> correcting the problems, you can resume the build <span class=\"keyword\">with</span> the command</span><br><span class=\"line\">[<span class=\"type\">ERROR</span>]   mvn &lt;goals&gt; -rf :spark-core_2<span class=\"number\">.11</span></span><br></pre></td></tr></table></figure>\n<p>解决办法：Spark编译需要在bash环境下，直接在windows环境下编译会报不支持bash错误，</p>\n<ol>\n<li>利用git的bash窗口进行编译</li>\n<li>用<code>Windows Subsystem for Linux</code>,具体的操作，大家可以参考<a href=\"https://medium.com/hungys-blog/windows-subsystem-for-linux-configuration-caf2f47d0dfb\" target=\"_blank\" rel=\"noopener\">这篇文章</a></li>\n</ol>\n<p><strong>报错信息2</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Output</span> path <span class=\"type\">E</span>:\\data\\gitee\\fork-spark2<span class=\"number\">.2</span><span class=\"number\">.0</span>\\spark\\external\\flume-assembly\\target\\scala<span class=\"number\">-2.11</span>\\classes is shared between: <span class=\"type\">Module</span> <span class=\"symbol\">'spark</span>-streaming-flume-assembly' production, <span class=\"type\">Module</span> <span class=\"symbol\">'spark</span>-streaming-flume-assembly_2<span class=\"number\">.11</span>' production</span><br><span class=\"line\"><span class=\"type\">Please</span> configure separate output paths to proceed <span class=\"keyword\">with</span> the compilation.</span><br><span class=\"line\"><span class=\"type\">TIP</span>: you can use <span class=\"type\">Project</span> <span class=\"type\">Artifacts</span> to combine compiled classes <span class=\"keyword\">if</span> needed.</span><br></pre></td></tr></table></figure>\n<p>解决办法： 项目里面.idea文件夹下，有一个models.xml文件，里面同一个文件包含了两个不同的引用。猜测原因maven编译完成后，在idea中打开时又再次选择使用sbt进行编译，结果出现了两份导致，再删掉所有.iml文件后，重新通过mvn进行编译，并且在打开时不选择用sbt进行编译解决。</p>\n<p><strong>报错信息</strong>4</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">ERROR</span>: <span class=\"type\">Cannot</span> load <span class=\"keyword\">this</span> <span class=\"type\">JVM</span> <span class=\"type\">TI</span> agent twice, check your java command line <span class=\"keyword\">for</span> duplicate jdwp options.</span><br><span class=\"line\"><span class=\"type\">Disconnected</span> from server</span><br><span class=\"line\"><span class=\"type\">Error</span> occurred during initialization of <span class=\"type\">VM</span></span><br><span class=\"line\">agent library failed to init: jdwp</span><br></pre></td></tr></table></figure>\n<p>解决办法：执行了多次设置调试模式的方法<code>set javaOptions in Test += &quot;-agentlib:jdwp=transport=dt_socket,server=n,suspend=n,address=localhost:5005&quot;</code>导致，关掉sbt shell重启重试</p>\n<p><strong>参考</strong>：</p>\n<p><a href=\"https://spark.apache.org/developer-tools.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/developer-tools.html</a></p>\n<p><a href=\"https://www.zhihu.com/question/24869894\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/24869894</a> </p>\n","categories":["spark"],"tags":["spark","源码"]},{"title":"shuffle 详解","url":"https://sustcoder.github.io/2018/12/17/2018-12-17-shuffle-explain/","content":"<p><strong>转载：时延军.<a href=\"http://shiyanjun.cn\" target=\"_blank\" rel=\"noopener\">http://shiyanjun.cn</a></strong></p>\n<p>Spark在Map阶段调度运行的ShuffleMapTask，最后会生成.data和.index文件，可以通过我的这篇文章 <a href=\"http://shiyanjun.cn/archives/1655.html\" target=\"_blank\" rel=\"noopener\">Spark Shuffle过程分析：Map阶段处理流程</a> 了解具体流程和详情。同时，在Executor上运行一个ShuffleMapTask，返回了一个MapStatus对象，下面是ShuffleMapTask执行后返回结果的相关代码片段：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\">  writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">  writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">  writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (writer != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        writer.stop(success = <span class=\"literal\">false</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">        log.debug(<span class=\"string\">\"Could not stop writer\"</span>, e)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果ShuffleMapTask执行过程没有发生异常，则最后执行的调用为：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">writer.stop(success = <span class=\"literal\">true</span>).get</span><br></pre></td></tr></table></figure>\n<p>这里返回了一个MapStatus类型的对象，MapStatus的定义如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"keyword\">sealed</span> <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">MapStatus</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">location</span></span>: <span class=\"type\">BlockManagerId</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getSizeForBlock</span></span>(reduceId: <span class=\"type\">Int</span>): <span class=\"type\">Long</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中包含了运行ShuffleMapTask所在的BlockManager的地址，以及后续Reduce阶段每个ResultTask计算需要Map输出的大小（Size）。我们可以看下MapStatus如何创建的，在SortShuffleWriter的write()方法中，可以看到MapStatus的创建，如下代码所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br></pre></td></tr></table></figure>\n<p>继续跟踪可以看到，调用了MapStatus的伴生对象的apply()方法：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(loc: <span class=\"type\">BlockManagerId</span>, uncompressedSizes: <span class=\"type\">Array</span>[<span class=\"type\">Long</span>]): <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (uncompressedSizes.length &gt; <span class=\"number\">2000</span>) &#123;</span><br><span class=\"line\">    <span class=\"type\">HighlyCompressedMapStatus</span>(loc, uncompressedSizes)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">CompressedMapStatus</span>(loc, uncompressedSizes)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>uncompressedSizes表示Partition的个数，如果大于2000则创建HighlyCompressedMapStatus对象，否则创建CompressedMapStatus对象，他们具体的实现可以参考源码。</p>\n<p><strong>含有Shuffle过程的Spark Application示例</strong></p>\n<p>我们先给出一个简单的Spark Application程序代码，如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">al rdd = sc.textFile(<span class=\"string\">\"/temp/*.h\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> finalRdd = rdd.flatMap(line =&gt; line.split(<span class=\"string\">\"\\\\s+\"</span>)).map(w =&gt; (w, <span class=\"number\">1</span>)).reduceByKey(_ + _)</span><br><span class=\"line\">finalRdd.toDebugString</span><br><span class=\"line\">finalRdd.saveAsTextFile(<span class=\"string\">\"/temp/output\"</span>)</span><br></pre></td></tr></table></figure>\n<p>通过RDD的toDebugString()方法，打印调试信息：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; finalRdd.toDebugString</span><br><span class=\"line\">res0: <span class=\"type\">String</span> = </span><br><span class=\"line\">(<span class=\"number\">133</span>) <span class=\"type\">ShuffledRDD</span>[<span class=\"number\">6</span>] at reduceByKey at &lt;console&gt;:<span class=\"number\">30</span> []</span><br><span class=\"line\">  +-(<span class=\"number\">133</span>) <span class=\"type\">MapPartitionsRDD</span>[<span class=\"number\">5</span>] at map at &lt;console&gt;:<span class=\"number\">30</span> []</span><br><span class=\"line\">      |   <span class=\"type\">MapPartitionsRDD</span>[<span class=\"number\">4</span>] at flatMap at &lt;console&gt;:<span class=\"number\">30</span> []</span><br><span class=\"line\">      |   /temp<span class=\"comment\">/*.h MapPartitionsRDD[3] at textFile at &lt;console&gt;:29 []</span></span><br><span class=\"line\"><span class=\"comment\">      |   /temp/*.h HadoopRDD[2] at textFile at &lt;console&gt;:29 []</span></span><br></pre></td></tr></table></figure>\n<p>可以看到这个过程中，调用了reduceByKey()，创建了一个ShuffledRDD，这在计算过程中会执行Shuffle操作。</p>\n<p><strong>ShuffleMapTask执行结果上报处理流程</strong></p>\n<p>Spark Application提交以后，会生成ShuffleMapStage和/或ResultStage，而一个ShuffleMapStage对应一组实际需要运行的ShuffleMapTask，ResultStage对应一组实际需要运行ResultTask，每组Task都是有TaskSetManager来管理的，并且只有ShuffleMapStage对应的一组ShuffleMapTask都运行成功结束以后，才会调度ResultStage。<br>所以，我们这里关注的是，当ShuffleMapStage中最后一个ShuffleMapTask运行成功后，如何将Map阶段的信息上报给调度器（Driver上的TaskScheduler和DAGScheduler），了解这个处理流程对理解后续的Reduce阶段处理至关重要，这个过程的详细处理流程，如下图所示：<br><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/ShuffleMapTask.succeeded.png\" alt=\"ShuffleMapTask.succeeded\"><br>我们将整个流程按照顺序分为如下几个过程来描述：</p>\n<p><strong>ShuffleMapTask完成后处理结果</strong><br>Executor会启动一个TaskRunner线程来运行ShuffleMapTask，ShuffleMapTask完成后，会对结果进行序列化处理，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> directResult = <span class=\"keyword\">new</span> <span class=\"type\">DirectTaskResult</span>(valueBytes, accumUpdates)</span><br><span class=\"line\"><span class=\"keyword\">val</span> serializedDirectResult = ser.serialize(directResult)</span><br><span class=\"line\"><span class=\"keyword\">val</span> resultSize = serializedDirectResult.limit</span><br></pre></td></tr></table></figure>\n<p>根据序列化后结果serializedDirectResult的大小resultSize，会进行一些优化，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> serializedResult: <span class=\"type\">ByteBuffer</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (maxResultSize &gt; <span class=\"number\">0</span> &amp;&amp; resultSize &gt; maxResultSize) &#123;</span><br><span class=\"line\">    logWarning(<span class=\"string\">s\"Finished <span class=\"subst\">$taskName</span> (TID <span class=\"subst\">$taskId</span>). Result is larger than maxResultSize \"</span> +</span><br><span class=\"line\">      <span class=\"string\">s\"(<span class=\"subst\">$&#123;Utils.bytesToString(resultSize)&#125;</span> &gt; <span class=\"subst\">$&#123;Utils.bytesToString(maxResultSize)&#125;</span>), \"</span> +</span><br><span class=\"line\">      <span class=\"string\">s\"dropping it.\"</span>)</span><br><span class=\"line\">    ser.serialize(<span class=\"keyword\">new</span> <span class=\"type\">IndirectTaskResult</span>[<span class=\"type\">Any</span>](<span class=\"type\">TaskResultBlockId</span>(taskId), resultSize))</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (resultSize &gt; maxDirectResultSize) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockId = <span class=\"type\">TaskResultBlockId</span>(taskId)</span><br><span class=\"line\">    env.blockManager.putBytes(</span><br><span class=\"line\">      blockId,</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">ChunkedByteBuffer</span>(serializedDirectResult.duplicate()),</span><br><span class=\"line\">      <span class=\"type\">StorageLevel</span>.<span class=\"type\">MEMORY_AND_DISK_SER</span>)</span><br><span class=\"line\">    logInfo(</span><br><span class=\"line\">      <span class=\"string\">s\"Finished <span class=\"subst\">$taskName</span> (TID <span class=\"subst\">$taskId</span>). <span class=\"subst\">$resultSize</span> bytes result sent via BlockManager)\"</span>)</span><br><span class=\"line\">    ser.serialize(<span class=\"keyword\">new</span> <span class=\"type\">IndirectTaskResult</span>[<span class=\"type\">Any</span>](blockId, resultSize))</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    logInfo(<span class=\"string\">s\"Finished <span class=\"subst\">$taskName</span> (TID <span class=\"subst\">$taskId</span>). <span class=\"subst\">$resultSize</span> bytes result sent to driver\"</span>)</span><br><span class=\"line\">    serializedDirectResult</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果结果大小没有超过指定的DirectTaskResult的最大限制值maxDirectResultSize，就直接将上面的DirectTaskResult的序列化结果发送给Driver；如果结果大小超过了Task结果的最大限制值maxResultSize，则直接丢弃结果；否则，当结果大小介于maxDirectResultSize与maxResultSize之间时，会基于Task ID创建一个TaskResultBlockId，然后通过BlockManager将结果暂时保存在Executor上（DiskStore或MemoryStore），以便后续计算直接请求获取该数据。<br>最后，结果会调用CoarseGrainedExecutorBackend的statusUpdate方法，如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">execBackend.statusUpdate(taskId, <span class=\"type\">TaskState</span>.<span class=\"type\">FINISHED</span>, serializedResult)</span><br></pre></td></tr></table></figure>\n<p>将Task对应的运行状态、运行结果发送给Driver。</p>\n<p><strong>Driver获取Task运行结果</strong><br>集群模式下，Driver端负责接收Task运行结果的是CoarseGrainedSchedulerBackend，它内部有一个DriverEndpoint来负责实际网络通信，以及接收Task状态及其结果，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"type\">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</span><br><span class=\"line\">  scheduler.statusUpdate(taskId, state, data.value)</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"type\">TaskState</span>.isFinished(state)) &#123;</span><br><span class=\"line\">    executorDataMap.get(executorId) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(executorInfo) =&gt;</span><br><span class=\"line\">        executorInfo.freeCores += scheduler.<span class=\"type\">CPUS_PER_TASK</span></span><br><span class=\"line\">        makeOffers(executorId)</span><br><span class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">        <span class=\"comment\">// Ignoring the update since we don't know about the executor.</span></span><br><span class=\"line\">        logWarning(<span class=\"string\">s\"Ignored task status update (<span class=\"subst\">$taskId</span> state <span class=\"subst\">$state</span>) \"</span> +</span><br><span class=\"line\">          <span class=\"string\">s\"from unknown executor with ID <span class=\"subst\">$executorId</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>如果消息类型为StatusUpdate，则首先直接调用了TaskSchedulerImpl的statusUpdate()方法，来获取Task的运行状态及其结果，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"type\">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</span><br><span class=\"line\">  scheduler.statusUpdate(taskId, state, data.value)</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"type\">TaskState</span>.isFinished(state)) &#123;</span><br><span class=\"line\">    executorDataMap.get(executorId) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(executorInfo) =&gt;</span><br><span class=\"line\">        executorInfo.freeCores += scheduler.<span class=\"type\">CPUS_PER_TASK</span></span><br><span class=\"line\">        makeOffers(executorId)</span><br><span class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">        <span class=\"comment\">// Ignoring the update since we don't know about the executor.</span></span><br><span class=\"line\">        logWarning(<span class=\"string\">s\"Ignored task status update (<span class=\"subst\">$taskId</span> state <span class=\"subst\">$state</span>) \"</span> +</span><br><span class=\"line\">          <span class=\"string\">s\"from unknown executor with ID <span class=\"subst\">$executorId</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>如果Task状态是TaskState.FINISHED，则通过TaskResultGetter来获取Task运行返回的结果，这里存在DirectTaskResult和IndirectTaskResult两种类型的结果，他们的处理方式不同：对于DirectTaskResult类型的结果，如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> directResult: <span class=\"type\">DirectTaskResult</span>[_] =&gt;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!taskSetManager.canFetchMoreResults(serializedData.limit())) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// deserialize \"value\" without holding any lock so that it won't block other threads.</span></span><br><span class=\"line\">  directResult.value(taskResultSerializer.get())</span><br></pre></td></tr></table></figure>\n<p>直接从DirectTaskResult中就可以通过反序列化得到结果，而对于IndirectTaskResult类型的结果，逻辑相对复杂一些，如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> directResult: <span class=\"type\">DirectTaskResult</span>[_] =&gt;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!taskSetManager.canFetchMoreResults(serializedData.limit())) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// deserialize \"value\" without holding any lock so that it won't block other threads.</span></span><br><span class=\"line\">  directResult.value(taskResultSerializer.get())</span><br></pre></td></tr></table></figure>\n<p>结果大小超过指定的限制值，在ShuffleMapTask运行过程中会直接通过BlockManager存储到Executor的内存/磁盘上，这里就会根据结果Block ID，通过BlockManager来获取到结果对应的Block数据。</p>\n<p><strong>更新Driver端Task、Stage状态，并调度Stage运行</strong><br>获取到ShuffleMapTask运行的结果数据后，需要更新TaskSetManager中对应的状态信息，以便为后续调度Task运行提供决策支持，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scheduler.handleSuccessfulTask(taskSetManager, tid, result)</span><br></pre></td></tr></table></figure>\n<p>上面代码调用了TaskSetManager的handleSuccessfulTask()方法，更新相关状态，同时继续更新DAGScheduler中对应的状态，代码片段如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">sched.dagScheduler.taskEnded(tasks(index), <span class=\"type\">Success</span>, result.value(), result.accumUpdates, info)</span><br><span class=\"line\">maybeFinishTaskSet()</span><br></pre></td></tr></table></figure>\n<p>调用DAGScheduler的taskEnded()方法，更新Stage信息。如果一个ShuffleMapTask运行完成后，而且是对应的ShuffleMapStage中最后一个ShuffleMapTask，则该ShuffleMapStage也完成了，则会注册该ShuffleMapStage运行得到的所有Map输出结果，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">mapOutputTracker.registerMapOutputs(</span><br><span class=\"line\">  shuffleStage.shuffleDep.shuffleId,</span><br><span class=\"line\">  shuffleStage.outputLocInMapOutputTrackerFormat(),</span><br><span class=\"line\">  changeEpoch = <span class=\"literal\">true</span>)</span><br></pre></td></tr></table></figure>\n<p>上面MapOutputTracker维护了一个ConcurrentHashMap[Int, Array[MapStatus]]内存结构，用来管理每个ShuffleMapTask运行完成返回的结果数据，其中Key是Shuffle ID，Value使用数组记录每个Map ID对应的输出结果信息。<br>下面代码判断ShuffleMapStage是否可用，从而进行相应的处理：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (!shuffleStage.isAvailable) &#123;</span><br><span class=\"line\">  <span class=\"comment\">// Some tasks had failed; let's resubmit this shuffleStage.</span></span><br><span class=\"line\">  <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Lower-level scheduler should also deal with this</span></span><br><span class=\"line\">  logInfo(<span class=\"string\">\"Resubmitting \"</span> + shuffleStage + <span class=\"string\">\" (\"</span> + shuffleStage.name +</span><br><span class=\"line\">    <span class=\"string\">\") because some of its tasks had failed: \"</span> +</span><br><span class=\"line\">    shuffleStage.findMissingPartitions().mkString(<span class=\"string\">\", \"</span>))</span><br><span class=\"line\">  submitStage(shuffleStage)</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">  <span class=\"comment\">// Mark any map-stage jobs waiting on this stage as finished</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (shuffleStage.mapStageJobs.nonEmpty) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> stats = mapOutputTracker.getStatistics(shuffleStage.shuffleDep)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (job &lt;- shuffleStage.mapStageJobs) &#123;</span><br><span class=\"line\">      markMapStageJobAsFinished(job, stats)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  submitWaitingChildStages(shuffleStage)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果ShuffleMapStage不可用，说明还有某些Partition对应的结果没有计算（或者某些计算失败），Spark会重新提交该ShuffleMapStage；如果可用，则说明当前ShuffleMapStage已经运行完成，更新对应的状态和结果信息：标记ShuffleMapStage已经完成，同时提交Stage依赖关系链中相邻下游的Stage运行。如果后面是ResultStage，则会提交该ResultStage运行。</p>\n<p><strong>释放资源、重新调度Task运行</strong><br>一个ShuffleMapTask运行完成，要释放掉对应的Executor占用的资源，在Driver端会增加对应的资源列表，同时调度Task到该释放的Executor上运行，可见CoarseGrainedSchedulerBackend.DriverEndpoint中对应的处理逻，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (<span class=\"type\">TaskState</span>.isFinished(state)) &#123;</span><br><span class=\"line\">  executorDataMap.get(executorId) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(executorInfo) =&gt;</span><br><span class=\"line\">      executorInfo.freeCores += scheduler.<span class=\"type\">CPUS_PER_TASK</span></span><br><span class=\"line\">      makeOffers(executorId)</span><br></pre></td></tr></table></figure>\n<p>上面makeOffers()方法，会调度一个Task到该executorId标识的Executor上运行。如果ShuffleMapStage已经完成，那么这里可能会调度ResultStage阶段的ResultTask运行。</p>\n<p><strong>Reduce阶段处理流程</strong></p>\n<p>上面我们给出的例子中，执行reduceByKey后，由于上游的RDD没有按照key执行分区操作，所以必定会创建一个ShuffledRDD，可以在PairRDDFunctions类的源码中看到combineByKeyWithClassTag方法，实现代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combineByKeyWithClassTag</span></span>[<span class=\"type\">C</span>](</span><br><span class=\"line\">    createCombiner: <span class=\"type\">V</span> =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    mergeValue: (<span class=\"type\">C</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    partitioner: <span class=\"type\">Partitioner</span>,</span><br><span class=\"line\">    mapSideCombine: <span class=\"type\">Boolean</span> = <span class=\"literal\">true</span>,</span><br><span class=\"line\">    serializer: <span class=\"type\">Serializer</span> = <span class=\"literal\">null</span>)(<span class=\"keyword\">implicit</span> ct: <span class=\"type\">ClassTag</span>[<span class=\"type\">C</span>]): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = self.withScope &#123;</span><br><span class=\"line\">  require(mergeCombiners != <span class=\"literal\">null</span>, <span class=\"string\">\"mergeCombiners must be defined\"</span>) <span class=\"comment\">// required as of Spark 0.9.0</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (keyClass.isArray) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mapSideCombine) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">SparkException</span>(<span class=\"string\">\"Cannot use map-side combining with array keys.\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (partitioner.isInstanceOf[<span class=\"type\">HashPartitioner</span>]) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">SparkException</span>(<span class=\"string\">\"Default partitioner cannot partition array keys.\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> aggregator = <span class=\"keyword\">new</span> <span class=\"type\">Aggregator</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">    self.context.clean(createCombiner),</span><br><span class=\"line\">    self.context.clean(mergeValue),</span><br><span class=\"line\">    self.context.clean(mergeCombiners))</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (self.partitioner == <span class=\"type\">Some</span>(partitioner)) &#123;</span><br><span class=\"line\">    self.mapPartitions(iter =&gt; &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> context = <span class=\"type\">TaskContext</span>.get()</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class=\"line\">    &#125;, preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ShuffledRDD</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](self, partitioner)</span><br><span class=\"line\">      .setSerializer(serializer)</span><br><span class=\"line\">      .setAggregator(aggregator)</span><br><span class=\"line\">      .setMapSideCombine(mapSideCombine)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里，因为我们给出的例子的上下文中，self.partitioner == Some(partitioner)不成立，所以最终创建了一个ShuffledRDD对象。所以，对于Reduce阶段的处理流程，我们基于ShuffledRDD的处理过程来进行分析。<br>我们从ResultTask类开始，该类中实现了runTask()方法，代码如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">  <span class=\"comment\">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> deserializeStartTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> ser = <span class=\"type\">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> (rdd, func) = ser.deserialize[(<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>)](</span><br><span class=\"line\">    <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value), <span class=\"type\">Thread</span>.currentThread.getContextClassLoader)</span><br><span class=\"line\">  _executorDeserializeTime = <span class=\"type\">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class=\"line\"> </span><br><span class=\"line\">  func(context, rdd.iterator(partition, context))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中，最核心的就是上面的rdd.iterator()调用，具体处理过程，如下图所示：<br><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/RDD.iterator.png\" alt=\"RDD.iterator\"><br>最终，它用来计算一个RDD，即对应ShuffledRDD的计算。iterator()方法是在RDD类中给出的，如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;</span><br><span class=\"line\">    getOrCompute(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>跟踪getOrCompute()方法，最终应该是在ShuffledRDD类的compute()方法中定义。</p>\n<p><strong>ShuffledRDD计算</strong><br>ShuffledRDD对应的compute方法的实现代码，如下所示：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> dep = dependencies.head.asInstanceOf[<span class=\"type\">ShuffleDependency</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">  <span class=\"type\">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class=\"number\">1</span>, context)</span><br><span class=\"line\">    .read()</span><br><span class=\"line\">    .asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面主要是通过BlockStoreShuffleReader的read()方法，来实现ShuffledRDD的计算，我们通过下面的序列图来看一下详细的执行流程：<br><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/Reduce.run_.png\" alt=\"Reduce.run\"><br>跟踪Map的输出结果，是基于Executor端的MapOutputTracker与Driver端的MapOutputTrackerMaster来实现的，其中MapOutputTrackerMaster作为Server端，MapOutputTracker作为Client端。Driver端管理了一个Spark Application计算程序的ShuffleMapStage中所有ShuffleMapTask的输出，所以在Reduce过程中Executor会通过MapOutputTracker与Driver的MapOutputTrackerMaster进行通信获取。<br>调用BlockStoreShuffleReader的read()方法，最终得到了Reduce过程中需要的输入，即ShuffleMapTask的输出结果所在的位置。通常，为了能够使计算在数据本地进行，每个ResultTask运行所在的Executor节点会存在对应的Map输出，是通过BlockManager来管理这些数据的，通过Block ID来标识。所以，上图中最后返回了一个BlockManager ID及受其管理的一个Block ID列表，然后Executor上的ResultTask就能够根据BlockManager ID来获取到对应的Map输出数据，从而进行数据的计算。<br>ResultTask运行完成后，最终返回一个记录的迭代器，此时计算得到的最终结果数据，是在各个ResultTask运行所在的Executor上的，而数据又是按Block来存储的，是通过BlockManager来管理的。</p>\n<p><strong>保存结果RDD</strong><br>根据前面的程序示例，最后调用了RDD的saveAsTextFile()，这会又生成一个ResultStage，进而对应着一组ResultTask。保存结果RDD的处理流程，如下图所示：<br><img src=\"http://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/RDD.saveAsTextFile.png\" alt=\"RDD.saveAsTextFile\"><br>上面整个流程，会执行设置RDD输出到HDFS的Writer（一个写文件的函数）、提交ResultStage、构建包含ResultTask的TaskSet、调度ResultTask到指定Executor上执行这几个核心的过程。实际上，在每个Executor上运行的ResultTask的核心处理逻辑，主要是下面这段函数代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> writer = <span class=\"keyword\">new</span> <span class=\"type\">SparkHadoopWriter</span>(hadoopConf)</span><br><span class=\"line\">writer.preSetup()</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">val</span> writeToFile = (context: <span class=\"type\">TaskContext</span>, iter: <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)]) =&gt; &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> taskAttemptId = (context.taskAttemptId % <span class=\"type\">Int</span>.<span class=\"type\">MaxValue</span>).toInt</span><br><span class=\"line\">  <span class=\"keyword\">val</span> (outputMetrics, callback) = <span class=\"type\">SparkHadoopWriterUtils</span>.initHadoopOutputMetrics(context)</span><br><span class=\"line\"> </span><br><span class=\"line\">  writer.setup(context.stageId, context.partitionId, taskAttemptId)</span><br><span class=\"line\">  writer.open()</span><br><span class=\"line\">  <span class=\"keyword\">var</span> recordsWritten = <span class=\"number\">0</span>L</span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"type\">Utils</span>.tryWithSafeFinallyAndFailureCallbacks &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (iter.hasNext) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> record = iter.next()</span><br><span class=\"line\">      writer.write(record._1.asInstanceOf[<span class=\"type\">AnyRef</span>], record._2.asInstanceOf[<span class=\"type\">AnyRef</span>])</span><br><span class=\"line\"> </span><br><span class=\"line\">      <span class=\"comment\">// Update bytes written metric every few records</span></span><br><span class=\"line\">      <span class=\"type\">SparkHadoopWriterUtils</span>.maybeUpdateOutputMetrics(outputMetrics, callback, recordsWritten)</span><br><span class=\"line\">      recordsWritten += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;(finallyBlock = writer.close())</span><br><span class=\"line\">  writer.commit()</span><br><span class=\"line\">  outputMetrics.setBytesWritten(callback())</span><br><span class=\"line\">  outputMetrics.setRecordsWritten(recordsWritten)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>还记得我们在计算ShuffledRDD的过程中，最终的ResultTask生成了一个结果的迭代器。当调用saveAsTextFile()时，ResultStage对应的一组ResultTask会在Executor上运行，将每个迭代器对应的结果数据保存到HDFS上。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md\" target=\"_blank\" rel=\"noopener\">https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md</a></p>\n<p><a href=\"https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-study.md\" target=\"_blank\" rel=\"noopener\">https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-study.md</a></p>\n<p><a href=\"https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-hash-sort.md\" target=\"_blank\" rel=\"noopener\">https://github.com/ColZer/DigAndBuried/blob/master/spark/shuffle-hash-sort.md</a></p>\n<p><a href=\"https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html\" target=\"_blank\" rel=\"noopener\">https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html</a></p>\n<p><a href=\"http://sharkdtu.com/posts/spark-shuffle.html\" target=\"_blank\" rel=\"noopener\">http://sharkdtu.com/posts/spark-shuffle.html</a></p>\n","categories":["spark"],"tags":["spark","RDD"]},{"title":"sparkStreaming源码解析之容错","url":"https://sustcoder.github.io/2018/12/12/2018-12-12-sparkStreaming-sourceCodeAnalysis_faultTolerance/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_DataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis_faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps753E.tmp.jpg\" alt=\"img\"></p>\n<p>​    策略        优点            缺点</p>\n<p>(1) 热备        无 recover time        需要占用双倍资源</p>\n<p>(2) 冷备        十分可靠                存在 recover time</p>\n<p>(3) 重放        不占用额外资源        存在 recover time</p>\n<p>(4) 忽略        无 recover time        准确性有损失</p>\n<h1 id=\"1-driver端容错\"><a href=\"#1-driver端容错\" class=\"headerlink\" title=\"1. driver端容错\"></a>1. <strong>driver端容错</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D6.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"2-executor端容错\"><a href=\"#2-executor端容错\" class=\"headerlink\" title=\"2. executor端容错\"></a>2. <strong>executor端容错</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D7.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"2-1-热备\"><a href=\"#2-1-热备\" class=\"headerlink\" title=\"2.1. 热备\"></a>2.1. <strong>热备</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D8.tmp.jpg\" alt=\"img\"> </p>\n<p>Receiver 收到的数据，通过 ReceiverSupervisorImpl，将数据交给 BlockManager 存储；而 BlockManager 本身支持将数据 replicate() 到另外的 executor 上，这样就完成了 Receiver 源头数据的热备过程。</p>\n<p>而在计算时，计算任务首先将获取需要的块数据，这时如果一个 executor 失效导致一份数据丢失，那么计算任务将转而向另一个 executor 上的同一份数据获取数据。因为另一份块数据是现成的、不需要像冷备那样重新读取的，所以这里不会有 recovery time。</p>\n<h3 id=\"2-1-1-备份\"><a href=\"#2-1-1-备份\" class=\"headerlink\" title=\"2.1.1. 备份\"></a>2.1.1. <strong>备份</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9D9.tmp.jpg\" alt=\"img\"> </p>\n<p>备份流程：</p>\n<p>​    先保存此block块，如果保存失败则不再进行备份，如果保存成功则获取保存的block块，执行复制操作。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockManager</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doPutIterator</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tdoPut(blockId,level,tellMaster)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 存储数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(level)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tmemoryStore.putIteratorAsBytes（）</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(level.useDisk)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tdiskStore.put()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 当前block已经存储成功则继续：\t\t</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(blockWasSuccessfullyStored)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 报告结果给master</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(tellMaster)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\treportBlockStatus(blockid,status)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 备份</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span>(level.replication&gt;<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"comment\">// 从上面保存成功的位置获取block</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t <span class=\"keyword\">val</span> bytesToReplicate =doGetLocalBytes(blockId, info)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"comment\">// 正式备份</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\treplicate(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tbytesToReplicate, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tlevel</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-1-2-恢复\"><a href=\"#2-1-2-恢复\" class=\"headerlink\" title=\"2.1.2. 恢复\"></a>2.1.2. <strong>恢复</strong></h3><p>计算任务首先将获取需要的块数据，这时如果一个 executor 失效导致一份数据丢失，那么计算任务将转而向另一个 executor 上的同一份数据获取数据。因为另一份块数据是现成的、不需要像冷备那样重新读取的，所以这里不会有 recovery time。</p>\n<h2 id=\"2-2-冷备\"><a href=\"#2-2-冷备\" class=\"headerlink\" title=\"2.2. 冷备\"></a>2.2. <strong>冷备</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DA.tmp.jpg\" alt=\"img\"> </p>\n<p>冷备是每次存储块数据时，除了存储到本 executor，还会把块数据作为 log 写出到 WriteAheadLog 里作为冷备。这样当 executor 失效时，就由另外的 executor 去读 WAL，再重做 log 来恢复块数据。WAL 通常写到可靠存储如 HDFS 上，所以恢复时可能需要一段 recover time</p>\n<h3 id=\"2-2-1-WriteAheadLog\"><a href=\"#2-2-1-WriteAheadLog\" class=\"headerlink\" title=\"2.2.1. WriteAheadLog\"></a>2.2.1. <strong>WriteAheadLog</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DB.tmp.jpg\" alt=\"img\"> </p>\n<p>WriteAheadLog 的特点是顺序写入，所以在做数据备份时效率较高，但在需要恢复数据时又需要顺序读取，所以需要一定 recovery time。</p>\n<p>不过对于 Spark Streaming 的块数据冷备来讲，在恢复时也非常方便。这是因为，对某个块数据的操作只有一次（即新增块数据），而没有后续对块数据的追加、修改、删除操作，这就使得在 WAL 里只会有一条此块数据的 log entry。所以，我们在恢复时只要 seek 到这条 log entry 并读取就可以了，而不需要顺序读取整个 WAL。</p>\n<p><strong>也就是，Spark Streaming 基于 WAL 冷备进行恢复，需要的 recovery time 只是 seek 到并读一条 log entry 的时间，而不是读取整个 WAL 的时间</strong>，这个是个非常大的节省</p>\n<h4 id=\"2-2-1-1-配置\"><a href=\"#2-2-1-1-配置\" class=\"headerlink\" title=\"2.2.1.1. 配置\"></a>2.2.1.1. <strong>配置</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DC.tmp.jpg\" alt=\"img\"> </p>\n<h5 id=\"2-2-1-1-1-存放目录配置\"><a href=\"#2-2-1-1-1-存放目录配置\" class=\"headerlink\" title=\"2.2.1.1.1. 存放目录配置\"></a>2.2.1.1.1. <strong>存放目录配置</strong></h5><p>WAL 存放的目录：<code>{checkpointDir}/receivedData/{receiverId}</code></p>\n<p>{checkpointDir} ：在 <code>ssc.checkpoint(checkpointDir)</code>指定的​    </p>\n<p>{receiverId} ：是 Receiver 的 id</p>\n<p>文件名：不同的 rolling log 文件的命名规则是     <code>log-{startTime}-{stopTime}</code></p>\n<h5 id=\"2-2-1-1-2-rolling配置\"><a href=\"#2-2-1-1-2-rolling配置\" class=\"headerlink\" title=\"2.2.1.1.2. rolling配置\"></a>2.2.1.1.2. <strong>rolling配置</strong></h5><p>FileBasedWriteAheadLog 的实现把 log 写到一个文件里（一般是 HDFS 等可靠存储上的文件），然后每隔一段时间就关闭已有文件，产生一些新文件继续写，也就是 rolling 写的方式</p>\n<p>rolling 写的好处是单个文件不会太大，而且删除不用的旧数据特别方便</p>\n<p>这里 rolling 的间隔是由参数 <strong>spark.streaming.receiver.writeAheadLog.rollingIntervalSecs</strong>（默认 = 60 秒） 控制的</p>\n<h4 id=\"2-2-1-2-读写对象管理\"><a href=\"#2-2-1-2-读写对象管理\" class=\"headerlink\" title=\"2.2.1.2. 读写对象管理\"></a>2.2.1.2. <strong>读写对象管理</strong></h4><p>WAL将读写对象和读写实现分离，由FileBasedWriterAheadLog管理读写对象，LogWriter和LogReader根据不同输出源实现其读写操作</p>\n<p>class FileBasedWriteAheadLog:</p>\n<p>write(byteBuffer:ByteBuffer,time:Long):</p>\n<p>​    1.  先调用getCurrentWriter(),获取当前currentWriter.</p>\n<p>​    2. 如果log file 需要rolling成新的，则currentWriter也需要更新为新的currentWriter</p>\n<p>​    3. 调用writer.write(byteBuffer)进行写操作</p>\n<p>​    4. 保存成功后返回： </p>\n<p>​        path:保存路径</p>\n<p>​        offset:偏移量</p>\n<p>​        length:长度</p>\n<p>read(segment:WriteAheadRecordHandle):</p>\n<p>​    ByteBuffer {}:</p>\n<p>​    1. 直接调用reader.read(fileSegment)</p>\n<p>read实现：</p>\n<p>// 来自 FileBasedWriteAheadLogRandomReader</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\tsegment: <span class=\"type\">FileBasedWriteAheadLogSegment</span>): <span class=\"type\">ByteBuffer</span> = synchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  assertOpen()</span><br><span class=\"line\"></span><br><span class=\"line\">  \t<span class=\"comment\">// 【seek 到这条 log 所在的 offset】</span></span><br><span class=\"line\"></span><br><span class=\"line\"> \t instream.seek(segment.offset)</span><br><span class=\"line\"></span><br><span class=\"line\"> \t <span class=\"comment\">// 【读一下 length】</span></span><br><span class=\"line\"></span><br><span class=\"line\"> \t <span class=\"keyword\">val</span> nextLength = instream.readInt()</span><br><span class=\"line\"></span><br><span class=\"line\">  \t <span class=\"keyword\">val</span> buffer = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Byte</span>](nextLength)</span><br><span class=\"line\"></span><br><span class=\"line\">  \t <span class=\"comment\">// 【读一下具体的内容】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  \t instream.readFully(buffer)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 【以 ByteBuffer 的形式，返回具体的内容】</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">ByteBuffer</span>.wrap(buffer)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-3-重放\"><a href=\"#2-3-重放\" class=\"headerlink\" title=\"2.3. 重放\"></a>2.3. <strong>重放</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DD.tmp.jpg\" alt=\"img\"> </p>\n<p>如果上游支持重放，比如 Apache Kafka，那么就可以选择不用热备或者冷备来另外存储数据了，而是在失效时换一个 executor 进行数据重放即可。</p>\n<h3 id=\"2-3-1-基于Receiver\"><a href=\"#2-3-1-基于Receiver\" class=\"headerlink\" title=\"2.3.1. 基于Receiver\"></a>2.3.1. <strong>基于Receiver</strong></h3><p><strong>偏移量又kafka负责，有可能导致重复消费</strong></p>\n<p>这种是将 Kafka Consumer 的偏移管理交给 Kafka —— 将存在 ZooKeeper 里，失效后由 Kafka 去基于 offset 进行重放</p>\n<p>这样可能的问题是，Kafka 将同一个 offset 的数据，重放给两个 batch 实例 —— 从而只能保证 at least once 的语义</p>\n<h3 id=\"2-3-2-Direct方式\"><a href=\"#2-3-2-Direct方式\" class=\"headerlink\" title=\"2.3.2. Direct方式\"></a>2.3.2. <strong>Direct方式</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9DE.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>偏移量由spark自己管理，可以保证exactly-once</strong></p>\n<p>由 Spark Streaming 直接管理 offset —— 可以给定 offset 范围，直接去 Kafka 的硬盘上读数据，使用 Spark Streaming 自身的均衡来代替 Kafka 做的均衡</p>\n<p>这样可以保证，每个 offset 范围属于且只属于一个 batch，从而保证 exactly-once</p>\n<p>所以看 Direct 的方式，<strong>归根结底是由 Spark Streaming 框架来负责整个 offset 的侦测、batch 分配、实际读取数据</strong>；并且这些分 batch 的信息都是 checkpoint 到可靠存储（一般是 HDFS）了。这就没有用到 Kafka 使用 ZooKeeper 来均衡 consumer 和记录 offset 的功能，而是把 Kafka 直接当成一个底层的文件系统来使用了。</p>\n<h4 id=\"2-3-2-1-DirectKafkaInputDStream\"><a href=\"#2-3-2-1-DirectKafkaInputDStream\" class=\"headerlink\" title=\"2.3.2.1. DirectKafkaInputDStream\"></a>2.3.2.1. <strong>DirectKafkaInputDStream</strong></h4><p>负责侦测最新 offset，并将 offset 分配至唯一个 batch</p>\n<h4 id=\"2-3-2-2-KafkaRDD\"><a href=\"#2-3-2-2-KafkaRDD\" class=\"headerlink\" title=\"2.3.2.2. KafkaRDD\"></a>2.3.2.2. <strong>KafkaRDD</strong></h4><p>负责去读指定 offset 范围内的数据，并基于此数据进行计算</p>\n<h2 id=\"2-4-忽略\"><a href=\"#2-4-忽略\" class=\"headerlink\" title=\"2.4. 忽略\"></a>2.4. <strong>忽略</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsB9EF.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"2-4-1-粗粒度忽略\"><a href=\"#2-4-1-粗粒度忽略\" class=\"headerlink\" title=\"2.4.1. 粗粒度忽略\"></a>2.4.1. <strong>粗粒度忽略</strong></h3><p>在driver端捕获job抛出的异常，防止当前job失败，这样做会忽略掉整个batch里面的数据</p>\n<h3 id=\"2-4-2-细粒度忽略\"><a href=\"#2-4-2-细粒度忽略\" class=\"headerlink\" title=\"2.4.2. 细粒度忽略\"></a>2.4.2. <strong>细粒度忽略</strong></h3><p>细粒度忽略是在excutor端进行的，如果接收的block失效后，将失败的Block忽略掉，只发送没有问题的block块到driver</p>\n<p><strong>脑图制作参考</strong>：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p><strong>完整脑图链接地址</strong>：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"RangePartitioner实现算法reservoirSampleAndCount","url":"https://sustcoder.github.io/2018/12/12/2018-12-12-sparkCore-sourceCodeAnalysis_reservoirSampleAndCount/","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>reservoir的作用是：<strong>在不知道文件总行数的情况下，如何从文件中随机的抽取一行？</strong>即是说如果最后发现文字档共有N行，则每一行被抽取的概率均为1/N？</p>\n<p>我们可以：定义取出的行号为choice，第一次直接以第一行作为取出行 choice ，而后第二次以二分之一概率决定是否用第二行替换 choice ，第三次以三分之一的概率决定是否以第三行替换 choice ……，以此类推。由上面的分析我们可以得出结论，<strong>在取第n个数据的时候，我们生成一个0到1的随机数p，如果p小于1/n，保留第n个数。大于1/n，继续保留前面的数。直到数据流结束，返回此数，算法结束。</strong></p>\n<p>这个问题的扩展就是：如何从未知或者很大样本空间随机地取k个数？亦即是说，如果档案共有N ≥ k行，则每一行被抽取的概率为k/N。</p>\n<p>　　根据上面（随机取出一元素）的分析，我们可以把上面的1/n变为k/n即可。思路为：<strong>在取第n个数据的时候，我们生成一个0到1的随机数p，如果p小于k/n，替换池中任意一个为第n个数。大于k/n，继续保留前面的数。直到数据流结束，返回此k个数。但是为了保证计算机计算分数额准确性，一般是生成一个0到n的随机数，跟k相比，道理是一样的。</strong></p>\n<h2 id=\"伪代码\"><a href=\"#伪代码\" class=\"headerlink\" title=\"伪代码\"></a><strong>伪代码</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">从S中抽取首k项放入「水塘」中</span><br><span class=\"line\">对于每一个S[j]项（j ≥ k）：</span><br><span class=\"line\">   随机产生一个范围0到j的整数r</span><br><span class=\"line\">   若 r &lt; k 则把水塘中的第r项换成S[j]项</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">  S has items to sample, R will contain the result</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"type\">ReservoirSample</span>(<span class=\"type\">S</span>[<span class=\"number\">1.</span>.n], <span class=\"type\">R</span>[<span class=\"number\">1.</span>.k])</span><br><span class=\"line\">  <span class=\"comment\">// fill the reservoir array</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> i = <span class=\"number\">1</span> to k</span><br><span class=\"line\">      <span class=\"type\">R</span>[i] := <span class=\"type\">S</span>[i]</span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"comment\">// replace elements with gradually decreasing probability</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> i = k+<span class=\"number\">1</span> to n</span><br><span class=\"line\">    j := random(<span class=\"number\">1</span>, i)   <span class=\"comment\">// important: inclusive range</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> j &lt;= k</span><br><span class=\"line\">        <span class=\"type\">R</span>[j] := <span class=\"type\">S</span>[i]</span><br></pre></td></tr></table></figure>\n<h2 id=\"实现概述\"><a href=\"#实现概述\" class=\"headerlink\" title=\"实现概述\"></a>实现概述</h2><ol>\n<li>获取到需要抽样RDD分区的样本大小k和分区的所有KEY数组input</li>\n<li>初始化抽样结果集reservoir为分区前K个KEY值</li>\n<li>如果分区的总数小于预计样本大小k,则将当前分区的所有数据作为样本数据，否则到第四步</li>\n<li>遍历分区里所有Key组成的数组input</li>\n<li>生成随机需要替换input数组的下标，如果下标小于K则替换</li>\n<li>返回抽取的key值数组和当前分区的总数据量： (reservoir, l)</li>\n</ol>\n<h2 id=\"实现源码\"><a href=\"#实现源码\" class=\"headerlink\" title=\"实现源码\"></a>实现源码</h2><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">   * Reservoir sampling implementation that also returns the input size.</span></span><br><span class=\"line\"><span class=\"comment\">   *</span></span><br><span class=\"line\"><span class=\"comment\">   * @param input:RDD的分区里面的key组成的Iterator</span></span><br><span class=\"line\"><span class=\"comment\">   * @param k :抽样大小=</span></span><br><span class=\"line\"><span class=\"comment\">   \t\tval sampleSize = math.min(20.0 * partitions, 1e6)</span></span><br><span class=\"line\"><span class=\"comment\">   \t\tval k=math.ceil(3.0 * sampleSize / rdd.partitions.length).toInt</span></span><br><span class=\"line\"><span class=\"comment\">   * @param seed random seed:选取随机数的种子</span></span><br><span class=\"line\"><span class=\"comment\">   * @return (samples, input size)</span></span><br><span class=\"line\"><span class=\"comment\">   */</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reservoirSampleAndCount</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">      input: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">      k: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      seed: <span class=\"type\">Long</span> = <span class=\"type\">Random</span>.nextLong())</span><br><span class=\"line\">    : (<span class=\"type\">Array</span>[<span class=\"type\">T</span>], <span class=\"type\">Long</span>) = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> reservoir = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">T</span>](k)</span><br><span class=\"line\">    <span class=\"comment\">// Put the first k elements in the reservoir.</span></span><br><span class=\"line\">    <span class=\"comment\">// 初始化水塘数据为input的钱K个数，即：reservoir数组中放了RDD分区的前K个key值</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> item = input.next()</span><br><span class=\"line\">      reservoir(i) = item</span><br><span class=\"line\">      i += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// If we have consumed all the elements, return them. Otherwise do the replacement.</span></span><br><span class=\"line\">    <span class=\"comment\">// 如果当前的RDD总数小于预设值的采样量则全部作为采样数据并结束采样</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (i &lt; k) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// If input size &lt; k, trim the array to return only an array of input size.</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> trimReservoir = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">T</span>](i)</span><br><span class=\"line\">      <span class=\"type\">System</span>.arraycopy(reservoir, <span class=\"number\">0</span>, trimReservoir, <span class=\"number\">0</span>, i)</span><br><span class=\"line\">      (trimReservoir, i)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// If input size &gt; k, continue the sampling process.</span></span><br><span class=\"line\">      <span class=\"keyword\">var</span> l = i.toLong</span><br><span class=\"line\">      <span class=\"keyword\">val</span> rand = <span class=\"keyword\">new</span> <span class=\"type\">XORShiftRandom</span>(seed)</span><br><span class=\"line\">      <span class=\"comment\">// 遍历所有的key</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (input.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> item = input.next()</span><br><span class=\"line\">        l += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\">// There are k elements in the reservoir, and the l-th element has been</span></span><br><span class=\"line\">        <span class=\"comment\">// consumed. It should be chosen with probability k/l. The expression</span></span><br><span class=\"line\">        <span class=\"comment\">// below is a random long chosen uniformly from [0,l)</span></span><br><span class=\"line\">        <span class=\"comment\">// 计算出需要替换的数组下标</span></span><br><span class=\"line\">        <span class=\"comment\">// 选取第n个数的概率是：n/l; 如果随机替换数组值的概率是p=rand.nextDouble，</span></span><br><span class=\"line\">        <span class=\"comment\">// 则如果p&lt;k/l;则替换池中任意一个数，即： p*l &lt; k 则进行替换，用p*l作为随机替换的下标</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> replacementIndex = (rand.nextDouble() * l).toLong</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (replacementIndex &lt; k) &#123;</span><br><span class=\"line\">          <span class=\"comment\">// 替换reservoir[随机抽取的下标]的值为input[l]的值item</span></span><br><span class=\"line\">          reservoir(replacementIndex.toInt) = item</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      (reservoir, l)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://www.iteblog.com/archives/1525.html\" target=\"_blank\" rel=\"noopener\">https://www.iteblog.com/archives/1525.html</a></p>\n","categories":["spark"],"tags":["spark","算法"]},{"title":"sparkStreaming源码解析之数据的产生与导入","url":"https://sustcoder.github.io/2018/12/09/2018-12-09-sparkStreaming-sourceCodeAnalysis_DataInputOutput/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_DataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis_faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps8B7D.tmp.jpg\" alt=\"img\"></p>\n<p>数据的产生与导入主要分为以下五个部分</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps231.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"1-分发receivers\"><a href=\"#1-分发receivers\" class=\"headerlink\" title=\"1. 分发receivers\"></a>1. 分发receivers</h1><p>由 Receiver 的总指挥 ReceiverTracker 分发多个 job（每个 job 有 1 个 task），到多个 executor 上分别启动 ReceiverSupervisor 实例</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps232.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>从ReceiverInputDStreams中获取Receivers，并把他们发送到所有的worker nodes:</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> endpoint:<span class=\"type\">RpcEndpointRef</span>=</span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">**launchReceivers**</span></span>()&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// DStreamGraph的属性inputStreams</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> receivers=inputStreams.map&#123;nis=&gt;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">val</span> rcvr=nis.getReceiver()</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// rcvr是对kafka,socket等接受数据的定义</span></span><br><span class=\"line\">\t\t\trcvr</span><br><span class=\"line\">\t\t&#125;\t</span><br><span class=\"line\">\t\t<span class=\"comment\">// 发送到worker</span></span><br><span class=\"line\">\t\t endpoint.send(<span class=\"type\">StartAllReceivers</span>(receivers))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-1-选择receivers位置\"><a href=\"#1-1-选择receivers位置\" class=\"headerlink\" title=\"1.1. 选择receivers位置\"></a>1.1. <strong>选择receivers位置</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps234.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>目的地选择分两种情况：初始化选择和失败重启选择</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 分发目的地的计算</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> schedulingPolicy=</span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSchedulingPolicy</span>() </span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 首次启动</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span>(receivers) =&gt;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-1-首次启动\"><a href=\"#1-1-1-首次启动\" class=\"headerlink\" title=\"1.1.1. 首次启动\"></a>1.1.1. <strong>首次启动</strong></h3><p><strong>1. 选择最优executors位置</strong></p>\n<p><strong>2. 遍历构造最终分发的excutor</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> schedulingPolicy=</span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSchedulingPolicy</span>() </span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 首次启动</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span>(receivers) =&gt;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 1. 选择最优executors位置</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">val</span> locations=</span><br><span class=\"line\">\t\t\t\tschedulingPolicy.scheduleReceivers(</span><br><span class=\"line\">\t\t\t\t\treceivers,getExecutors</span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 2. 遍历构造最终分发的excutor</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">for</span>(receiver&lt;- receivers)&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> executors = scheduledLocations(</span><br><span class=\"line\">\t\t\t\t\treceiver.streamId)</span><br><span class=\"line\">\t\t\t\tstartReceiver(receiver, executors)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-2-失败重启\"><a href=\"#1-1-2-失败重启\" class=\"headerlink\" title=\"1.1.2. 失败重启\"></a>1.1.2. <strong>失败重启</strong></h3><p><strong>1.获取之前的executors</strong></p>\n<p><strong>2. 计算新的excutor位置</strong></p>\n<p>​    <strong>2.1 之前excutors可用，则使用之前的</strong></p>\n<p>​    <strong>2.2 之前的不可用则重新计算位置</strong></p>\n<p><strong>3. 发送给worker重启receiver</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> schedulingPolicy=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSchedulingPolicy</span>() </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 首次启动</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span>(receivers) =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1.获取之前的executors</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> oldScheduledExecutors =getStoredScheduledExecutors(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.streamId</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 计算新的excutor位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t <span class=\"keyword\">val</span> scheduledLocations = <span class=\"keyword\">if</span> (oldScheduledExecutors.nonEmpty) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// 2.1 之前excutors可用，则使用之前的</span></span><br><span class=\"line\"></span><br><span class=\"line\">            oldScheduledExecutors</span><br><span class=\"line\"></span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 2.2 之前的不可用则重新计算位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tschedulingPolicy.rescheduleReceiver(）</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 发送给worker重启receiver</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t   startReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver, scheduledLocations)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-3-选择策略\"><a href=\"#1-1-3-选择策略\" class=\"headerlink\" title=\"1.1.3. 选择策略\"></a>1.1.3. <strong>选择策略</strong></h3><p><strong>策略选择由ReceiverSchedulingPolicy实现，默认策略是轮训（round-robin），在1.5版本之前是使用依赖 Spark Core 的 TaskScheduler 进行通用分发，</strong></p>\n<p><strong>在1.5之前存在executor分发不均衡问题导致Job执行失败：</strong></p>\n<p>如果某个 Task 失败超过 spark.task.maxFailures(默认=4) 次的话，整个 Job 就会失败。这个在长时运行的 Spark Streaming 程序里，Executor 多失效几次就有可能导致 Task 失败达到上限次数了，如果某个 Task 失效一下，Spark Core 的 TaskScheduler 会将其重新部署到另一个 executor 上去重跑。但这里的问题在于，负责重跑的 executor 可能是在下发重跑的那一刻是正在执行 Task 数较少的，但不一定能够将 Receiver 分布的最均衡的。</p>\n<p>策略代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> scheduledLocations =<span class=\"type\">ReceiverSchedulingPolicy</span>.scheduleReceivers(receivers,xecutors)</span><br><span class=\"line\"><span class=\"keyword\">val</span> scheduledLocations =<span class=\"type\">ReceiverSchedulingPolicy</span>.rescheduleReceiver(receiver, ...)</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-2-构造Receiver-RDD\"><a href=\"#1-2-构造Receiver-RDD\" class=\"headerlink\" title=\"1.2. 构造Receiver RDD\"></a>1.2. <strong>构造Receiver RDD</strong></h2><p><strong>将receiver列表转换为RDD</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstartReceiver(receiver, executors)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\">\t\t\treceiver: <span class=\"type\">Receiver</span>[_],</span><br><span class=\"line\"></span><br><span class=\"line\">        \tscheduledLocations: <span class=\"type\">Seq</span>[<span class=\"type\">TaskLocation</span>])&#123;\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> receiverRDD: <span class=\"type\">RDD</span>[<span class=\"type\">Receiver</span>] =</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (scheduledLocations.isEmpty) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          \t**ssc.sc.makeRDD(<span class=\"type\">Seq</span>(receiver), <span class=\"number\">1</span>)**</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          <span class=\"keyword\">val</span> preferredLocations = scheduledLocations.map(_.toString).distinct</span><br><span class=\"line\"></span><br><span class=\"line\">          ssc.sc.makeRDD(<span class=\"type\">Seq</span>(receiver -&gt; preferredLocations))</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      receiverRDD.setName(<span class=\"string\">s\" <span class=\"subst\">$receiverId</span>\"</span>)\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-3-定义启动reciever的Func\"><a href=\"#1-3-定义启动reciever的Func\" class=\"headerlink\" title=\"1.3. 定义启动reciever的Func\"></a>1.3. <strong>定义启动reciever的Func</strong></h2><p><strong>将每个receiver,spark环境变量，hadoop配置文件，检查点路径等信息传送给excutor的接收对象ReceiverSupervisorImpl</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">  ...</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> startReceiverFunc:</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"type\">Iterator</span>[<span class=\"type\">Receiver</span>[_]]=&gt;<span class=\"type\">Unit</span>=</span><br><span class=\"line\"></span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t(iterator:<span class=\"type\">Iterator</span>)=&gt;&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> receiver=iterator.next()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> supervisor=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverSupervisoImpl</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">SparkEnv</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">HadoopConf</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcheckpointDir,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.start(),</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.awaitTermination()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  ...</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-4-分发RDD-Receiver-和Func到具体的excutor\"><a href=\"#1-4-分发RDD-Receiver-和Func到具体的excutor\" class=\"headerlink\" title=\"1.4. 分发RDD(Receiver)和Func到具体的excutor\"></a>1.4. <strong>分发RDD(Receiver)和Func到具体的excutor</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps247.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>将前两部定义的rdd和fun从driver提交到excutor</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> future=ssc.sparkContext.submitJob(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiverRDD,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstartReceverFunc,</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t  ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-1-启动Executor\"><a href=\"#1-4-1-启动Executor\" class=\"headerlink\" title=\"1.4.1. 启动Executor\"></a>1.4.1. <strong>启动Executor</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps248.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>Executor的启动在Receiver类中定义，在ReceiverSupervisor类中调用，在Receiver的子类中实现</strong></p>\n<p>excutor中共需要启动两个线程</p>\n<p>​    -1. 启动Receiver接收数据</p>\n<p>​    - 2. 启动pushingThread定时推送数据到driver</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisor</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\treceiver: <span class=\"type\">Receiver</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tconf: sparkConf</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">    startReceiver()</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-4-1-1-启动Receiver\"><a href=\"#1-4-1-1-启动Receiver\" class=\"headerlink\" title=\"1.4.1.1. 启动Receiver\"></a>1.4.1.1. <strong>启动Receiver</strong></h4><p><strong>启动Receiver，开始接收数据</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisor</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\treceiver: <span class=\"type\">Receiver</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tconf: sparkConf</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">    startReceiver()</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 1. 启动Receiver，开始接收数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiverState=<span class=\"type\">Started</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiver.onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-4-1-2-启动blockPushingThread\"><a href=\"#1-4-1-2-启动blockPushingThread\" class=\"headerlink\" title=\"1.4.1.2. 启动blockPushingThread\"></a>1.4.1.2. <strong>启动blockPushingThread</strong></h4><p><strong>启动pushTread，定时推送信息到driver</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisor</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\treceiver: <span class=\"type\">Receiver</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tconf: sparkConf</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">    startReceiver()</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 1. 启动Receiver，开始接收数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiverState=<span class=\"type\">Started</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceiver.onStart()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 2. 启动pushTread，定时推送信息到driver</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    registeredBlockGenerators.asScala.foreach &#123; \t\t_.start() </span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// _.start() 的实现</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockIntervalTimer.start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockPushingThread.start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-5-监控excutor\"><a href=\"#1-5-监控excutor\" class=\"headerlink\" title=\"1.5. 监控excutor\"></a>1.5. <strong>监控excutor</strong></h2><p>启动 Receiver 实例，并一直 block 住当前线程</p>\n<p>在1.5版本之前，一个job包含多个task，一个task失败次数失败超过4次后，整个Job都会失败，<strong>1.5版本之后一个job只包含一个task,并且添加了可重试机制</strong>，大大增加了job的活性</p>\n<p> Spark Core 的 Task 下发时只会参考并大部分时候尊重 Spark Streaming 设置的 preferredLocation 目的地信息，还是有一定可能该分发 Receiver 的 Job 并没有在我们想要调度的 executor 上运行。此时，在第 1 次执行 Task 时，会首先向 ReceiverTracker 发送 RegisterReceiver 消息，只有得到肯定的答复时，才真正启动 Receiver，否则就继续做一个空操作，导致本 Job 的状态是成功执行已完成。当然，ReceiverTracker 也会另外调起一个 Job，来继续尝试 Receiver 分发……如此直到成功为止。</p>\n<p>一个 Receiver 的分发 Job 是有可能没有完成分发 Receiver 的目的的，所以 ReceiverTracker 会继续再起一个 Job 来尝试 Receiver 分发。这个机制保证了，如果一次 Receiver 如果没有抵达预先计算好的 executor，就有机会再次进行分发，从而实现在 Spark Streaming 层面对 Receiver 所在位置更好的控制。</p>\n<p> 对 Receiver 的监控重启机制</p>\n<p>上面分析了每个 Receiver 都有专门的 Job 来保证分发后，我们发现这样一来，Receiver 的失效重启就不受 spark.task.maxFailures(默认=4) 次的限制了。</p>\n<p>因为现在的 Receiver 重试不是在 Task 级别，而是在 Job 级别；并且 Receiver 失效后并不会导致前一次 Job 失败，而是前一次 Job 成功、并新起一个 Job 再次进行分发。这样一来，不管 Spark Streaming 运行多长时间，Receiver 总是保持活性的，不会随着 executor 的丢失而导致 Receiver 死去。</p>\n<p>// todo 阻塞，知道executor返回发送结果</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startReceiver</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\"> \t\tfuture.onComplete &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">Success</span>(_)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">Failure</span>())=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tonReceiverJobFinish(receiverId)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;&#125;(<span class=\"type\">ThreadUtils</span>.sameThread)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"2-数据接收\"><a href=\"#2-数据接收\" class=\"headerlink\" title=\"2. 数据接收\"></a>2. <strong>数据接收</strong></h1><p>每个 ReceiverSupervisor 启动后将马上生成一个用户提供的 Receiver 实现的实例 —— 该 Receiver 实现可以持续产生或者持续接收系统外数据，比如 TwitterReceiver 可以实时爬取 twitter 数据 —— 并在 Receiver 实例生成后调用 Receiver.onStart()。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps25A.tmp.jpg\" alt=\"img\"> </p>\n<p> <strong>数据的接收由Executor端的Receiver实现，启动和停止需要子类实现，存储基类实现，供子类调用</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Receiver</span>[<span class=\"type\">T</span>](<span class=\"params\">val storageLevel: <span class=\"type\">StorageLevel</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 启动和停止需要子类实现</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStop</span></span>()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储单条小数据】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataItem: <span class=\"type\">T</span>) &#123;...&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储数组形式的块数据】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataBuffer: <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">T</span>]) &#123;...&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储 iterator 形式的块数据】</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataIterator: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) &#123;...&#125;   </span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 【存储 ByteBuffer 形式的块数据】 </span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(bytes: <span class=\"type\">ByteBuffer</span>) &#123;...&#125;         </span><br><span class=\"line\"></span><br><span class=\"line\">   ...</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps25B.tmp.jpg\" alt=\"img\"> </p>\n<p>通过kafka去接收数据，</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span> <span class=\"title\">**extends</span> <span class=\"title\">Receiver**</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\t_ssc : <span class=\"type\">StreamingContext</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tkafkaParams : <span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\ttopics : <span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tuseReliableReceiver : <span class=\"type\">Boolean</span> </span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tstorageLevel : <span class=\"type\">StorageLevel</span> </span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-1-获取kafka参数\"><a href=\"#2-3-1-获取kafka参数\" class=\"headerlink\" title=\"2.3.1. 获取kafka参数\"></a>2.3.1. 获取kafka参数</h3><p>拼接kafka consumer所需参数</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t **<span class=\"comment\">// 1. 获取kafka参数**\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-2-链接到kafka\"><a href=\"#2-3-2-链接到kafka\" class=\"headerlink\" title=\"2.3.2. 链接到kafka\"></a>2.3.2. <strong>链接到kafka</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// kafka链接器</span></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> consumerConnector:<span class=\"type\">ConsumerConnector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 获取kafka参数\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 链接到kafka</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> consumerConf=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ConsumerConfig</span>(props)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconsumerConnector=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Consumer</span>.create(consumerConf)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-3-监听所有topic\"><a href=\"#2-3-3-监听所有topic\" class=\"headerlink\" title=\"2.3.3. 监听所有topic\"></a>2.3.3. 监听所有topic</h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// kafka链接器</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> consumerConnector:<span class=\"type\">ConsumerConnector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 获取kafka参数\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 链接到kafka</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> consumerConf=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ConsumerConfig</span>(props)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconsumerConnector=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Consumer</span>.create(consumerConf)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 3. 监听所有topic</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> topicMessageStreams=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tconsumerConnector.createMessage()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> executorPool=<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tnewDaemonFixedTreadPool(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttopics.values.sum,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"kafkaMessageHandler\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttopicMessageStreams.values.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreams=&gt;streams.foreach&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstream=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\texecutorPool.submit(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">MessageHandler</span>(stream)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-4-异步保存数据\"><a href=\"#2-3-4-异步保存数据\" class=\"headerlink\" title=\"2.3.4. 异步保存数据\"></a>2.3.4. 异步保存数据</h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KafkaInputDStream</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// kafka链接器</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> consumerConnector:<span class=\"type\">ConsumerConnector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStart</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 获取kafka参数\t </span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"keyword\">val</span> props=<span class=\"keyword\">new</span> <span class=\"type\">Properties</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkafkaParams.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tp=&gt;props.put(p._1,p._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 链接到kafka</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> consumerConf=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ConsumerConfig</span>(props)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconsumerConnector=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Consumer</span>.create(consumerConf)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 3. 监听所有topic</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> topicMessageStreams=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tconsumerConnector.createMessage()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> executorPool=<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tnewDaemonFixedTreadPool(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttopics.values.sum,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"kafkaMessageHandler\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttopicMessageStreams.values.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreams=&gt;streams.foreach&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstream=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\texecutorPool.submit(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">MessageHandler</span>(stream)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 4. 异步保存数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MessageHandler</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\t stream:<span class=\"type\">KafkaStream</span>[<span class=\"type\">K</span>,<span class=\"type\">V</span>]</span>) <span class=\"keyword\">extends</span> <span class=\"title\">Runable</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t \t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">val</span> streamIterator=stream.iterator()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span>(streamIterator.hasNext())&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> msgAndMetadata=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstreamIterator.next()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**store(**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t**msgAndMetadata.key,**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t**msgAndMetadata.message**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**)**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>自定义的Receiver只需要继承Receiver类，并实现onStart方法里新拉起数据接收线程，并在接收到数据时 store() 到 Spark Streamimg 框架就可以了。</p>\n<h1 id=\"3-数据转存\"><a href=\"#3-数据转存\" class=\"headerlink\" title=\"3. 数据转存\"></a>3. <strong>数据转存</strong></h1><p>Receiver 在 onStart() 启动后，就将持续不断地接收外界数据，并持续交给 ReceiverSupervisor 进行数据转储</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps260.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"3-1-存储格式\"><a href=\"#3-1-存储格式\" class=\"headerlink\" title=\"3.1. 存储格式\"></a>3.1. <strong>存储格式</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps261.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>Receiver在调用store方法后，根据不同的入参会调用ReceiverSupervisor的不同方法。ReceiverSupervisor的方法由ReceiverSupervisorImpl实现</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Receiver</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> supervisor:<span class=\"type\">ReceiverSupervisor</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 1.单条数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">strore</span></span>(dataItem: <span class=\"type\">T</span> )&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pushSigle(dataItem)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 2. byte数组</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(bytes : <span class=\"type\">ByteBuffer</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pushBytes(bytes,<span class=\"type\">None</span>,<span class=\"type\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 3. 迭代器格式</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataIterator : <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>])&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pusthIteratro(dataIterator)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 4. ByteBuffer格式</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">store</span></span>(dataBuffer:<span class=\"type\">ArrayBuffer</span>[<span class=\"type\">T</span>])&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsupervisor.pushArrayBuffer(dataBuffer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-1-单条数据\"><a href=\"#3-1-1-单条数据\" class=\"headerlink\" title=\"3.1.1. 单条数据\"></a>3.1.1. <strong>单条数据</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps272.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>调用ReceiverSupervisorImpl的pushSigle方法保存单条数据</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> defaultBlockGenerator=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">BlockGenerator</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockGeneratorListener, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\tenv.conf</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushSinge</span></span>(data:<span class=\"type\">Any</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tdefaultBlockGenerator.addData(data)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-1-控制batch大小\"><a href=\"#3-1-1-1-控制batch大小\" class=\"headerlink\" title=\"3.1.1.1. 控制batch大小\"></a>3.1.1.1. <strong>控制batch大小</strong></h4><p><strong>先检查接收数据的频率，控制住频率就控制了每个batch需要处理的最大数据量</strong></p>\n<p>就是在加入 currentBuffer 数组时会先由 rateLimiter 检查一下速率，是否加入的频率已经太高。如果太高的话，就需要 block 住，等到下一秒再开始添加。这里的最高频率是由 spark.streaming.receiver.maxRate (default = Long.MaxValue) 控制的，是单个 Receiver 每秒钟允许添加的条数。控制了这个速率，就控制了整个 Spark Streaming 系统每个 batch 需要处理的最大数据量。</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RateLimiter</span>(<span class=\"params\">conf:<span class=\"type\">SparkConf</span></span>)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> maxRateLimit=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tconf.getLong(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"string\">\"spark.streaming.receiver.maxRate\"</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">Long</span>.<span class=\"type\">MaxValue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> rateLimiter=<span class=\"type\">GuavaRateLimiter</span>.create(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tmaxRateLimit.toDouble</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">waitToPush</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\trateLimiter.acquire()</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-2-添加数据到arrayBuffer\"><a href=\"#3-1-1-2-添加数据到arrayBuffer\" class=\"headerlink\" title=\"3.1.1.2. 添加数据到arrayBuffer\"></a>3.1.1.2. <strong>添加数据到arrayBuffer</strong></h4><p><strong>如果频率正常，则把数据添加到数组中，否则抛异常</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> currentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 添加数据到currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(state==<span class=\"type\">Active</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer+=data</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">SparkException</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"string\">\"connot add data ...\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-3-定时放入blocakQueue\"><a href=\"#3-1-1-3-定时放入blocakQueue\" class=\"headerlink\" title=\"3.1.1.3. 定时放入blocakQueue\"></a>3.1.1.3. <strong>定时放入blocakQueue</strong></h4><p>3.1 清空currentBuffer</p>\n<p>3.2 将block块放入blocakQueue</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> currentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 定时器：定时更新currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blockIntervalTimer=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tclock,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockIntervalMs,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tupdateCurrentBuffer,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"string\">\"BlockGenerator\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 保存block的数组大小，默认是10</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> queueSize=conf.getInt(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"string\">\"spark.streaming.blockQueueSize\"</span>,<span class=\"number\">10</span>)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blocksForPushing=</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">ArrayBlockingQueue</span>[<span class=\"type\">Block</span>](queueSize)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 添加数据到currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer+=data</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateCurrentBuffer</span></span>(timer:<span class=\"type\">Long</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">var</span> newBlock:<span class=\"type\">Block</span>=<span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3.1 清空currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> newBlockBuffer=currentBuffer</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 2 将block块放入blocakQueue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnewBlock=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">Block</span>(id,newBlockBuffer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblocksForPushing.put(newBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-1-4-保存并推送blocks\"><a href=\"#3-1-1-4-保存并推送blocks\" class=\"headerlink\" title=\"3.1.1.4. 保存并推送blocks\"></a>3.1.1.4. <strong>保存并推送blocks</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps286.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>在初始化BlockGenerator时，启动一个线程去持续的执行pushBlocks方法。如果还没有生成blocks，则阻塞调用queue.poll去获取数据，如果已经存在blocks块，则直接queue.take(10)</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockGenerator</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> currentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 定时器：定时更新currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blockIntervalTimer=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tclock,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockIntervalMs,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tupdateCurrentBuffer,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"string\">\"BlockGenerator\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 保存block的数组大小，默认是10</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> queueSize=conf.getInt(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"string\">\"spark.streaming.blockQueueSize\"</span>,<span class=\"number\">10</span>)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blocksForPushing=</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">ArrayBlockingQueue</span>[<span class=\"type\">Block</span>](queueSize)</span><br><span class=\"line\"></span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 推送block块</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> blockPushingThread=<span class=\"keyword\">new</span> <span class=\"type\">Thread</span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>()&#123;keepPushingBlocks()&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addData</span></span>(data:<span class=\"type\">Any</span>)=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 检查接收频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\twaitToPush()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 添加数据到currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer+=data</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateCurrentBuffer</span></span>(timer:<span class=\"type\">Long</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">var</span> newBlock:<span class=\"type\">Block</span>=<span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3.1 清空currentBuffer</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> newBlockBuffer=currentBuffer</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcurrentBuffer=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Any</span>]</span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 2 将block块放入blocakQueue</span></span><br><span class=\"line\">\t\t\t\tnewBlock=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">Block</span>(id,newBlockBuffer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblocksForPushing.put(newBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">keepPushingBlocks</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// **4.1 当block正在产时，等待其生成**</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span>(areBlocksBeingGenerated)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Option</span>(blocksForPushing.poll(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\twaitingTime</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t) <span class=\"keyword\">match</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">Some</span>(block)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\tpushBLock(block)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 4.2 block块已经生成</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span>(!blocksForPushing.isEmpty)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> block=blocksForPushing.take()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tpushBlock(block)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-1-1-4-1-保存\"><a href=\"#3-1-1-4-1-保存\" class=\"headerlink\" title=\"3.1.1.4.1. 保存\"></a>3.1.1.4.1. <strong>保存</strong></h5><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushAndReportBlock</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> blockStoreResult =</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t**receivedBlockHandler.storeBlock**(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlock</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-1-1-4-2-推送\"><a href=\"#3-1-1-4-2-推送\" class=\"headerlink\" title=\"3.1.1.4.2. 推送\"></a>3.1.1.4.2. <strong>推送</strong></h5><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushAndReportBlock</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> blockStoreResult =</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceivedBlockHandler.**storeBlock**(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlock</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t    <span class=\"keyword\">val</span> blockInfo = <span class=\"type\">ReceivedBlockInfo</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tmetadataOption, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockStoreResult</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    trackerEndpoint.askSync[<span class=\"type\">Boolean</span>](<span class=\"type\">AddBlock</span>(blockInfo))</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-2-bytes格式数据\"><a href=\"#3-1-2-bytes格式数据\" class=\"headerlink\" title=\"3.1.2. bytes格式数据\"></a>3.1.2. <strong>bytes格式数据</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>   <span class=\"title\">ReceiverSupervisorImpl</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushBytes</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">      bytes: <span class=\"type\">ByteBuffer</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">      metadataOption: <span class=\"type\">Option</span>[<span class=\"type\">Any</span>],</span><br><span class=\"line\"></span><br><span class=\"line\">      blockIdOption: <span class=\"type\">Option</span>[<span class=\"type\">StreamBlockId</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    pushAndReportBlock(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"type\">ByteBufferBlock</span>(bytes), </span><br><span class=\"line\"></span><br><span class=\"line\">\t\tmetadataOption,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tblockIdOption</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-3-iterator格式数据\"><a href=\"#3-1-3-iterator格式数据\" class=\"headerlink\" title=\"3.1.3. iterator格式数据\"></a>3.1.3. <strong>iterator格式数据</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>   <span class=\"title\">ReceiverSupervisorImpl</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushIterator</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">      iterator: <span class=\"type\">Iterator</span>[_],</span><br><span class=\"line\"></span><br><span class=\"line\">      metadataOption: <span class=\"type\">Option</span>[<span class=\"type\">Any</span>],</span><br><span class=\"line\"></span><br><span class=\"line\">      blockIdOption: <span class=\"type\">Option</span>[<span class=\"type\">StreamBlockId</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    pushAndReportBlock(<span class=\"type\">IteratorBlock</span>(iterator), metadataOption, blockIdOption)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-4-ByteBuffer格式数据\"><a href=\"#3-1-4-ByteBuffer格式数据\" class=\"headerlink\" title=\"3.1.4. ByteBuffer格式数据\"></a>3.1.4. <strong>ByteBuffer格式数据</strong></h3> <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>   <span class=\"title\">ReceiverSupervisorImpl</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushArrayBuffer</span></span>(</span><br><span class=\"line\"></span><br><span class=\"line\">      arrayBuffer: <span class=\"type\">ArrayBuffer</span>[_],</span><br><span class=\"line\"></span><br><span class=\"line\">      metadataOption: <span class=\"type\">Option</span>[<span class=\"type\">Any</span>],</span><br><span class=\"line\"></span><br><span class=\"line\">      blockIdOption: <span class=\"type\">Option</span>[<span class=\"type\">StreamBlockId</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    pushAndReportBlock(<span class=\"type\">ArrayBufferBlock</span>(arrayBuffer), metadataOption, blockIdOption)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-2-存储方式\"><a href=\"#3-2-存储方式\" class=\"headerlink\" title=\"3.2. 存储方式\"></a>3.2. <strong>存储方式</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps297.tmp.jpg\" alt=\"img\"> </p>\n<p>ReceivedBlockHandler 有两个具体的存储策略的实现：</p>\n<p>(a) BlockManagerBasedBlockHandler，是直接存到 executor 的内存或硬盘</p>\n<p>(b) WriteAheadLogBasedBlockHandler，是先写 WAL，再存储到 executor 的内存或硬盘</p>\n<h3 id=\"3-2-1-BlockManager\"><a href=\"#3-2-1-BlockManager\" class=\"headerlink\" title=\"3.2.1. BlockManager\"></a>3.2.1. <strong>BlockManager</strong></h3><p><strong>将数据存储交给blockManager进行管理，调用blockmanager的putIterator方法，由其实现在不同excutor上的复制以及缓存策略。</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BlockManagerBasedBlockHandler</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tblockManager:<span class=\"type\">BlockManager</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tstorageLevel:<span class=\"type\">StorageLevel</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)<span class=\"keyword\">extends</span> <span class=\"title\">ReceivedBlockHandler</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">storeBlock</span></span>(blockId,block)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">var</span> numRecords:<span class=\"type\">Option</span>[<span class=\"type\">Long</span>]=<span class=\"type\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> putSucceeded:<span class=\"type\">Boolean</span> = block <span class=\"keyword\">match</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">ArrayBufferBlock</span>(arrayBuffer)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords=<span class=\"type\">Some</span>(arrayBuffer.size)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockManager.putIterator(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tarrayBuffer.iterator,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstorageLevel,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\ttellMaster=<span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">IteratorBlock</span>(iterator)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> countIterator=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">CountingIterator</span>(iterator)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">val</span> putResult=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t**blockManager.putIterato**r(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tarrayBuffer.iterator,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tstorageLevel,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\ttellMaster=<span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords=countIterator.count</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tputResult</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> <span class=\"type\">ByteBufferBlock</span>(byteBuffer)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockManager.putBytes(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ChunkedBytedBuffer</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tbyteBuffer.duplicate(),</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tstorageLevel,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\ttellMaster=<span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 报告给driver的信息：id和num</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">BlockManagerBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tnumRecords</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ChunkedBytedBuffer: 将byte数组分片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// byteBuffer.duplicate(): 复制</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-2-WAL\"><a href=\"#3-2-2-WAL\" class=\"headerlink\" title=\"3.2.2. WAL\"></a>3.2.2. <strong>WAL</strong></h3><p>WriteAheadLogBasedBlockHandler 的实现则是同时写到可靠存储的 WAL 中和 executor 的 BlockManager 中；在<strong>两者都写完成后，再上报块数据的 meta 信息</strong>。</p>\n<p><strong>BlockManager 中的块数据是计算时首选使用的，只有在 executor 失效时，才去 WAL 中读取写入过的数据</strong>。</p>\n<p>同其它系统的 WAL 一样，数据是完全顺序地写入 WAL 的；在稍后上报块数据的 meta 信息，就额外包含了块数据所在的 WAL 的路径，及在 WAL 文件内的偏移地址和长度。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WriteAheadLogBasedBlockHandler</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">\tblockManager: <span class=\"type\">BlockManager</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    serializerManager: <span class=\"type\">SerializerManager</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    streamId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    storageLevel: <span class=\"type\">StorageLevel</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    conf: <span class=\"type\">SparkConf</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    hadoopConf: <span class=\"type\">Configuration</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    checkpointDir: <span class=\"type\">String</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    clock: <span class=\"type\">Clock</span> = new <span class=\"type\">SystemClock</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\"></span>)<span class=\"keyword\">extends</span> <span class=\"title\">ReceivedBlockHandler</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 保存超时时间</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tblockStoreTimeout = conf.getInt(    \t\t</span><br><span class=\"line\">    </span><br><span class=\"line\">    \t\t<span class=\"string\">\"spark.streaming.receiver.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\t\t\tblockStoreTimeout\"</span>,<span class=\"number\">30</span>).seconds</span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 写log类</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> writeAheadLog=<span class=\"type\">WriteAheadLogUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcreatLogForReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tconf,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcheckpointDirToLogDir(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tcheckpointDir,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstreamId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\thadoopConf</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">storeBlock</span></span>()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1. 执行blockManager</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> serializedBlock = block <span class=\"keyword\">match</span> &#123;...&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 执行保存到log</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 用future异步执行</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> storeInBlockManagerFuture=<span class=\"type\">Future</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockManger.putBytes(...serializedBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> storeInWriteAheadLogFuture=<span class=\"type\">Future</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\twriteAheadLog.write(...serializedBlock)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> combineFuture=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstoreInBlockManagerFuture.zip(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstoreInWriteAHeadLogFuture\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t).map(_._2)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> walRecordHandle=<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tawaitUtils.awaitResult(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcombineFuture,blockStoreTimeout</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"type\">WriteAheandLogBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tnumRecords,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\twalRecordHandle</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// future1.zip(future2): 合并future,返回tuple(future)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 两个future中有一个失败，则失败</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"4-数据上报\"><a href=\"#4-数据上报\" class=\"headerlink\" title=\"4. 数据上报\"></a>4. <strong>数据上报</strong></h1><p>每次成块在 executor 存储完毕后，ReceiverSupervisor 就会及时上报块数据的 meta 信息给 driver 端的 ReceiverTracker；这里的 meta 信息包括数据的标识 id，数据的位置，数据的条数，数据的大小等信息</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps298.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>ReceiverSupervisor会将数据的标识ID，数据的位置，数据的条数，数据的大小等信息上报给driver</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverSupervisorImpl</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pushAndReportBlock</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> blockStoreResult =</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treceivedBlockHandler.storeBlock(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlock</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t    <span class=\"keyword\">val</span> blockInfo = <span class=\"type\">ReceivedBlockInfo</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**streamId,** </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**numRecords,** </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**metadataOption,** </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t**blockStoreResult**</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    trackerEndpoint.askSync[<span class=\"type\">Boolean</span>](<span class=\"type\">AddBlock</span>(blockInfo))</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-1-wal上报\"><a href=\"#4-1-wal上报\" class=\"headerlink\" title=\"4.1. wal上报\"></a>4.1. <strong>wal上报</strong></h2><p>// 报告给driver的信息：blockId，block数量，<strong>walRecordHandle</strong>        </p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">WriteAheandLogBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\tblockId,</span><br><span class=\"line\"></span><br><span class=\"line\">\tnumRecords,</span><br><span class=\"line\"></span><br><span class=\"line\">\t**walRecordHandle**</span><br><span class=\"line\"></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-2-BlockManager上报\"><a href=\"#4-2-BlockManager上报\" class=\"headerlink\" title=\"4.2. BlockManager上报\"></a>4.2. <strong>BlockManager上报</strong></h2><p>// <strong>报告给driver的信息：id和num</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">BlockManagerBasedStoreResult</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\tblockId, </span><br><span class=\"line\"></span><br><span class=\"line\">\tnumRecords</span><br><span class=\"line\"></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<h1 id=\"5-receiver管理\"><a href=\"#5-receiver管理\" class=\"headerlink\" title=\"5. receiver管理\"></a>5. <strong>receiver管理</strong></h1><ol>\n<li>分发和监控receiver</li>\n<li>作为RpcEndpoint和reciever通信，接收和发送消息</li>\n<li>管理上报的meta信息</li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps299.tmp.jpg\" alt=\"img\"> </p>\n<p>一方面 Receiver 将通过 AddBlock 消息上报 meta 信息给 ReceiverTracker，另一方面 JobGenerator 将在每个 batch 开始时要求 ReceiverTracker 将已上报的块信息进行 batch 划分，ReceiverTracker 完成了块数据的 meta 信息管理工作。</p>\n<p>具体的，ReceiverTracker 有一个成员 ReceivedBlockTracker，专门负责已上报的块数据 meta 信息管理。</p>\n<h2 id=\"5-1-分发\"><a href=\"#5-1-分发\" class=\"headerlink\" title=\"5.1. 分发\"></a>5.1. <strong>分发</strong></h2><p>在 ssc.start() 时，将隐含地调用 ReceiverTracker.start()；而 ReceiverTracker.start() 最重要的任务就是调用自己的 launchReceivers() 方法将 Receiver 分发到多个 executor 上去。然后在每个 executor 上，由 <strong>ReceiverSupervisor</strong> 来分别启动一个 Receiver 接收数据</p>\n<p>而且在 1.5.0 版本以来引入了 <strong>ReceiverSchedulingPolicy</strong>，是在 Spark Streaming 层面添加对 Receiver 的分发目的地的计算，相对于之前版本依赖 Spark Core 的 TaskScheduler 进行通用分发，新的 ReceiverSchedulingPolicy 会对 Streaming 应用的更好的语义理解，也能计算出更好的分发策略。</p>\n<p>并且还通过每个 <strong>Receiver</strong> 对应 1 个 Job 的方式，保证了 Receiver 的多次分发，和失效后的重启、永活</p>\n<h2 id=\"5-2-监控\"><a href=\"#5-2-监控\" class=\"headerlink\" title=\"5.2. 监控\"></a>5.2. <strong>监控</strong></h2><h2 id=\"5-3-消息类型\"><a href=\"#5-3-消息类型\" class=\"headerlink\" title=\"5.3. 消息类型\"></a>5.3. <strong>消息类型</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2A9.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>ReceiverTracker</strong>：</p>\n<p>RpcEndPoint 可以理解为 RPC 的 server 端,底层由netty提供通信支持，供 client 调用。</p>\n<p>ReceiverTracker 作为 RpcEndPoint 的地址 —— 即 driver 的地址 —— 是公开的，可供 Receiver 连接；如果某个 Receiver 连接成功，那么 ReceiverTracker 也就持有了这个 Receiver 的 RpcEndPoint。这样一来，通过发送消息，就可以实现双向通信。</p>\n<h3 id=\"5-3-1-只接收不回复\"><a href=\"#5-3-1-只接收不回复\" class=\"headerlink\" title=\"5.3.1. 只接收不回复\"></a>5.3.1. <strong>只接收不回复</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2AA.tmp.jpg\" alt=\"img\"> </p>\n<p>只接收消息不回复，除了错误上报消息是excutor发送的以外，其余都是driver的tracker自己给自己发送的命令,接收消息均在ReceiverTracker.receive方法中实现</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-1-首次启动消息\"><a href=\"#5-3-1-1-首次启动消息\" class=\"headerlink\" title=\"5.3.1.1. 首次启动消息\"></a>5.3.1.1. <strong>首次启动消息</strong></h4><p>在 ReceiverTracker 刚启动时，发给自己这个消息，触发具体的 schedulingPolicy 计算，和后续分发</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> scheduledLocations = schedulingPolicy.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tscheduleReceivers(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceivers, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tgetExecutors</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (receiver &lt;- receivers) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          <span class=\"keyword\">val</span> executors = scheduledLocations(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">          updateReceiverScheduledExecutors(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\texecutors</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">          receiverPreferredLocations(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.streamId) = </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceiver.preferredLocation</span><br><span class=\"line\"></span><br><span class=\"line\">          \tstartReceiver(receiver, executors)</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-2-重新启动消息\"><a href=\"#5-3-1-2-重新启动消息\" class=\"headerlink\" title=\"5.3.1.2. 重新启动消息\"></a>5.3.1.2. <strong>重新启动消息</strong></h4><p>当初始分发的 executor 不对，或者 Receiver 失效等情况出现，发给自己这个消息，触发 Receiver 重新分发</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 失败重启</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span>(receiver)=&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 1.获取之前的executors</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> oldScheduledExecutors = \t\t\tgetStoredScheduledExecutors(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver.streamId</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 2. 计算新的excutor位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t <span class=\"keyword\">val</span> scheduledLocations = <span class=\"keyword\">if</span> \t\t\t(oldScheduledExecutors.nonEmpty) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// 2.1 之前excutors可用，则使用之前的</span></span><br><span class=\"line\"></span><br><span class=\"line\">            oldScheduledExecutors</span><br><span class=\"line\"></span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 2.2 之前的不可用则重新计算位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tschedulingPolicy.rescheduleReceiver(）</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 3. 发送给worker重启receiver</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t   startReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiver, scheduledLocations)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-3-清除blocks消息\"><a href=\"#5-3-1-3-清除blocks消息\" class=\"headerlink\" title=\"5.3.1.3. 清除blocks消息\"></a>5.3.1.3. <strong>清除blocks消息</strong></h4><p>当块数据已完成计算不再需要时，发给自己这个消息，将给所有的 Receiver 转发此 CleanupOldBlocks 消息</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceiverTrackingInfos.values.flatMap(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t_.endpoint</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t).foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t_.send(c)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-4-更新接收频率\"><a href=\"#5-3-1-4-更新接收频率\" class=\"headerlink\" title=\"5.3.1.4. 更新接收频率\"></a>5.3.1.4. <strong>更新接收频率</strong></h4><p>ReceiverTracker 动态计算出某个 Receiver 新的 rate limit，将给具体的 Receiver 发送 UpdateRateLimit 消息</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t <span class=\"keyword\">for</span> (info &lt;- \t\t\treceiverTrackingInfos.get(streamUID);</span><br><span class=\"line\"></span><br><span class=\"line\"> \t\t\teP &lt;- info.endpoint) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">          \t\teP.send(<span class=\"type\">UpdateRateLimit</span>(newRate))</span><br><span class=\"line\"></span><br><span class=\"line\">        \t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-1-5-错误上报消息\"><a href=\"#5-3-1-5-错误上报消息\" class=\"headerlink\" title=\"5.3.1.5. 错误上报消息\"></a>5.3.1.5. <strong>错误上报消息</strong></h4> <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>:<span class=\"type\">PartialFunction</span>[<span class=\"type\">Any</span>,<span class=\"type\">Unit</span>]=&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StartAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RestartReceiver</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">CleanupOldBlocks</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">UpdateReceiverRateLimit</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">ReportError</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\treportError(streamId, message, error)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-3-2-接收并回复\"><a href=\"#5-3-2-接收并回复\" class=\"headerlink\" title=\"5.3.2. 接收并回复\"></a>5.3.2. <strong>接收并回复</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2AB.tmp.jpg\" alt=\"img\"> </p>\n<p>接收executor的消息，处理完毕后并回复给executor</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-1-注册Receiver消息\"><a href=\"#5-3-2-1-注册Receiver消息\" class=\"headerlink\" title=\"5.3.2.1. 注册Receiver消息\"></a>5.3.2.1. <strong>注册Receiver消息</strong></h4><p>由 Receiver 在试图启动的过程中发来，将回复允许启动，或不允许启动</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> successful=registerReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreamId,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"class\"><span class=\"keyword\">type</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">host</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">executorId</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">receiverEndpoint</span>,</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t\t<span class=\"title\">context</span>.<span class=\"title\">senderAddress</span>)</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">context</span>.<span class=\"title\">reply</span>(<span class=\"params\">successful</span>)</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t</span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">AddBlock</span>(<span class=\"params\"></span>) <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">DeregisterReceiver</span>(<span class=\"params\"></span>) <span class=\"title\">=&gt;</span> ... </span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">AllReceiverIds</span> <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">GetAllReceiverInfo</span> <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t\t<span class=\"title\">case</span> <span class=\"title\">StopAllReceivers</span> <span class=\"title\">=&gt;</span> ...</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">\t&#125;</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">&#125;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-2-meta上报消息\"><a href=\"#5-3-2-2-meta上报消息\" class=\"headerlink\" title=\"5.3.2.2. meta上报消息\"></a>5.3.2.2. <strong>meta上报消息</strong></h4><p>具体的块数据 meta 上报消息，由 Receiver 发来，将返回成功或失败</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcontext.reply(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\taddBlock(receivedBlockInfo)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-3-查询ReceiverIds消息\"><a href=\"#5-3-2-3-查询ReceiverIds消息\" class=\"headerlink\" title=\"5.3.2.3. 查询ReceiverIds消息\"></a>5.3.2.3. <strong>查询ReceiverIds消息</strong></h4><p>executor发送的本地消息。在 ReceiverTracker stop() 的过程中，查询是否还有活跃的 Receiver，返回所有或者的receiverId</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcontext.reply(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiverTrackingInfos.filter(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t_._2.state != \t\t\t\t\t<span class=\"type\">ReceiverState</span>.<span class=\"type\">INACTIVE</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t).keys.toSeq</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-4-查询ReceiversInfo消息\"><a href=\"#5-3-2-4-查询ReceiversInfo消息\" class=\"headerlink\" title=\"5.3.2.4. 查询ReceiversInfo消息\"></a>5.3.2.4. <strong>查询ReceiversInfo消息</strong></h4><p>查询所有excutors的信息给receiver</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tcontext.reply(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceiverTrackingInfos.toMap</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-5-停止所有Receiver消息\"><a href=\"#5-3-2-5-停止所有Receiver消息\" class=\"headerlink\" title=\"5.3.2.5. 停止所有Receiver消息\"></a>5.3.2.5. <strong>停止所有Receiver消息</strong></h4><p>在 ReceiverTracker stop() 的过程刚开始时，要求 stop 所有的 Receiver；将向所有的 Receiver 发送 stop 信息,并返回true</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt; ... </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t assert(isTrackerStopping || \t\t\t\t\tisTrackerStopped)</span><br><span class=\"line\"></span><br><span class=\"line\">        \t\t\treceiverTrackingInfos.values.flatMap(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t_.endpoint</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t).foreach &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t _.send(<span class=\"type\">StopReceiver</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        \tcontext.reply(<span class=\"literal\">true</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-3-2-6-注销Receiver消息\"><a href=\"#5-3-2-6-注销Receiver消息\" class=\"headerlink\" title=\"5.3.2.6. 注销Receiver消息\"></a>5.3.2.6. <strong>注销Receiver消息</strong></h4><p>由 Receiver 发来，停止receiver，处理后，无论如何都返回 true</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receiveAndReply</span></span>(context:<span class=\"type\">RpcCallContext</span>)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">RegisterReceiver</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AddBlock</span>() =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">DeregisterReceiver</span>() =&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tderegisterReceiver(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tmessage, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\terror</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">       \t\t context.reply(<span class=\"literal\">true</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">AllReceiverIds</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">GetAllReceiverInfo</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> <span class=\"type\">StopAllReceivers</span> =&gt; ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-4-meta信息管理\"><a href=\"#5-4-meta信息管理\" class=\"headerlink\" title=\"5.4. meta信息管理\"></a>5.4. <strong>meta信息管理</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2BC.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"5-4-1-接收meta信息\"><a href=\"#5-4-1-接收meta信息\" class=\"headerlink\" title=\"5.4.1. 接收meta信息\"></a>5.4.1. <strong>接收meta信息</strong></h3><p>addBlock(receivedBlockInfo: ReceivedBlockInfo)方法接收到某个 Receiver 上报上来的块数据 meta 信息，将其加入到 streamIdToUnallocatedBlockQueues 里</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 上报上来的、但尚未分配入 batch 的 Block 块数据的 meta</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> streamIdToUnallocatedBlockQueues = </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ReceivedBlockQueue</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// WAL</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> writeResult=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\twriteToLog(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">BlockAdditionEvent</span>(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\treceivedBlockInfo</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span>(writeResult)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsynchronized&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tstreamIdToUnallocatedBlockQueues.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tgetOrElseUpdate(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\tstreamId, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceivedBlockQueue</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t)+=</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\treceivedBlockInfo</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-4-2-batch分配\"><a href=\"#5-4-2-batch分配\" class=\"headerlink\" title=\"5.4.2. batch分配\"></a>5.4.2. <strong>batch分配</strong></h3><p>JobGenerator 在发起新 batch 的计算时，将 streamIdToUnallocatedBlockQueues 的内容，以传入的 batchTime 参数为 key，<strong>添加到 timeToAllocatedBlocks 里，并更新 lastAllocatedBatchTime</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 上报上来的、已分配入 batch 的 Block 块数据的 meta,按照 batch 进行一级索引、再按照 receiverId 进行二级索引的 queue，所以是一个 HashMap: time → HashMap</span></span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> timeToAllocatedBlocks = </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">new</span> mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">Time</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"type\">AllocatedBlocks</span>:<span class=\"type\">Map</span>[</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"type\">Int</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t<span class=\"type\">Seq</span>[<span class=\"type\">ReceivedBlockInfo</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 记录了最近一个分配完成的 batch 是哪个</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> lastAllocatedBatchTime: <span class=\"type\">Time</span> = <span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 收集所有未分配的blocks</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">allocateBlocksToBatch</span></span>(batchTime: <span class=\"type\">Time</span>): \t<span class=\"type\">Unit</span> = synchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断时间是否合法：大于最近收集的时间</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (lastAllocatedBatchTime == <span class=\"literal\">null</span> || batchTime &gt; lastAllocatedBatchTime) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t <span class=\"comment\">// 从未分配队列中取出blocks</span></span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> streamIdToBlocks = streamIds.map &#123; \t\t\t</span><br><span class=\"line\">    \t\tstreamId =&gt;(streamId,getReceivedBlockQueue(streamId)\t\t\t\t\t</span><br><span class=\"line\">\t\t\t\t.dequeueAll(x =&gt; <span class=\"literal\">true</span>))</span><br><span class=\"line\">      \t\t&#125;.toMap</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> allocatedBlocks =<span class=\"type\">AllocatedBlocks</span>(streamIdToBlocks)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (writeToLog(<span class=\"type\">BatchAllocationEvent</span>(batchTime, allocatedBlocks))) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 放入已分配队列</span></span><br><span class=\"line\"></span><br><span class=\"line\">        timeToAllocatedBlocks.put(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tbatchTime, allocatedBlocks)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 更新最近分配的时间戳</span></span><br><span class=\"line\"></span><br><span class=\"line\">        lastAllocatedBatchTime = batchTime</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">     logInfo(<span class=\"string\">s\"Possibly processed batch <span class=\"subst\">$batchTime</span> needs to be processed again in WAL recovery\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-4-3-计算DAG生成\"><a href=\"#5-4-3-计算DAG生成\" class=\"headerlink\" title=\"5.4.3. 计算DAG生成\"></a>5.4.3. <strong>计算DAG生成</strong></h3><p>JobGenerator 在发起新 batch 的计算时，由 DStreamGraph 生成 RDD DAG 实例时，调用getBlocksOfBatch(batchTime: Time)查 timeToAllocatedBlocks，获得划入本 batch 的块数据元信息，由此生成处理对应块数据的 RDD</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getBlocksOfBatch</span></span>(batchTime: <span class=\"type\">Time</span>): \t<span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">ReceivedBlockInfo</span>]] = \tsynchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    timeToAllocatedBlocks.get(batchTime).map &#123; \t\t_.streamIdToAllocatedBlocks \t&#125;.getOrElse(<span class=\"type\">Map</span>.empty)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-4-4-清除meta信息\"><a href=\"#5-4-4-清除meta信息\" class=\"headerlink\" title=\"5.4.4. 清除meta信息\"></a>5.4.4. <strong>清除meta信息</strong></h3><p>当一个 batch 已经计算完成、可以把已追踪的块数据的 meta 信息清理掉时调用，将通过job清理 timeToAllocatedBlocks 表里对应 cleanupThreshTime 之前的所有 batch 块数据 meta 信息</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceivedBlockTracker</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cleanupOldBatches</span></span>(cleanupThreshTime: <span class=\"type\">Time</span>, waitForCompletion: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = synchronized &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> timesToCleanup = \ttimeToAllocatedBlocks.keys.</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfilter &#123; _ &lt; cleanupThreshTime &#125;.toSeq&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (writeToLog(</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">BatchCleanupEvent</span>(timesToCleanup))) &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t <span class=\"comment\">// 清除已分配batch队列 </span></span><br><span class=\"line\"></span><br><span class=\"line\">\ttimeToAllocatedBlocks --= timesToCleanup</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// 清除WAL</span></span><br><span class=\"line\"></span><br><span class=\"line\">\twriteAheadLogOption.foreach(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t_.clean(</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcleanupThreshTime.milliseconds, \t\twaitForCompletion)</span><br><span class=\"line\"></span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> 脑图制作参考：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p>完整脑图链接地址：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkStreaming源码解析之Job动态生成","url":"https://sustcoder.github.io/2018/12/03/2018-12-03-sparkStreaming-sourceCodeAnalysis_job/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_DataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis_faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps1C70.tmp.jpg\" alt=\"img\"></p>\n<p>在 Spark Streaming 程序的入口，我们都会定义一个 batchDuration，就是需要每隔多长时间就比照静态的 DStreamGraph 来动态生成一个 RDD DAG 实例。在 Spark Streaming 里，总体负责动态作业调度的具体类是 JobScheduler。</p>\n<p> JobScheduler 有两个非常重要的成员：JobGenerator 和 ReceiverTracker。JobScheduler 将每个 batch 的 RDD DAG 具体生成工作委托给 JobGenerator，而将源头输入数据的记录工作委托给 ReceiverTracker。</p>\n<h1 id=\"1-启动\"><a href=\"#1-启动\" class=\"headerlink\" title=\"1. 启动\"></a>1. <strong>启动</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2045.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"1-1-JobScheduler\"><a href=\"#1-1-JobScheduler\" class=\"headerlink\" title=\"1.1. JobScheduler\"></a>1.1. <strong>JobScheduler</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2046.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>job运行的总指挥是JobScheduler.start()，</strong></p>\n<p> JobScheduler 有两个非常重要的成员：JobGenerator 和 ReceiverTracker。JobScheduler 将每个 batch 的 RDD DAG 具体生成工作委托给 JobGenerator，而将源头输入数据的记录工作委托给 ReceiverTracker。</p>\n<p><strong>在StreamingContext中启动scheduler</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamingContext</span>(<span class=\"params\">sc,cp,batchDur</span>)</span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> scheduler = <span class=\"keyword\">new</span> <span class=\"type\">JobScheduler</span>(<span class=\"keyword\">this</span>)</span><br><span class=\"line\">\tstart()&#123;</span><br><span class=\"line\">\t\tscheduler.start()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>在JobScheduler中启动recieverTracker和JobGenerator</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobScheduler</span>(<span class=\"params\">ssc</span>) </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> receiverTracker:<span class=\"type\">ReceiverTracker</span>=<span class=\"literal\">null</span></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> jobGenerator=<span class=\"keyword\">new</span> <span class=\"type\">JobGenerator</span>(<span class=\"keyword\">this</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobExecutor=<span class=\"type\">ThreadUtils</span>.newDaemonFixedThreadPool()</span><br><span class=\"line\">\t<span class=\"keyword\">if</span>(stared) <span class=\"keyword\">return</span> <span class=\"comment\">// 只启动一次</span></span><br><span class=\"line\">\treceiverTracker.start()</span><br><span class=\"line\">    jobGenerator.start()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-1-启动ReceiverTracker\"><a href=\"#1-1-1-启动ReceiverTracker\" class=\"headerlink\" title=\"1.1.1. 启动ReceiverTracker\"></a>1.1.1. <strong>启动ReceiverTracker</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2057.tmp.jpg\" alt=\"img\"> </p>\n<ol>\n<li><p>在JobScheduler的start中启动ReceiverTraker:<code>receiverTracker.start()：</code></p>\n</li>\n<li><p>RecieverTracker 调用launchReceivers方法</p>\n</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span>  <span class=\"title\">ReceiverTracker</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> endpoint:<span class=\"type\">RpcEndpointRef</span>=<span class=\"literal\">null</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start</span></span>()=synchronized&#123;</span><br><span class=\"line\">\t\tendpoint=ssc.env.rpcEnv.setEndpoint(</span><br><span class=\"line\">\t\t\t<span class=\"string\">\"receiverTracker\"</span>,</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">ReceiverTrackerEndpoint</span>() </span><br><span class=\"line\">\t\t)</span><br><span class=\"line\">\t\tlaunchReceivers()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-1-1-1-ReceiverSupervisor\"><a href=\"#1-1-1-1-ReceiverSupervisor\" class=\"headerlink\" title=\"1.1.1.1. ReceiverSupervisor\"></a>1.1.1.1. <strong>ReceiverSupervisor</strong></h4><p> ReceiverTracker将RDD DAG和启动receiver的Func包装成ReceiverSupervisor发送到最优的Excutor节点上</p>\n<h4 id=\"1-1-1-2-拉起receivers\"><a href=\"#1-1-1-2-拉起receivers\" class=\"headerlink\" title=\"1.1.1.2. 拉起receivers\"></a>1.1.1.2. <strong>拉起receivers</strong></h4><p> 从ReceiverInputDStreams中获取Receivers，并把他们发送到所有的worker nodes:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class  ReceiverTracker &#123;</span><br><span class=\"line\">\tvar endpoint:RpcEndpointRef=</span><br><span class=\"line\">\tprivate def launchReceivers()&#123;</span><br><span class=\"line\">\t\t// DStreamGraph的属性inputStreams</span><br><span class=\"line\">\t\tval receivers=inputStreams.map&#123;nis=&gt;</span><br><span class=\"line\">\t\t\tval rcvr=nis.getReceiver()</span><br><span class=\"line\">\t\t\t// rcvr是对kafka,socket等接受数据的定义</span><br><span class=\"line\">\t\t\trcvr</span><br><span class=\"line\">\t\t&#125;\t</span><br><span class=\"line\">\t\t// 发送到worker</span><br><span class=\"line\">\t\t endpoint.send(StartAllReceivers(receivers))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-2-启动DAG生成\"><a href=\"#1-1-2-启动DAG生成\" class=\"headerlink\" title=\"1.1.2. 启动DAG生成\"></a>1.1.2. <strong>启动DAG生成</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2058.tmp.jpg\" alt=\"img\"> </p>\n<p> 在JobScheduler的start中启动JobGenerator:<code>JobGenerator.start()</code></p>\n<h4 id=\"1-1-2-1-startFirstTime\"><a href=\"#1-1-2-1-startFirstTime\" class=\"headerlink\" title=\"1.1.2.1. startFirstTime\"></a>1.1.2.1. <strong>startFirstTime</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2059.tmp.jpg\" alt=\"img\"> </p>\n<p>  <strong>首次启动</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">startFirstTime</span></span>() &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 定义定时器</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> startTime = </span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(timer.getStartTime())</span><br><span class=\"line\">\t<span class=\"comment\">// 启动DStreamGraph</span></span><br><span class=\"line\">    graph.start(startTime - graph.batchDuration)</span><br><span class=\"line\">    <span class=\"comment\">//  启动定时器</span></span><br><span class=\"line\">\t timer.start(startTime.milliseconds)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-1-2-1-1-启动DAG\"><a href=\"#1-1-2-1-1-启动DAG\" class=\"headerlink\" title=\"1.1.2.1.1. 启动DAG\"></a>1.1.2.1.1. <strong>启动DAG</strong></h5><p><strong>graph的生成是在StreamingContext中</strong>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> graph: <span class=\"type\">DStreamGraph</span>=&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 重启服务时</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span>（isCheckpointPresent）&#123;</span><br><span class=\"line\">\t\tcheckPoint.graph.setContext(<span class=\"keyword\">this</span>)</span><br><span class=\"line\">\t\tcheckPoint.graph.restoreCheckPointData()</span><br><span class=\"line\">\t\tcheckPoint.graph</span><br><span class=\"line\">\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">// 首次初始化时</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> newGraph=<span class=\"keyword\">new</span> <span class=\"type\">DStreamGraph</span>()</span><br><span class=\"line\">\t\tnewGraph.setBatchDuration(_batchDur)</span><br><span class=\"line\">\t\tnewGraph</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>在GenerateJobs中启动graph</strong>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">graph.start(nowTime-batchDuration)</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-1-2-1-2-启动timer\"><a href=\"#1-1-2-1-2-启动timer\" class=\"headerlink\" title=\"1.1.2.1.2. 启动timer\"></a>1.1.2.1.2. <strong>启动timer</strong></h5><p><strong>JobGenerator中定义了一个定时器：</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> timer=<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(colck,batchDuaraion,</span><br><span class=\"line\">\t\tlongTime=&gt;eventLoop.post(</span><br><span class=\"line\">            <span class=\"type\">GenerateJobs</span>(</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(longTime)</span><br><span class=\"line\">            )</span><br><span class=\"line\">         )</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p><strong>在JobGenerator启动时会开始执行这个调度器：</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">timer.start(startTime.milliseconds)</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-2-RecurringTimer：定时器\"><a href=\"#1-2-RecurringTimer：定时器\" class=\"headerlink\" title=\"1.2. RecurringTimer：定时器\"></a>1.2. <strong>RecurringTimer：定时器</strong></h2><p>// 来自 JobGenerator</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[streaming]</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobGenerator</span>(<span class=\"params\">jobScheduler: <span class=\"type\">JobScheduler</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> timer = <span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(clock, ssc.graph.batchDuration.milliseconds,</span><br><span class=\"line\">      longTime =&gt; eventLoop.post(<span class=\"type\">GenerateJobs</span>(<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(longTime))), <span class=\"string\">\"JobGenerator\"</span>)</span><br><span class=\"line\">...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过代码也可以看到，整个 timer 的调度周期就是 batchDuration，每次调度起来就是做一个非常简单的工作：往 eventLoop 里发送一个消息 —— 该为当前 batch (new Time(longTime)) GenerateJobs 了！</p>\n<h1 id=\"2-生成\"><a href=\"#2-生成\" class=\"headerlink\" title=\"2. 生成\"></a>2. <strong>生成</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps205A.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>JobGenerator中定义了一个定时器，在定时器中启动生成job操作</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobGenerator</span></span>:</span><br><span class=\"line\"><span class=\"comment\">// 定义定时器</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> timer=</span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">RecurringTimer</span>(colck,batchDuaraion,</span><br><span class=\"line\">\tlongTime=&gt;eventLoop.post(<span class=\"type\">GenerateJobs</span>(</span><br><span class=\"line\">\t<span class=\"keyword\">new</span> <span class=\"type\">Time</span>(longTime))))</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJobs</span></span>(time: <span class=\"type\">Time</span>) &#123;</span><br><span class=\"line\">  <span class=\"type\">Try</span> &#123;</span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 1. 将已收到的数据进行一次 allocate</span></span><br><span class=\"line\">  receiverTracker.allocateBlocksToBatch(time)  </span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">//   2. 复制一份新的DAG实例</span></span><br><span class=\"line\">  graph.generateJobs(time)                                                 </span><br><span class=\"line\">   &#125; <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">Success</span>(jobs) =&gt;</span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 3. 获取 meta 信息</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> streamIdToInputInfos = jobScheduler.inputInfoTracker.getInfo(time)  </span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 4. 提交job     </span></span><br><span class=\"line\"> jobScheduler.submitJobSet(<span class=\"type\">JobSet</span>(time, jobs, streamIdToInputInfos))   </span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Failure</span>(e) =&gt;</span><br><span class=\"line\">      jobScheduler.reportError(<span class=\"string\">\"Error generating jobs for time \"</span> + time, e)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 5. checkpoint</span></span><br><span class=\"line\">  eventLoop.post(<span class=\"type\">DoCheckpoint</span>(time, clearCheckpointDataLater = <span class=\"literal\">false</span>))      </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-2-获取DAG实例\"><a href=\"#2-2-获取DAG实例\" class=\"headerlink\" title=\"2.2. 获取DAG实例\"></a>2.2. <strong>获取DAG实例</strong></h2><p>在生成Job并提交到excutor的第二步，</p>\n<p>JobGenerator-&gt;DStreamGraph-&gt;OutputStreams-&gt;ForEachDStream-&gt;TransformationDStream-&gt;InputDStream</p>\n<p>具体流程是：</p>\n<p>- 1. JobGenerator调用了DStreamGraph里面的gererateJobs(time)方法</p>\n<p>- 2. DStreamGraph里的generateJobs方法遍历了outputStreams</p>\n<p>- 3. OutputStreams调用了其generateJob(time)方法</p>\n<p>- 4. ForEachDStream实现了generateJob方法，调用了：</p>\n<p>​    parent.getOrCompute(time)</p>\n<p>递归的调用父类的getOrCompute方法去动态生成物理DAG图</p>\n<h1 id=\"3-运行\"><a href=\"#3-运行\" class=\"headerlink\" title=\"3. 运行\"></a>3. <strong>运行</strong></h1><h2 id=\"3-1-异步处理-JobScheduler\"><a href=\"#3-1-异步处理-JobScheduler\" class=\"headerlink\" title=\"3.1. 异步处理:JobScheduler\"></a>3.1. <strong>异步处理:JobScheduler</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps2081.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>JobScheduler通过线程池执行从JobGenerator提交过来的Job，jobExecutor异步的去处理提交的job</strong></p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JobScheduler</span></span>&#123;</span><br><span class=\"line\">  numConcurrentJobs = ssc.conf.getInt(<span class=\"string\">\"spark.streaming.concurrentJobs\"</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> jobExecutor =<span class=\"type\">ThreadUtils</span>.</span><br><span class=\"line\">\tnewDaemonFixedThreadPool(numConcurrentJobs, <span class=\"string\">\"streaming-job-executor\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submitJobSet</span></span>(jobSet: <span class=\"type\">JobSet</span>) &#123;</span><br><span class=\"line\">\t\tjobSet.jobs.foreach(job =&gt;</span><br><span class=\"line\">                            jobExecutor.execute(<span class=\"keyword\">new</span> <span class=\"type\">JobHandler</span>(job)))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-1-Job-类比Thread\"><a href=\"#3-1-1-Job-类比Thread\" class=\"headerlink\" title=\"3.1.1. Job:类比Thread\"></a>3.1.1. <strong>Job:类比Thread</strong></h3><h3 id=\"3-1-2-JobHandler：真正执行job\"><a href=\"#3-1-2-JobHandler：真正执行job\" class=\"headerlink\" title=\"3.1.2. JobHandler：真正执行job\"></a>3.1.2. <strong>JobHandler：真正执行job</strong></h3><p> JobHandler 除了做一些状态记录外，最主要的就是调用 job.run()，</p>\n<p> 在 ForEachDStream.generateJob(time) 时，是定义了 Job 的运行逻辑，即定义了 Job.func。而在 <strong>JobHandler 这里，是真正调用了 Job.run()、将触发 Job.func 的真正执行</strong>！</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 来自 JobHandler</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  <span class=\"comment\">// 【发布 JobStarted 消息】</span></span><br><span class=\"line\">  _eventLoop.post(<span class=\"type\">JobStarted</span>(job))</span><br><span class=\"line\">  <span class=\"type\">PairRDDFunctions</span>.disableOutputSpecValidation.withValue(<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 【主要逻辑，直接调用了 job.run()】</span></span><br><span class=\"line\">    job.run()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _eventLoop = eventLoop</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (_eventLoop != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">  <span class=\"comment\">// 【发布 JobCompleted 消息】</span></span><br><span class=\"line\">    _eventLoop.post(<span class=\"type\">JobCompleted</span>(job))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-1-3-concurrentJobs-job并行度\"><a href=\"#3-1-3-concurrentJobs-job并行度\" class=\"headerlink\" title=\"3.1.3. concurrentJobs : job并行度\"></a>3.1.3. <strong>concurrentJobs : job并行度</strong></h3><p><strong>spark.streaming.concurrentJobs job并行度</strong></p>\n<p>这里 jobExecutor 的线程池大小，是由 spark.streaming.concurrentJobs 参数来控制的，当没有显式设置时，其取值为 1。</p>\n<p>进一步说，这里 jobExecutor 的线程池大小，就是能够并行执行的 Job 数。而回想前文讲解的 DStreamGraph.generateJobs(time) 过程，一次 batch 产生一个 Seq[Job}，里面可能包含多个 Job —— 所以，确切的，有几个 output 操作，就调用几次 ForEachDStream.generatorJob(time)，就产生出几个 Job</p>\n<p><strong>脑图制作参考</strong>：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p><strong>完整脑图链接地址</strong>：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"sparkStreaming源码解析之DAG定义","url":"https://sustcoder.github.io/2018/12/01/2018-12-01-sparkStreaming-sourceCodeAnalysis_DAG/","content":"<p>此文是从思维导图中导出稍作调整后生成的，思维脑图对代码浏览支持不是很好，为了更好阅读体验，文中涉及到的源码都是删除掉不必要的代码后的伪代码，如需获取更好阅读体验可下载脑图配合阅读：</p>\n<p> 此博文共分为四个部分：</p>\n<ol>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82A.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/01/sparkStreaming-sourceCodeAnalysis_DAG/\">DAG定义</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsC3C6.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/03/sparkStreaming-sourceCodeAnalysis_job/\">Job动态生成</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps230.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/09/sparkStreaming-sourceCodeAnalysis_DataInputOutput/\">数据的产生与导入</a></li>\n<li><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wps614F.tmp.jpg\" alt=\"img\"><a href=\"https://sustcoder.github.io/2018/12/12/sparkStreaming-sourceCodeAnalysis_faultTolerance/\">容错</a></li>\n</ol>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/DAG.jpg\" alt=\"img\"></p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82B.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"1-DStream\"><a href=\"#1-DStream\" class=\"headerlink\" title=\"1. DStream\"></a>1. <strong>DStream</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA82C.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"1-1-RDD\"><a href=\"#1-1-RDD\" class=\"headerlink\" title=\"1.1. RDD\"></a>1.1. <strong>RDD</strong></h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA83D.tmp.jpg\" alt=\"img\"> </p>\n<p><strong>DStream和RDD关系：</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">DStream is a continuous sequence of RDDs：</span><br><span class=\"line\">generatedRDDs=new HashMap[Time,RDD[T]]()</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-1-存储\"><a href=\"#1-1-1-存储\" class=\"headerlink\" title=\"1.1.1. 存储\"></a>1.1.1. <strong>存储</strong></h3><p> <strong>存储格式</strong></p>\n<p>DStream内部通过一个HashMap的变量generatedRDD来记录生成的RDD:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">private[streaming] var generatedRDDs = new HashMap[Time, RDD[T]] ()</span><br></pre></td></tr></table></figure>\n<p><em>其中 ：</em></p>\n<p>​    <em>- key: time是生成当前batch的时间戳</em></p>\n<p>​    <em>- value: 生成的RDD实例</em></p>\n<p> <strong>每一个不同的 DStream 实例，都有一个自己的 generatedRDD，即每个转换操作的结果都会保留</strong></p>\n<h3 id=\"1-1-2-获取\"><a href=\"#1-1-2-获取\" class=\"headerlink\" title=\"1.1.2. 获取\"></a>1.1.2. <strong>获取</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA83E.tmp.jpg\" alt=\"img\"> </p>\n<h4 id=\"1-1-2-1-getOrCompute\"><a href=\"#1-1-2-1-getOrCompute\" class=\"headerlink\" title=\"1.1.2.1. getOrCompute\"></a>1.1.2.1. <strong>getOrCompute</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA83F.tmp.jpg\" alt=\"img\"> </p>\n<ol>\n<li><p>从rdd的map中获取：generatedRDDs.get(time).orElse</p>\n</li>\n<li><p>map中没有则计算：val newRDD=compute(time)</p>\n</li>\n<li><p>将计算的newRDD放入map中：generatedRDDs.put(time, newRDD)</p>\n<p>其中compute方法有以下特点：</p>\n</li>\n</ol>\n<ul>\n<li><p>不同DStream的计算方式不同</p>\n</li>\n<li><p>inputStream会对接对应数据源的API</p>\n</li>\n<li><p>transformStream会从父依赖中去获取RDD并进行转换得新的DStream</p>\n</li>\n</ul>\n<p>compute方法实现：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReceiverInputDStream</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(validTime: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">RDD</span>[<span class=\"type\">T</span>]] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockRDD = &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">      <span class=\"keyword\">if</span> (validTime &lt; graph.startTime) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// If this is called for any time before the start time of the context,</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// then this returns an empty RDD. This may happen when recovering from a</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// driver failure without any write ahead log to recover pre-failure data.</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"type\">BlockRDD</span>[<span class=\"type\">T</span>](ssc.sc, <span class=\"type\">Array</span>.empty)</span><br><span class=\"line\"></span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Otherwise, ask the tracker for all the blocks that have been allocated to this stream</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// for this batch</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">val</span> receiverTracker = ssc.scheduler.receiverTracker</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">val</span> blockInfos = receiverTracker.getBlocksOfBatch(validTime).getOrElse(id, <span class=\"type\">Seq</span>.empty)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// Register the input blocks information into InputInfoTracker</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">val</span> inputInfo = <span class=\"type\">StreamInputInfo</span>(id, blockInfos.flatMap(_.numRecords).sum)</span><br><span class=\"line\"></span><br><span class=\"line\">        ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\">// Create the BlockRDD</span></span><br><span class=\"line\"></span><br><span class=\"line\">        createBlockRDD(validTime, blockInfos)</span><br><span class=\"line\"></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">Some</span>(blockRDD)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-1-3-生成\"><a href=\"#1-1-3-生成\" class=\"headerlink\" title=\"1.1.3. 生成\"></a>1.1.3. <strong>生成</strong></h3><p>RDD主要分为以下三个过程：InputStream -&gt; TransFormationStream -&gt; OutputStream</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA864.tmp.jpg\" alt=\"img\"> </p>\n<h4 id=\"1-1-3-1-InputStream\"><a href=\"#1-1-3-1-InputStream\" class=\"headerlink\" title=\"1.1.3.1. InputStream\"></a>1.1.3.1. <strong>InputStream</strong></h4><p>inputstream包括FileInputStream，KafkaInputStream等等</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA865.tmp.jpg\" alt=\"img\"> </p>\n<h5 id=\"1-1-3-1-1-FileInputStream\"><a href=\"#1-1-3-1-1-FileInputStream\" class=\"headerlink\" title=\"1.1.3.1.1. FileInputStream\"></a>1.1.3.1.1. <strong>FileInputStream</strong></h5><p>FileInputStream的生成步骤：</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA866.tmp.jpg\" alt=\"img\"> </p>\n<ol>\n<li><p>找到新产生的文件：val newFiles = findNewFiles(validTime.milliseconds)</p>\n</li>\n<li><p>将newFiles转换为RDDs：val rdds=filesToRDD(newFiles)</p>\n<p>2.1.  遍历文件列表获取生成RDD: val fileRDDs=files.map(file=&gt;newAPIHadoop(file))</p>\n<p>2.2.  将每个文件的RDD进行合并并返回：return new UnionRDD(fileRDDs)</p>\n</li>\n<li><p>返回生成的rdds</p>\n</li>\n</ol>\n<h4 id=\"1-1-3-2-TransformationStream\"><a href=\"#1-1-3-2-TransformationStream\" class=\"headerlink\" title=\"1.1.3.2. TransformationStream\"></a>1.1.3.2. <strong>TransformationStream</strong></h4><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA87D.tmp.jpg\" alt=\"img\"> </p>\n<p>RDD的转换实现：</p>\n<ol>\n<li>获取parent DStream：val parentDs=parent.getOrCompute(validTime)</li>\n<li>执行转换函数并返回转换结果：return parentDs.map(mapFunc)</li>\n</ol>\n<p><strong>转换类的DStream实现特点</strong>：</p>\n<ul>\n<li><p>传入parent DStream和转换函数</p>\n</li>\n<li><p>compute方法中从parent DStream中获取DStream并对其作用转换函数</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[streaming]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MappedDStream</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>] (<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    parent: <span class=\"type\">DStream</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    mapFunc: <span class=\"type\">T</span> =&gt; <span class=\"type\">U</span></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">DStream</span>[<span class=\"type\">U</span>](<span class=\"params\">parent.ssc</span>) </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dependencies</span></span>: <span class=\"type\">List</span>[<span class=\"type\">DStream</span>[_]] = <span class=\"type\">List</span>(parent)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">slideDuration</span></span>: <span class=\"type\">Duration</span> = parent.slideDuration</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(validTime: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">RDD</span>[<span class=\"type\">U</span>]] = &#123;</span><br><span class=\"line\">    parent.getOrCompute(validTime).map(_.map[<span class=\"type\">U</span>](mapFunc))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>不同DStream的getOrCompute方法实现：</p>\n<ul>\n<li>FilteredDStream：<code>parent.getOrCompute(validTime).map(_.filter(filterFunc)</code></li>\n<li>FlatMapValuedDStream:<code>parent.getOrCompute(validTime).map(_.flatMapValues[U](flatMapValueFunc)</code></li>\n<li>MappedDStream:<code>parent.getOrCompute(validTime).map(_.map[U](mapFunc))</code></li>\n</ul>\n<p>在最开始， <strong>DStream 的 transformation 的 API 设计与 RDD 的 transformation 设计保持了一致，就使得，每一个 dStreamA.transformation() 得到的新 dStreamB 能将 dStreamA.transformation() 操作完美复制为每个 batch 的 rddA.transformation() 操作</strong>。这也就是 DStream 能够作为 RDD 模板，在每个 batch 里实例化 RDD 的根本原因。</p>\n<h4 id=\"1-1-3-3-OutputDStream\"><a href=\"#1-1-3-3-OutputDStream\" class=\"headerlink\" title=\"1.1.3.3. OutputDStream\"></a>1.1.3.3. <strong>OutputDStream</strong></h4><p>OutputDStream的操作最后都转换到ForEachDStream(),ForeachDStream中会生成Job并返回。</p>\n<p> <strong>伪代码</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJob</span></span>(time:<span class=\"type\">Time</span>)&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobFunc=()=&gt;crateRDD&#123;</span><br><span class=\"line\">\t\tforeachFunc(rdd,time)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"type\">Some</span>(<span class=\"keyword\">new</span> <span class=\"type\">Job</span>(time,jobFunc))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>源码</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[streaming]</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ForEachDStream</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>] (<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    parent: <span class=\"type\">DStream</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    foreachFunc: (<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], <span class=\"type\">Time</span></span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Unit</span></span></span><br><span class=\"line\"><span class=\"class\">  ) <span class=\"keyword\">extends</span> <span class=\"title\">DStream</span>[<span class=\"type\">Unit</span>](<span class=\"params\">parent.ssc</span>) </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dependencies</span></span>: <span class=\"type\">List</span>[<span class=\"type\">DStream</span>[_]] = <span class=\"type\">List</span>(parent)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">slideDuration</span></span>: <span class=\"type\">Duration</span> = parent.slideDuration</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(validTime: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">RDD</span>[<span class=\"type\">Unit</span>]] = <span class=\"type\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJob</span></span>(time: <span class=\"type\">Time</span>): <span class=\"type\">Option</span>[<span class=\"type\">Job</span>] = &#123;</span><br><span class=\"line\">    parent.getOrCompute(time) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(rdd) =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> jobFunc = () =&gt; createRDDWithLocalProperties(time) &#123;</span><br><span class=\"line\">          ssc.sparkContext.setCallSite(creationSite)</span><br><span class=\"line\">          foreachFunc(rdd, time)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"type\">Some</span>(<span class=\"keyword\">new</span> <span class=\"type\">Job</span>(time, jobFunc))</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; <span class=\"type\">None</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过对output stream节点进行遍历，就可以得到所有上游依赖的DStream,直至找到没有父依赖的inputStream。</p>\n<h2 id=\"1-2-特征\"><a href=\"#1-2-特征\" class=\"headerlink\" title=\"1.2. 特征\"></a>1.2. <strong>特征</strong></h2><p> DStream基本属性:</p>\n<ul>\n<li><p>父依赖： dependencies: List[DStream[_]]</p>\n</li>\n<li><p>时间间隔：slideDuration:Duration</p>\n</li>\n<li><p>生成RDD的函数：compute</p>\n</li>\n</ul>\n<h2 id=\"1-3-实现类\"><a href=\"#1-3-实现类\" class=\"headerlink\" title=\"1.3. 实现类\"></a>1.3. <strong>实现类</strong></h2><p>DStream的实现类可分为三种：输入，转换和输出</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA880.tmp.jpg\" alt=\"img\"> </p>\n<p>DStream之间的转换类似于RDD之间的转换，对于wordCount的例子，实现代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> lines=ssc.socketTextStream(ip,port)</span><br><span class=\"line\"><span class=\"keyword\">val</span> worlds=lines.flatMap(_.split(<span class=\"string\">\"_\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> pairs=words.map(word=&gt;(word,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> wordCounts=pairs.reduceByKey(_+_)</span><br><span class=\"line\">wordCounts.print()</span><br></pre></td></tr></table></figure>\n<p>每个函数的返回对象用具体实现代替：</p>\n <figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> lines=<span class=\"keyword\">new</span> <span class=\"type\">SocketInputDStream</span>(ip,port)</span><br><span class=\"line\"><span class=\"keyword\">val</span> words=<span class=\"keyword\">new</span> <span class=\"type\">FlatMappedDStream</span>(lines,_.split(<span class=\"string\">\"_\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> pairs=<span class=\"keyword\">new</span> <span class=\"type\">MappedDStream</span>(words,word=&gt;(word,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> wordCounts=<span class=\"keyword\">new</span> <span class=\"type\">ShuffledDStream</span>(pairs,_+_)</span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">ForeachDStream</span>(wordCounts,cnt=&gt;cnt.print())</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-1-ForeachDStream\"><a href=\"#1-3-1-ForeachDStream\" class=\"headerlink\" title=\"1.3.1. ForeachDStream\"></a>1.3.1. <strong>ForeachDStream</strong></h3><p> DStream的实现分为两种，transformation和output</p>\n<p>不同的转换操作有其对应的DStream实现，所有的output操作只对应于ForeachDStream</p>\n<h3 id=\"1-3-2-Transformed-DStream\"><a href=\"#1-3-2-Transformed-DStream\" class=\"headerlink\" title=\"1.3.2. Transformed DStream\"></a>1.3.2. <strong>Transformed DStream</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA890.tmp.jpg\" alt=\"img\"> </p>\n<h3 id=\"1-3-3-InputDStream\"><a href=\"#1-3-3-InputDStream\" class=\"headerlink\" title=\"1.3.3. InputDStream\"></a>1.3.3. <strong>InputDStream</strong></h3><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA891.tmp.jpg\" alt=\"img\"> </p>\n<h1 id=\"2-DStreamGraph\"><a href=\"#2-DStreamGraph\" class=\"headerlink\" title=\"2. DStreamGraph\"></a>2. <strong>DStreamGraph</strong></h1><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/wpsA892.tmp.jpg\" alt=\"img\"> </p>\n<h2 id=\"2-1-DAG分类\"><a href=\"#2-1-DAG分类\" class=\"headerlink\" title=\"2.1 DAG分类\"></a>2.1 <strong>DAG分类</strong></h2><ul>\n<li><p>逻辑DAG: 通过transformation操作正向生成</p>\n</li>\n<li><p>物理DAG: 惰性求值的原因，在遇到output操作时根据dependency逆向宽度优先遍历求值。</p>\n</li>\n</ul>\n<h2 id=\"2-2-DAG生成\"><a href=\"#2-2-DAG生成\" class=\"headerlink\" title=\"2.2 DAG生成\"></a>2.2 DAG生成</h2><p> <strong>DStreamGraph属性</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">inputStreams=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">InputDStream</span>[_]]()</span><br><span class=\"line\">outputStreams=<span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">DStream</span>[_]]()</span><br></pre></td></tr></table></figure>\n<p><strong>DAG实现过程</strong></p>\n<p>​    通过对output stream节点进行遍历，就可以得到所有上游依赖的DStream,直至找到没有父依赖的inputStream。</p>\n<p>​    sparkStreaming 记录整个DStream DAG的方式就是通过一个DStreamGraph 实例记录了到所有output stream节点的引用</p>\n<p><strong>generateJobs</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateJobs</span></span>(time: <span class=\"type\">Time</span>): <span class=\"type\">Seq</span>[<span class=\"type\">Job</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> jobs = <span class=\"keyword\">this</span>.synchronized &#123;</span><br><span class=\"line\">      outputStreams.flatMap &#123; </span><br><span class=\"line\">\t\toutputStream =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> jobOption = </span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 调用了foreachDStream来生成每个job</span></span><br><span class=\"line\">\t\t\toutputStream.generateJob(time)   jobOption.foreach(_.setCallSite(outputStream.creationSite))</span><br><span class=\"line\">        jobOption</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t<span class=\"comment\">// 返回生成的Job列表</span></span><br><span class=\"line\">    jobs</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p> <strong>脑图制作参考</strong>：<a href=\"https://github.com/lw-lin/CoolplaySpark\" target=\"_blank\" rel=\"noopener\">https://github.com/lw-lin/CoolplaySpark</a></p>\n<p><strong>完整脑图链接地址</strong>：<a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png\" target=\"_blank\" rel=\"noopener\">https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/srccode/spark-streaming-all.png</a></p>\n","categories":["spark"],"tags":["spark","源码解析"]},{"title":"微服务架构中的数据一致性","url":"https://sustcoder.github.io/2018/11/14/2018-11-14-Data consistency  in microservices/","content":"<p>Data <strong>consistency</strong><sub>一致性</sub> in <strong>microservices</strong><sub>微服务</sub><strong>architecture</strong><sub>架构</sub></p>\n<p>In microservices, one logically <strong>atomic operation</strong><sub>原子操作</sub> can frequently <strong>span</strong> <sub>跨越</sub>multiple microservices. Even a <strong>monolithic</strong><sub>单片的</sub> system might use multiple databases or messaging solutions. With several independent <strong>data storage solutions</strong><sub>数据存储方案</sub>, we risk inconsistent data if one of the distributed process participants fails — such as <strong>charging</strong><sub>收费</sub> a customer without placing the order or not <strong>notifying</strong> <sub>通知</sub>the customer that the order succeeded. In this article, I’d like to share some of the techniques I’ve learned for making data between microservices <strong>eventually consistent</strong><sub>最终一致性</sub></p>\n<p>在微服务中，逻辑上的原子操作经常跨越多个微服务。即使是单体应用架构下，也可能使用多个数据库或消息队列解决方案。使用几种独立的数据存储解决方案，如果分布式流程参与者之一失败，我们将面临数据不一致的风险，例如，向客户收费而不下订单，或者不通知客户订单成功。在本文中，我想分享一些在微服务体系架构下确保数据最终一致的技术。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/1.jpg\" alt=\"Distributed process failures\"></p>\n<p>Why is it so <strong>challenging</strong> <sub>挑战性</sub>to achieve this? <strong>As long as</strong> <sub>只要</sub>we have multiple places where the data is stored (which are not in a single database), consistency is not solved <strong>automatically</strong><sub>自动的</sub> and engineers need to take care of consistency while designing the system. For now, in my opinion, the <strong>industry</strong> <sub>行业</sub>doesn’t yet have a widely known solution for updating data atomically in multiple different data sources — and we probably shouldn’t wait for one to be available soon.</p>\n<p>为什么要做到这一点如此具有挑战性？只要我们有多个存储数据的地方(而不是在一个数据库中)，一致性就不会自动解决，工程师在设计系统时就需要考虑一致性。就目前而言，在我看来，业界还没有一个广为人知的解决方案来原子化地更新多个不同数据源中的数据，而且在可预见的将来，也不会有一个很快就可以使用的解决方案。</p>\n<p>One attempt to solve this problem in an automated and <strong>hassle-free</strong> <sub>无忧</sub><strong>manner</strong> <sub>方式</sub>is the XA <strong>protocol</strong> <sub>协议</sub><strong>implementing</strong><sub>实现</sub> the two-phase commit (<strong>2PC</strong><sub>两阶段提交</sub>) pattern. But in <strong>modern</strong><sub>现代的</sub> <strong>high-scale applications</strong><sub>高可用服务</sub> (especially in <strong>a cloud environment</strong><sub>云环境</sub>), 2PC doesn’t seem to perform so well. To <strong>eliminate</strong><sub>消除</sub> the disadvantages of 2PC, we have to <strong>trade</strong><sub>交易，交换</sub> ACID for BASE and <strong>cover</strong><sub>覆盖</sub> consistency concerns ourselves in different ways depending on the requirements.</p>\n<p>以自动化和省力的方式解决这个问题的一个尝试是以<a href=\"http://t.cn/EzBmvFS\" target=\"_blank\" rel=\"noopener\">XA协议</a>实现<a href=\"http://t.cn/RckficO\" target=\"_blank\" rel=\"noopener\">两阶段提交(2PC)模式</a>但是在现代大规模应用程序中(特别是在云环境中)，2PC的性能似乎不太好。为了消除2PC的缺点，我们必须牺牲ACID来遵循BASE原则，并根据需要采用不同的方式来满足数据一致性的要求</p>\n<h2 id=\"SAGA模式\"><a href=\"#SAGA模式\" class=\"headerlink\" title=\"SAGA模式\"></a>SAGA模式</h2><p>The most well-known way of <strong>handling</strong><sub>处理</sub> consistency <strong>concerns</strong><sub>忧虑，问题</sub> in multiple microservices is the Saga Pattern. You may <strong>treat</strong> <sub>看待</sub>Sagas as application-level distributed <strong>coordination</strong> <sub>协调</sub>of multiple transactions. Depending on the use-case and requirements, you <strong>optimize</strong> <sub>优化</sub>your own Saga <strong>implementation</strong>.<sub>实现</sub> <strong>In contrast</strong><sub>相反</sub>, the XA <strong>protocol</strong><sub>协议</sub> tries to cover all the <strong>scenarios</strong><sub>场景</sub>. The Saga Pattern is also not new. It was known and used in ESB and SOA architectures in the past. Finally, it successfully transitioned to the microservices world. Each <strong>atomic business operation</strong> <sub>原子操作</sub>that spans multiple services might <strong>consist of</strong><sub>由…组成</sub> multiple transactions on a technical level. The key idea of the Saga Pattern is to be able to roll back one of the individual transactions. As we know, rollback is not possible for already committed individual transactions out of the box. But this is achieved by invoking a <strong>compensation action</strong><sub>补偿措施</sub> — by introducing a “Cancel” operation.</p>\n<p>在多个微服务中处理一致性问题的最著名方法是<a href=\"http://t.cn/EzB3uQA\" target=\"_blank\" rel=\"noopener\">SAGA模式</a>可以将SAGA视为多个事务的应用程序级分布式协调机制。根据用例和需求，可以优化自己的SAGA实现，XA协议正相反，试图以通用方案涵盖所有的场景。SAGA模式也并非什么新概念。它过去在ESB和SOA体系结构中就得到认知和使用，并最终成功地向微服务世界过渡。跨多个服务的每个原子业务操作可能由一个技术级别上的多个事务组成。Saga模式的关键思想是能够回滚单个事务。正如我们所知道的，对于已经提交的单个事务来说，回滚是不可能的。但通过调用补偿行动，即通过引入“Cancel”操作可以变相的实现这一点。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/2.jpg\" alt=\"Compensating operations\"></p>\n<p>In addition to <strong>cancelation</strong><sub>取消</sub>, you should consider making your service <strong>idempotent</strong><sub>幂等</sub>, so you can retry or restart certain operations in case of failures. Failures should be monitored and reaction to failures should be <strong>proactive</strong><sub>主动的</sub>.</p>\n<p>除了取消操作之外，还需要考虑使服务的幂等性，以便在发生故障时可以重新尝试或重新启动某些操作。应该对失败进行监测，对失败的反应应该积极主动。</p>\n<h3 id=\"Reconciliation-对账\"><a href=\"#Reconciliation-对账\" class=\"headerlink\" title=\"Reconciliation 对账\"></a>Reconciliation 对账</h3><p><strong>What if</strong><sub>如果</sub> in the middle of the process the system responsible for calling a compensation action crashes or restarts. In this case, the user may receive an error message and the compensation logic should be triggered or — when processing <strong>asynchronous</strong> <sub>异步</sub>user requests, the <strong>execution logic</strong><sub>执行逻辑</sub> should be <strong>resumed</strong><sub>恢复，重新开始</sub>.</p>\n<p>如果在进程中间，负责调用补偿操作的系统崩溃或重新启动怎么办？在这种情况下，用户可能会收到错误消息，触发补偿逻辑，在处理异步用户请求时，重试执行逻辑。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/3.jpg\" alt=\"main process failure\"></p>\n<p>To find <strong>crashed</strong><sub>崩溃</sub> transactions and resume operation or apply compensation, we need to <strong>reconcile</strong> <sub>协调</sub>data from multiple services. Reconciliation is a technique familiar to engineers who have worked in the financial domain. Did you ever wonder how banks make sure your money transfer didn’t get lost or how money transfer happens between two different banks in general? The quick answer is reconciliation.</p>\n<p>要查找崩溃的事务并恢复操作或应用补偿，我们需要协调来自多个服务的数据。对从事金融领域工作的工程师来说，对账是一种熟悉的技术。你有没有想过，银行如何确保你的汇款不会丢失，或者在两家不同的银行之间是如何发生转账的？快速的答案是对账。</p>\n<p>In <strong>accounting</strong><sub>会计</sub>, reconciliation is the process of ensuring that two sets of records (usually the <strong>balances</strong><sub>余额</sub> of two accounts) are in <strong>agreement</strong><sub>一致</sub>. Reconciliation is used to ensure that the money leaving an account matches the actual money spent. This is done by making sure the balances match at the end of a particular accounting period </p>\n<p>在会计领域，对账是确保两套记录(通常是两个账户的余额)一致的过程。对账手段确保离开帐户的钱与实际花费的钱相符。这是通过确保在特定会计期间结束时的余额匹配来实现的。</p>\n<p>Coming back to microservices, using the same principle we can reconcile data from multiple services on some action trigger. Actions could be triggered on a scheduled basis or by a monitoring system when failure is <strong>detected</strong><sub>检测到</sub>. The simplest <strong>approach</strong><sub>途径方法</sub> is to run a <strong>record-by-record comparison</strong><sub>逐条比较</sub>. This process could be optimized by comparing aggregated values. In this case, one of the systems will be a <em>source of truth</em> for each record.</p>\n<p>回到微服务方面，使用相同的理念，我们可以在某些操作触发器上协调来自多个服务的数据。可以按计划执行对账操作，也可以在检测到出状况时，由监视系统触发相关操作。最简单的方法是按记录逐条进行比较，当然，也可以通过比较汇总值来优化此过程。在这种情况下，某个系统的数据将成为基准数据来对每条数据进行比对。</p>\n<h2 id=\"Event-log-日志事件\"><a href=\"#Event-log-日志事件\" class=\"headerlink\" title=\"Event log 日志事件\"></a>Event log 日志事件</h2><p><strong>Imagine</strong><sub>设想</sub> <strong>multistep transactions</strong><sub>多重事物</sub>. How to determine during reconciliation which transactions might have failed and which steps have failed? One solution is to check the status of each transaction. In some cases, this functionality is not <strong>available</strong><sub>可达的，可用的</sub> (imagine a stateless mail service that sends email or produces other kinds of messages). In some other <strong>cases</strong><sub>场景案例</sub>, you might want to get immediate visibility on the transaction state, especially in <strong>complex scenarios</strong><sub>复杂场景</sub> with many steps. For example, a multistep order with booking flights, hotels, and transfers.</p>\n<p>再来讨论多步事务的情况。如何确定在对账过程中哪些事务在哪些环节上失败了？一种解决方案是检查每个事务的状态。在某些情况下，这个方法并不适用（比如无状态的邮件服务发送电子邮件或生成其他类型的消息）。在其他一些情况下，我们可能希望获得事务状态的即时可见性（也就是实时知晓其当前的状态），特别是在具有多个步骤的复杂场景中。例如，一个多步骤的订单，包括预订航班、酒店和转乘。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/5.jpg\" alt=\"main process failure\"></p>\n<p>In these situations, an event log can help. Logging is a simple but powerful technique. Many distributed systems rely on logs. “Write-ahead logging” is how databases achieve transactional behavior or maintain consistency between replicas <strong>internally</strong><sub>内部的</sub>. The same technique could be applied to microservices design. Before making an actual data change, the service writes a log entry about its <strong>intent</strong><sub>意图</sub> to make a change. In practice, the event log could be a table or a collection inside a database owned by the <strong>coordinating</strong><sub>协调</sub> service.</p>\n<p>在这种情况下，事件日志可能会有所帮助。日志记录是一种简单但功能强大的技术。许多分布式系统依赖日志。“预写日志“就是在数据库内部实现事务行为或保持副本之间的一致性的方法。同样的技术也可以应用于微服务设计。在进行实际的数据更改之前，服务会编写一个日志条目，对即将实施的更改操作进行说明。实现方式上，事件日志是从属于协调服务的数据库中的表或集合。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/6.jpg\" alt=\"sample event log\"></p>\n<p>The event log could be used not only to resume transaction processing but also to provide visibility to system users, customers, or to the support team. However, in simple scenarios a service log might be <strong>redundant</strong><sub>多余的</sub> and status <strong>endpoints</strong> <sub>状态节点</sub>or status fields be enough.</p>\n<p>事件日志不仅可用于恢复事务处理，还可用于向系统用户、客户或支持团队提供可见性。但是，在简单的场景中，服务日志可能是多余的，状态端点或状态字段就足够了。</p>\n<h3 id=\"Orchestration-vs-choreography-编曲和编舞\"><a href=\"#Orchestration-vs-choreography-编曲和编舞\" class=\"headerlink\" title=\"Orchestration vs. choreography 编曲和编舞\"></a>Orchestration vs. choreography 编曲和编舞</h3><p>By this point, you might think sagas are only a part of orchestration scenarios. But sagas can be used in choreography as well, where each microservice knows only a part of the process. Sagas include the knowledge on handling both <strong>positive</strong><sub>正向积极</sub> and <strong>negative</strong><sub>负向，否定</sub> flows of distributed transaction. In choreography, each of the distributed transaction participants has this kind of knowledge.</p>\n<p>至此，您可能会认为SAGA只适用于编曲场景的一部分。但是SAGA也可以用于编舞场景，每个微服务只知道其中的一部分。SAGA内置了处理分布式事务的正向流和负向流的相关机制。在编舞场景中，每个分布式事务参与者都有这样的知识。</p>\n<h3 id=\"Single-write-with-events\"><a href=\"#Single-write-with-events\" class=\"headerlink\" title=\"Single-write with events\"></a>Single-write with events</h3><p>单一写入事件</p>\n<p>The consistency solutions described <strong>so far</strong><sub>到目前为止</sub> are not easy. They are <strong>indeed</strong><sub>确实</sub> complex. But there is a simpler way: modifying a single datasource at a time.Instead of changing the state of the service and <strong>emitting</strong><sub>触发</sub> the event in one process, we could separate those two steps.</p>\n<p>到目前为止，上述的一致性解决方案并不容易。它们确实很复杂。但有一种更简单的方法：每次只修改一个数据源。我们可以将更改服务的状态并发出事件这两个步骤分开，而不是在一个进程中处理。</p>\n<p><strong>Change-first “变更优先”原则</strong></p>\n<p>In a main business operation, we modify our own state of the service while a separate process <strong>reliably</strong> <sub>可靠地</sub>captures the change and <strong>produces</strong> <sub>生成</sub>the event. This technique is known as <em>Change Data Capture (CDC)</em>. Some of the technologies implementing this approach are Kafka Connect or Debezium.</p>\n<p>在主要业务操作中，我们可以修改自己服务的状态，同时由一个单独的进程捕获相关变更并生成事件。这种技术被称为变更数据捕获(CDC)。实现此方法的一些技术包括<a href=\"http://t.cn/EzrZOpE\" target=\"_blank\" rel=\"noopener\"> Kafka Connect</a>或 <a href=\"https://debezium.io/\" target=\"_blank\" rel=\"noopener\">Debezium</a>。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/7.jpg\" alt=\"Change Data Capture with Debezium and Kafka Connect\"></p>\n<p>However, sometimes no specific framework is required. Some databases offer a friendly way to <strong>tail</strong> <sub>跟踪</sub>their operations log, such as MongoDB Oplog. If there is no such functionality in the database, changes can be polled by timestamp or queried with the last processed ID for immutable records. The key to avoiding inconsistency is making the data change notification a separate process. The database record is in this case the <em>single source of truth</em>. A change is only captured if it happened in the first place.</p>\n<p>然而，有时不需要特定的框架来进行处理。一些数据库提供了一种跟踪操作日志的友好方法，如<a href=\"http://t.cn/Ezrw6xj\" target=\"_blank\" rel=\"noopener\">MongoDB Oplog</a>。如果数据库中没有这样的功能，则可以使用时间戳轮询更改，或者使用最后处理的ID查询不可变记录。避免不一致的关键是使数据更改通知成为一个单独的进程。数据库记录在这种情况下为基准数据。一旦发生数据变更，相关数据即被捕获和记录。</p>\n<p>  <img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/8.jpg\" alt=\"Change Data Capture without specific tools\"></p>\n<p>The biggest <strong>drawback</strong><sub>退税，缺陷</sub> of change data capture is the separation of business logic. Change capture <strong>procedures</strong><sub>程序</sub> will most likely live in your <strong>codebase</strong><sub>代码库</sub> separate from the change logic itself — which is <strong>inconvenient</strong><sub>不方便</sub>. The most well-known application of change data capture is domain-agnostic change replication such as sharing data with a <strong>data warehouse</strong><sub>数仓</sub>. For domain events, it’s better to employ a different <strong>mechanism</strong><sub>机制</sub> such as sending events <strong>explicitly</strong><sub>显式地，明确的</sub>.</p>\n<p>变更数据捕获的最大缺点是业务逻辑的分离。更改捕获过程很可能存在于您的代码库中，与更改逻辑本身分离，这是不方便的（所谓的不方便，我的理解，不是指更改捕获过程与业务逻辑的分离，而是指用户需要为每个业务逻辑单独的实现更改捕获逻辑）。最广为人知的更改数据捕获应用场景是领域无关的更改复制，例如通过数据仓库进行数据共享。对于域内的事件，最好使用不同的机制，比如显式地发送事件。</p>\n<h3 id=\"Event-first-“事件优先”原则\"><a href=\"#Event-first-“事件优先”原则\" class=\"headerlink\" title=\"Event-first “事件优先”原则\"></a>Event-first “事件优先”原则</h3><p>Let’s look at <strong>the <em>single source of truth</em><sub>唯一来源，基础数据</sub></strong> <strong>upside down</strong><sub>逆向</sub>. What if <strong>instead of</strong><sub>而不是</sub> writing to the database first we trigger an event instead and share it with ourselves and with other services. In this case, the event <strong>becomes</strong><sub>成为</sub> the single source of truth. This would be a form of event-sourcing where the state of our own service effectively becomes a read model and each event is a write model.</p>\n<p>让我们对“基准数据”做一个逆向思考。如果我们不是首先写入数据库，而是先触发一个事件并与我们自己和其他服务共享这个事件呢？在这种情况下，事件成为基准数据。这将是一种 event-sourcing的形态，在这种情况下，服务状态实际上变成了一个读模型，而每个事件都是一个写模型。</p>\n<p>  <img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/9.jpg\" alt=\"Event-first approach\"></p>\n<p>On the one hand, it’s a <strong>command query responsibility segregation</strong><sub>查询责任分离</sub> (CQRS) <strong>pattern</strong><sub>模式</sub> where we separate the read and write models, but CQRS by itself doesn’t focus on the most important part of the solution — consuming the events with multiple services.</p>\n<p>所以，这也是一种命令查询责任分离(CQRS)模式，将读写模型分离开来，但是CQRS本身并没有关注解决方案中最重要的部分，即如何由多个服务来对事件进行处理。</p>\n<p>In contrast, event-driven architectures focus on events consumed by multiple systems but don’t <strong>emphasize</strong> <sub>强调</sub>the fact that events are the only atomic pieces of data update. So I’d like to introduce “e<em>vent-first”</em> as a name to this approach: updating the internal state of the microservice by emitting a single event — both to our own service and any other interested microservices.</p>\n<p>相反，事件驱动体系结构关注多个系统对事件的处理，但不突出强调事件是数据更新的基准数据。所以我想引入 “事件优先”原则作为此方法的名称：通过发出单个事件来更新微服务的内部状态-包括对我们自己的服务和任何其他感兴趣的微服务。</p>\n<p>The challenges with an “event-first” approach are also the challenges of CQRS itself. Imagine that before making an order we want to check item availability. What if two instances concurrently receive an order of the same item? Both will concurrently check the inventory in a read model and <strong>emit</strong><sub>发出，触发</sub> an order event. Without some sort of <strong>covering</strong> <sub>覆盖</sub>scenario we could run into troubles.</p>\n<p>采用“事件优先”方法的挑战也是CQRS本身的挑战。想象一下，在下订单之前，我们要检查商品的可用性。如果两个实例同时接收同一项的订单怎么办？两个实例将以读取模型同时检查库存，并触发一个订单事件。如果不解决这个问题，我们可能会遇到麻烦。</p>\n<p>The usual way to handle these cases is <strong>optimistic</strong><sub>乐观</sub> <strong>concurrency</strong><sub>并发</sub>: to <strong>place</strong><sub>放置</sub> a read model version into the event and ignore it on the consumer side if the read model was already updated on consumer side. The other solution would be using <strong>pessimistic</strong><sub>悲观</sub> concurrency control, such as creating a lock for an item while we check its availability.</p>\n<p>处理这些情况的通常方法是乐观并发：在事件中放置一个读取模型版本，如果已在使用者端更新读取模型，则忽略这个读取操作。另一种解决方案是使用悲观的并发控制，例如在查询项目可用性时为其创建锁。</p>\n<p>The other challenge of the “event-first” approach is a challenge of any event-driven architecture — <strong>the order of events</strong><sub>时间顺序</sub>. Processing events in the wrong order by multiple concurrent consumers might give us another kind of consistency issue, for example processing an order of a customer who hasn’t been created yet.</p>\n<p>“事件优先”方法的另一个挑战是对任何事件驱动的体系结构的挑战，即事件的顺序。多个并发消费者以错误的顺序处理事件可能会给我们带来另一种一致性问题，例如，处理尚未创建的客户的订单。</p>\n<p>Data streaming solutions such as Kafka or AWS Kinesis can <strong>guarantee</strong><sub>保证</sub> that events related to a single entity will be processed sequentially (such as creating an order for a customer only after the user is created). In Kafka for example, you can partition topics by user ID so that all events related to a single user will be processed by a single consumer assigned to the partition, thus allowing them to be processed sequentially. In contrast, in Message Brokers, message queues have an order but multiple concurrent consumers make message processing in a given order hard, <strong>if not impossible</strong><sub>甚至不可能</sub>. In this case, your could run into concurrency issues.</p>\n<p>数据流解决方案(如Kafka或AWS Kinesis)可以保证与单个实体相关的事件将按顺序处理(例如，只在创建用户之后才为客户创建订单)。例如，在Kafka中，您可以通过用户ID对主题进行分区，这样与单个用户相关的所有事件都将由分配给该分区的单个使用者处理，从而允许按顺序处理这些事件。相反，在使用消息代理机制时，消息队列虽然有其固有的执行顺序，但多个并发使用者使得按给定顺序进行消息处理非常困难，甚至不可能。这样就可能会遇到并发问题。</p>\n<p>In practice, an “event-first” approach is hard to implement in scenarios when <strong>linearizability</strong><sub>线性化</sub> is required or in scenarios with many data <strong>constraints</strong><sub>约束</sub> such as uniqueness checks. But it really <strong>shines</strong> <sub>发光，大放异彩</sub>in other scenarios. However, due to its asynchronous nature, challenges with concurrency and race conditions still need to be <strong>addressed</strong><sub>解决</sub>.</p>\n<p>实际上，当<strong>线性一致性</strong>是必需的，或者在有许多数据约束(如唯一性检查)的情况下，“事件优先”方法很难实现。但在其他场景中但它确实可以大放异彩。然而，由于它的异步性质，并发和竞争条件的挑战仍然需要解决。</p>\n<h2 id=\"Consistency-by-design\"><a href=\"#Consistency-by-design\" class=\"headerlink\" title=\"Consistency by design\"></a>Consistency by design</h2><p>设计一致性</p>\n<p>There many ways to split the system into multiple services. We <strong>strive</strong> <sub>努力</sub>to match separate microservices with separate domains. But how <strong>granular</strong><sub>粒度</sub> are the domains? Sometimes it’s hard to <strong>differentiate</strong> <sub>区分</sub>domains from subdomains or aggregation roots. There is no simple rule to define your microservices split.</p>\n<p>有许多方法可以将系统分成多个服务。我们努力将不同的微服务与不同的域相匹配。但是这些域有多细呢？有时很难将域与子域或聚合根区分开来。没有简单的规则来定义您的微服务拆分。</p>\n<p><strong>Rather than</strong><sub>与其</sub> focusing only on domain-driven design, I suggest to be <strong>pragmatic</strong><sub>务实</sub> and consider all the implications of the design options. One of those implications is how well microservices <strong>isolation</strong> <sub>独立</sub><strong>aligns with</strong><sub>对齐</sub> the transaction boundaries. A system where transactions only reside within microservices doesn’t require any of the solutions above. We should definitely consider the <strong>transaction boundaries</strong><sub>事物边界</sub> while designing the system. In practice, it might be hard to design the whole system in this manner, but I think we should aim to <strong>minimize</strong><sub>最小化</sub> data consistency challenges.</p>\n<p>与其只关注领域驱动的设计，我建议采取务实的态度，并考虑所有设计选项的含义。其中一个含义是微服务隔离与事务边界的匹配程度。事务只驻留在微服务中的系统不需要上述任何解决方案。在设计系统时，一定要考虑事务边界。在实践中，可能很难以这种方式设计整个系统，但我认为我们的目标应该是尽量减少数据一致性的挑战。</p>\n<h4 id=\"Accepting-inconsistency-接受不一致\"><a href=\"#Accepting-inconsistency-接受不一致\" class=\"headerlink\" title=\"Accepting inconsistency 接受不一致\"></a>Accepting inconsistency 接受不一致</h4><p>While it’s <strong>crucial</strong><sub>重要</sub> to match the account balance, there are many use cases where consistency is much less important. Imagine <strong>gathering</strong><sub>收集</sub> data for analytics or statistics purposes. Even if we lose 10% of data from the system randomly, most likely the business value from analytics won’t be affected.</p>\n<p>虽然与帐户余额匹配是至关重要的，但在许多用例中，一致性的重要性要小得多。比如，为分析或统计目的收集数据。即使我们随机丢失了10%的系统数据，从分析中获得的业务价值也很可能不会受到影响。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/distribute/10.jpg\" alt=\"Sharing data with events\"></p>\n<h2 id=\"Which-solution-to-choose\"><a href=\"#Which-solution-to-choose\" class=\"headerlink\" title=\"Which solution to choose\"></a>Which solution to choose</h2><p><strong>选择哪种解决方案</strong></p>\n<p>Atomic update of data requires a <strong>consensus</strong><sub>共识</sub> between two different systems, an <strong>agreement</strong> <sub>协议</sub>if a single value is 0 or 1. When it comes to microservices, it <strong>comes down</strong><sub>归结</sub> to problem of consistency between two <strong>participants</strong><sub>参与者</sub> and all practical solutions follow a single <strong>rule of thumb</strong><sub>经验法则</sub>:</p>\n<p>数据的原子更新需要两个不同系统之间的协商一致，形成对某值为0或者为1的共识。当涉及到微服务时，它归结为两个参与者之间的一致性问题，所有实际的解决方案都遵循一个经验法则：</p>\n<blockquote>\n<p>In a given moment, for each data record, you need to find which data source is trusted by your system</p>\n<p>在给定的时刻，对于每个数据记录，需要找到可信的基准数据</p>\n</blockquote>\n<p>The source of truth could be events, the database or one of the services. Achieving consistency in microservice systems is developers’ responsibility. My approach is the following:</p>\n<ol>\n<li><p>Try to design a system that doesn’t require distributed consistency. Unfortunately, that’s <strong>barely possible</strong><sub>几乎不可能</sub> for complex systems.</p>\n</li>\n<li><p>Try to reduce the number of <strong>inconsistencies</strong> <sub>不一致性</sub>by modifying one data source at a time.</p>\n</li>\n<li><p>Consider event-driven architecture. A big strength of event-driven architecture in addition to <strong>loose coupling</strong><sub>松耦合</sub> is a natural way of achieving data consistency by having events as a single source of truth or producing events as a result of change data capture.</p>\n</li>\n<li><p>More complex scenarios might still require synchronous calls between services, failure handling and compensations. Know that sometimes you may have to reconcile afterwards.</p>\n</li>\n<li><p>Design your service capabilities to be <strong>reversible</strong>可逆的, decide how you will handle failure scenarios and achieve consistency early in the design phase.</p>\n</li>\n</ol>\n<p>基准数据可以是事件、数据库或某个服务。在微服务系统中实现一致性是开发人员的责任。我的做法如下：</p>\n<ol>\n<li><p>尝试设计一个不需要分布式一致性的系统。不幸的是，对于复杂的系统来说，这几乎是不可能的。</p>\n</li>\n<li><p>尝试通过一次修改一个数据源来减少不一致的数量。</p>\n</li>\n<li><p>考虑一下事件驱动的体系结构。除了松散耦合之外，事件驱动体系结构的一大优势是天然的支持基于事件的数据一致性，可以将事件作为基准数据，也可以由变更数据捕获（CDC）生成事件。</p>\n</li>\n<li><p>更复杂的场景可能仍然需要服务、故障处理和补偿之间的同步调用。要知道，有时你可能不得不在事后对账。</p>\n</li>\n<li><p>将您的服务功能设计为可逆的，决定如何处理故障场景，并在设计阶段早期实现一致性。</p>\n</li>\n</ol>\n<p><strong>参考</strong>：</p>\n<p>英文：<a href=\"https://ebaytech.berlin/data-consistency-in-microservices-architecture-bf99ba31636f\" target=\"_blank\" rel=\"noopener\">https://ebaytech.berlin/data-consistency-in-microservices-architecture-bf99ba31636f</a></p>\n<p>翻译：<a href=\"https://mp.weixin.qq.com/s/nFHkvwSmjDd9ruKHH4eomA\" target=\"_blank\" rel=\"noopener\">https://mp.weixin.qq.com/s/nFHkvwSmjDd9ruKHH4eomA</a></p>\n","categories":["translate"],"tags":["translate","microservices"]},{"title":"BitMap的JAVA实现","url":"https://sustcoder.github.io/2018/10/23/2018-10-23-bitMap-explain-and-use/","content":"<h2 id=\"相关概念\"><a href=\"#相关概念\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h2><h3 id=\"基础类型\"><a href=\"#基础类型\" class=\"headerlink\" title=\"基础类型\"></a>基础类型</h3><p>在java中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">byte  -&gt;   8 bits  --&gt;1字节</span><br><span class=\"line\">char  -&gt;   16 bit  --&gt;2字节</span><br><span class=\"line\">short -&gt;   16 bits --&gt;2字节</span><br><span class=\"line\">int   -&gt;   32 bits --&gt;4字节</span><br><span class=\"line\">float -&gt;   32 bits --&gt;4字节</span><br><span class=\"line\">long  -&gt;   64 bits --&gt;8字节</span><br></pre></td></tr></table></figure>\n<h3 id=\"位运算符\"><a href=\"#位运算符\" class=\"headerlink\" title=\"位运算符\"></a>位运算符</h3><p>在java中，int数据底层以补码形式存储。int型变量使用32bit存储数据，其中最高位是符号位，0表示正数，1表示负数，可通过<code>Integer.toBinaryString()</code>转换为bit字符串，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 若最高的几位为0则不输出这几位，从为1的那一位开始输出</span><br><span class=\"line\">System.out.println(Integer.toBinaryString(10)); </span><br><span class=\"line\">System.out.println(Integer.toBinaryString(-10));</span><br><span class=\"line\">// 会输出（手工排版过，以下的输出均会被手工排版）：</span><br><span class=\"line\">                            1010</span><br><span class=\"line\">11111111111111111111111111110110</span><br></pre></td></tr></table></figure>\n<p><strong>左移&lt;&lt;</strong></p>\n<blockquote>\n<p>5&lt;&lt;2=20</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">首先会将5转为2进制表示形式: 0000 0000 0000 0000 0000 0000 0000 0101  </span><br><span class=\"line\">然后左移2位后，低位补0：    0000 0000 0000 0000 0000 0000 0001 0100  </span><br><span class=\"line\">换算成10进制为20</span><br></pre></td></tr></table></figure>\n<p><strong>右移&gt;&gt;</strong></p>\n<blockquote>\n<p>5&gt;&gt;2=1</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">还是先将5转为2进制表示形式：0000 0000 0000 0000 0000 0000 0000 0101 </span><br><span class=\"line\">然后右移2位，高位补0：     0000 0000 0000 0000 0000 0000 0000 0001</span><br><span class=\"line\">换算成十进制后是1</span><br></pre></td></tr></table></figure>\n<p><strong>无符号右移&gt;&gt;&gt;</strong></p>\n<blockquote>\n<p>5&gt;&gt;&gt;3</p>\n</blockquote>\n<p>我们知道在Java中int类型占32位，可以表示一个正数，也可以表示一个负数。正数换算成二进制后的最高位为0，负数的二进制最高为为1。<strong>对于2进制补码的加法运算，和平常的计算一样，而且符号位也参与运算，不过最后只保留32位。</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">-5换算成二进制： 1111 1111 1111 1111 1111 1111 1111 1011</span><br><span class=\"line\">-5右移3位：     1111 1111 1111 1111 1111 1111 1111 1111   // (用1进行补位，结果为-1)</span><br><span class=\"line\">-5无符号右移3位: 0001 1111 1111 1111 1111 1111 1111 1111   // (用0进行补位,结果536870911 )</span><br></pre></td></tr></table></figure>\n<p><strong>位与</strong>&amp;</p>\n<p>第一个操作数的的第n位于第二个操作数的第n位如果都是1，那么结果的第n为也为1，否则为0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101</span><br><span class=\"line\">3转换为二进制：0000 0000 0000 0000 0000 0000 0000 0011</span><br><span class=\"line\">------------------------------------------------------------</span><br><span class=\"line\">1转换为二进制：0000 0000 0000 0000 0000 0000 0000 0001</span><br></pre></td></tr></table></figure>\n<p><strong>位或|</strong></p>\n<p>第一个操作数的的第n位于第二个操作数的第n位只要有一个为1则为1，否则为0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101</span><br><span class=\"line\">3转换为二进制：0000 0000 0000 0000 0000 0000 0000 0011</span><br><span class=\"line\">-------------------------------------------------------------------------------------</span><br><span class=\"line\">6转换为二进制：0000 0000 0000 0000 0000 0000 0000 0111</span><br></pre></td></tr></table></figure>\n<p>对于移位运算，例如将x左移/右移n位，如果x是byte、short、char、int，<strong>n会先模32（即n=n%32），然后再进行移位操作</strong>。可以这样解释：int类型为32位,移动32位（或以上）没有意义。</p>\n<p>同理若x是long，n=n%64。</p>\n<p><strong>左移和右移代替乘除</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">a=a*4;</span><br><span class=\"line\">b=b/4;</span><br></pre></td></tr></table></figure>\n<p>　可以改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">a=a&lt;&lt;2;</span><br><span class=\"line\">b=b&gt;&gt;2;</span><br></pre></td></tr></table></figure>\n<p>　　<strong>说明：</strong><br>　　除2 = 右移1位 乘2 = 左移1位<br>　　除4 = 右移2位 乘4 = 左移2位<br>　　除8 = 右移3位 乘8 = 左移3位<br>　　… …<br>　　类比十进制中的满十进一，向左移动小数点后，数字就会缩小十倍，在二进制中满二进一，进行右移一次相当于缩小了2两倍，右移两位相当于缩小了4倍，右移三位相当于缩小了8倍。通常如果需要乘以或除以2的n次方，都可以用移位的方法代替。 　　<br>　　<strong>实际上，只要是乘以或除以一个整数，均可以用移位的方法得到结果，</strong>如：<br>　　a=a<em>9<br>　　分析a</em>9可以拆分成a<em>(8+1)即a</em>8+a<em>1, 因此可以改为： a=(a&lt;&lt;3)+a<br>　　a=a</em>7<br>　　分析a<em>7可以拆分成a</em>(8-1)即a<em>8-a</em>1, 因此可以改为： a=(a&lt;&lt;3)-a<br>　　关于除法读者可以类推, 此略。<br>　　【注意】由于+/-运算符优先级比移位运算符高，所以在写公式时候一定要记得添加括号，不可以 a = a*12 等价于 a = a&lt;&lt;3 +a &lt;&lt;2; 要写成a = (a&lt;&lt;3)+(a &lt;&lt;2 )。</p>\n<p><strong>与运算代替取余</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">31转换为二进制：011111,0,31</span><br><span class=\"line\">32转换为二进制：100010 与31取交集的结果是：10转换为十进制为2</span><br><span class=\"line\">31转换为二进制：100001 与31取交集的结果是：01转换为十进制为1</span><br><span class=\"line\">30转换为二进制：011110 与31取交集的结果是：11110转换为十进制为30</span><br><span class=\"line\">29转换为二进制：011101 与31取交集的结果是：11101转换为十进制为29</span><br><span class=\"line\">33转换为二进制：100001 与31取交集的结果是：1转换为十进制为1</span><br></pre></td></tr></table></figure>\n<p>31转换为二进制后，低位值全部为1，高位全为0。所以和其进行与运算，高位和0与，结果是0，相当于将高位全部截取，截取后的结果肯定小于等于31，地位全部为1，与1与值为其本身，所以相当于对数进行了取余操作。</p>\n<h3 id=\"进制转换\"><a href=\"#进制转换\" class=\"headerlink\" title=\"进制转换\"></a>进制转换</h3><ul>\n<li><code>0x</code>开头表示16进制，例如：0x2表示：2，0x2f表示48</li>\n<li><code>0</code>开头表示8进制，例如：02表示：2,010表示：8</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Integer.toHexString(int i)   // 十进制转成十六进制</span><br><span class=\"line\">Integer.toOctalString(int i) // 十进制转成八进制 </span><br><span class=\"line\">Integer.toBinaryString(int i)// 十进制转成二进制</span><br><span class=\"line\">Integer.valueOf(m,n).toString() // 把n进制的m转换为10进制</span><br></pre></td></tr></table></figure>\n<h2 id=\"BitMap实现原理\"><a href=\"#BitMap实现原理\" class=\"headerlink\" title=\"BitMap实现原理\"></a>BitMap实现原理</h2><p>在java中，一个int类型占32个字节，我们用一个int数组来表示时未new int[32],总计占用内存32*32bit,现假如我们用int字节码的每一位表示一个数字的话，那么32个数字只需要一个int类型所占内存空间大小就够了，这样在大数据量的情况下会节省很多内存。</p>\n<p>具体思路：</p>\n<p>   1个int占4字节即4*8=32位，那么我们只需要申请一个int数组长度为 int tmp[1+N/32]即可存储完这些数据，其中N代表要进行查找的总数，tmp中的每个元素在内存在占32位可以对应表示十进制数0~31,所以可得到BitMap表:</p>\n<p>tmp[0]:可表示0~31</p>\n<p>tmp[1]:可表示32~63</p>\n<p>tmp[2]可表示64~95</p>\n<p>…….</p>\n<p>那么接下来就看看十进制数如何转换为对应的bit位：</p>\n<p>假设这40亿int数据为：6,3,8,32,36,……，那么具体的BitMap表示为：</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/algorithm/bitMap.jpg\" alt=\"bitMap.jpg\"></p>\n<p>如何判断int数字在tmp数组的哪个下标，这个其实可以通过直接除以32取整数部分，例如：整数8除以32取整等于0，那么8就在tmp[0]上。另外，我们如何知道了8在tmp[0]中的32个位中的哪个位，这种情况直接mod上32就ok，又如整数8，在tmp[0]中的第8 mod上32等于8，那么整数8就在tmp[0]中的第八个bit位（从右边数起）。</p>\n<h2 id=\"BitMap源码\"><a href=\"#BitMap源码\" class=\"headerlink\" title=\"BitMap源码\"></a>BitMap源码</h2><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">long</span> length;</span><br><span class=\"line\">   <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">int</span>[] bitsMap;</span><br><span class=\"line\">   <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span>[] BIT_VALUE = &#123;<span class=\"number\">0x00000001</span>, <span class=\"number\">0x00000002</span>, <span class=\"number\">0x00000004</span>, <span class=\"number\">0x00000008</span>, <span class=\"number\">0x00000010</span>, <span class=\"number\">0x00000020</span>,</span><br><span class=\"line\">           <span class=\"number\">0x00000040</span>, <span class=\"number\">0x00000080</span>, <span class=\"number\">0x00000100</span>, <span class=\"number\">0x00000200</span>, <span class=\"number\">0x00000400</span>, <span class=\"number\">0x00000800</span>, <span class=\"number\">0x00001000</span>, <span class=\"number\">0x00002000</span>, <span class=\"number\">0x00004000</span>,</span><br><span class=\"line\">           <span class=\"number\">0x00008000</span>, <span class=\"number\">0x00010000</span>, <span class=\"number\">0x00020000</span>, <span class=\"number\">0x00040000</span>, <span class=\"number\">0x00080000</span>, <span class=\"number\">0x00100000</span>, <span class=\"number\">0x00200000</span>, <span class=\"number\">0x00400000</span>, <span class=\"number\">0x00800000</span>,</span><br><span class=\"line\">           <span class=\"number\">0x01000000</span>, <span class=\"number\">0x02000000</span>, <span class=\"number\">0x04000000</span>, <span class=\"number\">0x08000000</span>, <span class=\"number\">0x10000000</span>, <span class=\"number\">0x20000000</span>, <span class=\"number\">0x40000000</span>, <span class=\"number\">0x80000000</span>&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">BitMap2</span><span class=\"params\">(<span class=\"keyword\">long</span> length)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"keyword\">this</span>.length = length;</span><br><span class=\"line\">       <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">        * 根据长度算出，所需数组大小</span></span><br><span class=\"line\"><span class=\"comment\">        * 当 length%32=0 时大小等于</span></span><br><span class=\"line\"><span class=\"comment\">        * = length/32</span></span><br><span class=\"line\"><span class=\"comment\">        * 当 length%32&gt;0 时大小等于</span></span><br><span class=\"line\"><span class=\"comment\">        * = length/32+l</span></span><br><span class=\"line\"><span class=\"comment\">        */</span></span><br><span class=\"line\">       bitsMap = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[(<span class=\"keyword\">int</span>) (length &gt;&gt; <span class=\"number\">5</span>) + ((length &amp; <span class=\"number\">31</span>) &gt; <span class=\"number\">0</span> ? <span class=\"number\">1</span> : <span class=\"number\">0</span>)];</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">    * <span class=\"doctag\">@param</span> n 要被设置的值为n</span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setN</span><span class=\"params\">(<span class=\"keyword\">long</span> n)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (n &lt; <span class=\"number\">0</span> || n &gt; length) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"length value \"</span>+n+<span class=\"string\">\" is  illegal!\"</span>);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"comment\">// 求出该n所在bitMap的下标,等价于\"n/5\"</span></span><br><span class=\"line\">       <span class=\"keyword\">int</span> index = (<span class=\"keyword\">int</span>) n&gt;&gt;<span class=\"number\">5</span>;</span><br><span class=\"line\">       <span class=\"comment\">// 求出该值的偏移量(求余),等价于\"n%31\"</span></span><br><span class=\"line\">       <span class=\"keyword\">int</span> offset = (<span class=\"keyword\">int</span>) n &amp; <span class=\"number\">31</span>;</span><br><span class=\"line\">       <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">        * 等价于</span></span><br><span class=\"line\"><span class=\"comment\">        * int bits = bitsMap[index];</span></span><br><span class=\"line\"><span class=\"comment\">        * bitsMap[index]=bits| BIT_VALUE[offset];</span></span><br><span class=\"line\"><span class=\"comment\">        * 例如,n=3时,设置byte第4个位置为1 （从0开始计数，bitsMap[0]可代表的数为：0~31，从左到右每一个bit位表示一位数）</span></span><br><span class=\"line\"><span class=\"comment\">        * bitsMap[0]=00000000 00000000 00000000 00000000  |  00000000 00000000 00000000 00001000=00000000 00000000 00000000 00000000 00001000</span></span><br><span class=\"line\"><span class=\"comment\">        * 即: bitsMap[0]= 0 | 0x00000008 = 3</span></span><br><span class=\"line\"><span class=\"comment\">        *</span></span><br><span class=\"line\"><span class=\"comment\">        * 例如,n=4时,设置byte第5个位置为1</span></span><br><span class=\"line\"><span class=\"comment\">        * bitsMap[0]=00000000 00000000 00000000 00001000  |  00000000 00000000 00000000 00010000=00000000 00000000 00000000 00000000 00011000</span></span><br><span class=\"line\"><span class=\"comment\">        * 即: bitsMap[0]=3 | 0x00000010 = 12</span></span><br><span class=\"line\"><span class=\"comment\">        */</span></span><br><span class=\"line\">       bitsMap[index] |= BIT_VALUE[offset];</span><br><span class=\"line\"></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">    * 获取值N是否存在</span></span><br><span class=\"line\"><span class=\"comment\">    * <span class=\"doctag\">@return</span> 1：存在，0：不存在</span></span><br><span class=\"line\"><span class=\"comment\">    */</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">isExist</span><span class=\"params\">(<span class=\"keyword\">long</span> n)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (n &lt; <span class=\"number\">0</span> || n &gt; length) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"length value illegal!\"</span>);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">int</span> index = (<span class=\"keyword\">int</span>) n&gt;&gt;<span class=\"number\">5</span>;</span><br><span class=\"line\">       <span class=\"keyword\">int</span> offset = (<span class=\"keyword\">int</span>) n &amp; <span class=\"number\">31</span>;</span><br><span class=\"line\">       <span class=\"keyword\">int</span> bits = (<span class=\"keyword\">int</span>) bitsMap[index];</span><br><span class=\"line\">       <span class=\"comment\">// System.out.println(\"n=\"+n+\",index=\"+index+\",offset=\"+offset+\",bits=\"+Integer.toBinaryString(bitsMap[index]));</span></span><br><span class=\"line\">       <span class=\"keyword\">return</span> ((bits &amp; BIT_VALUE[offset])) &gt;&gt;&gt; offset;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"BitMap应用\"><a href=\"#BitMap应用\" class=\"headerlink\" title=\"BitMap应用\"></a>BitMap应用</h2><ol>\n<li>BitMap小小变种:2-BitMap。</li>\n</ol>\n<p>看个小场景：在3亿个整数中找出不重复的整数，限制内存不足以容纳3亿个整数。</p>\n<p>对于这种场景我可以采用2-BitMap来解决，即为每个整数分配2bit，用不同的0、1组合来标识特殊意思，如00表示此整数没有出现过，01表示出现一次，11表示出现过多次，就可以找出重复的整数了，其需要的内存空间是正常BitMap的2倍，为：3亿*2/8/1024/1024=71.5MB。</p>\n<p>具体的过程如下：</p>\n<p>   扫描着3亿个整数，组BitMap，先查看BitMap中的对应位置，如果00则变成01，是01则变成11，是11则保持不变，当将3亿个整数扫描完之后也就是说整个BitMap已经组装完毕。最后查看BitMap将对应位为11的整数输出即可。</p>\n<ol start=\"2\">\n<li>已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。</li>\n</ol>\n<p>8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。 （可以理解为从0-99 999 999的数字，每个数字对应一个Bit位，所以只需要99M个Bit==1.2MBytes，这样，就用了小小的1.2M左右的内存表示了所有的8位数的电话）</p>\n<h2 id=\"BitMap问题\"><a href=\"#BitMap问题\" class=\"headerlink\" title=\"BitMap问题\"></a>BitMap问题</h2><p>BitMap 的思想在面试的时候还是可以用来解决不少问题的，然后在很多系统中也都会用到，算是一种不错的解决问题的思路。</p>\n<p>但是 BitMap 也有一些局限，因此会有其它一些基于 BitMap 的算法出现来解决这些问题。</p>\n<ul>\n<li>数据碰撞。比如将字符串映射到 BitMap 的时候会有碰撞的问题，那就可以考虑用 Bloom Filter 来解决，Bloom Filter 使用多个 Hash 函数来减少冲突的概率。</li>\n<li>数据稀疏。又比如要存入(10,8887983,93452134)这三个数据，我们需要建立一个 99999999 长度的 BitMap ，但是实际上只存了3个数据，这时候就有很大的空间浪费，碰到这种问题的话，可以通过引入 Roaring BitMap 来解决。</li>\n</ul>\n<p><strong>参考链接</strong></p>\n<p> <a href=\"http://www.infoq.com/cn/articles/the-secret-of-bitmap\" target=\"_blank\" rel=\"noopener\">Bitmap的秘密</a></p>\n","categories":["algorithm"],"tags":["algorithm","java"]},{"title":"spark环境搭建杂记","url":"https://sustcoder.github.io/2018/10/20/2018-10-20-spark env building process/","content":"<p><em>开发者如何进行持续高效的学习</em> ？？？</p>\n<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>最初搭建环境的目的只是为了跑spark程序，从开始搭建spark环境到最后跑spark环境断断续续用了16天，最后却发现如果想实现初衷，仅仅需要在之前运行scala程序的基础上，花费一天最多两天的时间就可以完成（实际完成这块搭建工作用了不到两小时）。严重浪费了时间，影响了学习进度。在解决问题的过程中发现一个园岭只有五个月的博客，但是高产了一百多偏博客，其中spark模块的阅读量基本在500左右，其也是从搭建环境开始写博客，内容详细且易懂。觉得是时候对此过程进行复盘了。</p>\n<h2 id=\"心路历程\"><a href=\"#心路历程\" class=\"headerlink\" title=\"心路历程\"></a>心路历程</h2><p>我是照着<a href=\"http://allaboutscala.com\" target=\"_blank\" rel=\"noopener\">http://allaboutscala.com</a> 网站进行学习的，在学完scala后，开始学习spark了，但是第一步就卡主了，网站没有搭建开发环境的教程。但是示例中有提到SBT，所以就弃用maven转战SBT了，本以为和maven类似，配置完仓库自动下载就ok了，但事与愿违，依赖总是下载失败，在各种google后，<a href=\"https://sustcoder.github.io/2018/09/13/sbt%E5%85%A5%E9%97%A8/\">终于搭建好了sbt环境</a>。此过程浪费时间主要在于好多博客介绍方法比如oschina的仓库已经过时，阿里云的仓库数据不全等。</p>\n<p>sbt环境搭建完毕后开始写spark程序，但是spark-context直接就报错了，然后就开始了搭建spark环境，因为vm上已经有一个伪分布式的spark集群了，所以就想着通过idea直连伪集群，将代码直接提交到伪集群上运行。按照这个思想在idea里面写了一个程序后就直接跑了，各种报错各种找原因，最后博友建议先保证服务器环境没问题，所以开始了spark-submit提交Spark-Pi.jar。此过程是照着spark官网给的demo进行测试的，在local模式下没问题，但是在cluster模式下一直报错，然后开始疯狂寻找答案，网上也给报错对应了很多解决方法。但无一例外失败告终，最后无奈之下，放弃了在伪分布式集群上提交jar，转真正的全分布式。</p>\n<p>之前有搭建过hadoop的高可用集群，想着这个过程应该轻车熟路了吧，但是依旧有好多坑，记录的笔记有遗漏点，本着对自己的信任，照着笔记按步照班，不幸的是这个过程中错误百出。好多细节笔记上没有记录，细节的叠加导致新的问题，只能再次疯狂的寻找各种答案各种尝试。</p>\n<p>集群搭建完成后，开始配置spark环境，发现之前在伪分布式上搜索的好多答案都是针对完全分布式的解决方法，心痛。配置完成后，通过spark-submit也可以提交jar了，开始运行idea中的spark程序，不出意外还是报错，报错原因和最开始调用伪分布式环境时的类似，而且都有类似前提就是本地需要spark环境，在最开始就是为了不想在本地配置spark环境才开始直接在服务器上配置的，无奈之下配置好本地spark环境，意外发现通过local模式可以不连接服务器也可以在idea跑spark程序了，这不就是我最初的目标吗？伤心欲绝，打算暂停本地访问yarn集群时，还是决定最后一搏，通过报错信息搜索到了一个解决方法，初看不是很靠谱，但是还是照着其步骤修改了，惊喜随之而来，至此环境搭建完成。</p>\n<h2 id=\"复盘\"><a href=\"#复盘\" class=\"headerlink\" title=\"复盘\"></a>复盘</h2><h3 id=\"决策\"><a href=\"#决策\" class=\"headerlink\" title=\"决策\"></a>决策</h3><p>决策取决于很多因素，在完全不懂spark环境时，是根据自己仅有的知识去判断下一步应该怎么做，导致路线方针性错误。如果最开始直接搭建本地环境，则不会有这么曲折的过程，还有过程中在伪分布式上跑jar，使用sbt还是maven等。</p>\n<p>怎样才能做好决策。<strong>首先得弄清楚自己的根本需求是什么，我接下来要干什么</strong>，我的最初目标只是为了让spark程序跑起来，至于怎么跑我是不关心的。那么去寻找解决办法时就会变成“怎样跑spark程序”，然后就会发现可以直接在本地运行，也可以在服务器上运行。然后就是选择问题了，我到底选择哪一种呢，那么就得需要<strong>收集各种方法的优劣来进行权衡</strong>，但是如果再选一次我依旧会选择在服务器上跑吧，因为伪分布式环境是现成的，这中间又牵引出另一个问题，什么时候进行放弃，当此路不通的时候，我是誓死完成，还是换条路，我当时的想法是非要把你弄出来。</p>\n<h3 id=\"放弃\"><a href=\"#放弃\" class=\"headerlink\" title=\"放弃\"></a>放弃</h3><p>什么时候放弃当前的方向。我们经常会因为一个问题阻塞很久，但是不愿放弃当前的方向，主要原因有两个：不想放弃已有成果、再想想办法肯定可以解决。但往往也是在此时陷入了死循环。那么在什么时候放弃呢，就目前经验而言：</p>\n<ol>\n<li>搜索出来的解决方案大同小异，而且都尝试过。此时也需要考虑搜索方式是否正确</li>\n<li>停止寻找答案，对问题再次进行分析，找次思路可行性的论证，如果发现当前方向没有理论支持时</li>\n</ol>\n<h3 id=\"搜索\"><a href=\"#搜索\" class=\"headerlink\" title=\"搜索\"></a>搜索</h3><p>越来越觉得搜索是门艺术，许多问题的解决都是在输入了对应关键字后，得到了搜索结果。一个是不断整理搜索引擎的使用技巧，google有联系搜索的网站，后期有时间了还是需要学习一下。另一个就是当搜索结果没有答案时，就需要考虑自己的关键字是否正确了，还是那个问题，我到底要什么，这个问题的根本原因是什么，大家在记录此问题时会有哪些关键信息，这个关键字还有哪些叫法等等，此处还需加强联系，搜索技能太落后。</p>\n<h3 id=\"信息\"><a href=\"#信息\" class=\"headerlink\" title=\"信息\"></a>信息</h3><p>目前解决问题的问题比较单一，主要还是通过google,对源码和官方文档的依赖较少，对于初学者直接阅读源码效果较慢但是可以更深入理解问题原因。官方文档需要去快速定位到文档位置，此处就需要英文阅读能力了，因为英文的原因，总是对阅读文档有内心的抗拒，博客的内容都是在特定场景下发生的，不一定适用自己，而文档的内容是最权威的。还有一个就是对于论坛，文档搜索答案困难可以去其对应论坛找答案比在博客找到的答案更精确，解答也更深入，Spark user list是spark的问题的一个问题聚集地，在解决掉问题后才发现这个地方，以后应主动去寻找类似论坛，然后去寻找答案。</p>\n","categories":["replay"],"tags":["replay"]},{"title":"RDD详解","url":"https://sustcoder.github.io/2018/10/16/2018-10-16-RDD explain/","content":"<h2 id=\"1-1-什么是RDD\"><a href=\"#1-1-什么是RDD\" class=\"headerlink\" title=\"1.1 什么是RDD\"></a>1.1 什么是RDD</h2><h3 id=\"1-1-1-产生背景\"><a href=\"#1-1-1-产生背景\" class=\"headerlink\" title=\"1.1.1 产生背景\"></a>1.1.1 产生背景</h3><p>当初设计RDD主要是为了解决三个问题：</p>\n<ul>\n<li><strong>Fast</strong>: Spark之前的Hadoop用的是MapReduce的编程模型，没有很好的利用分布式内存系统，中间结果都需要保存到external disk，运行效率很低。RDD模型是in-memory computing的，中间结果不需要被物化（materialized），它的<strong>persistence</strong>机制，可以保存中间结果重复使用，对需要迭代运算的机器学习应用和交互式数据挖掘应用，加速显著。Spark快还有一个原因是开头提到过的<strong>Delay Scheduling</strong>机制，它得益于RDD的Dependency设计。</li>\n<li><strong>General: MapReduce</strong>编程模型只能提供有限的运算种类（Map和Reduce），RDD希望支持更广泛更多样的operators（map，flatMap，filter等等），然后用户可以任意地组合他们。</li>\n</ul>\n<blockquote>\n<p>The ability of RDDs to accommodate computing needs that were previously met only by introducing new frameworks is, we believe, the most credible evidence of the power of the RDD abstraction.</p>\n</blockquote>\n<ul>\n<li><strong>Fault tolerance</strong>: 其他的in-memory storage on clusters，基本单元是可变的，用细粒度更新（<strong>fine-grained updates</strong>）方式改变状态，如改变table/cell里面的值，这种模型的容错只能通过复制多个数据copy，需要传输大量的数据，容错效率低下。而RDD是<strong>不可变的（immutable）</strong>，通过粗粒度变换（<strong>coarse-grained transformations</strong>），比如map，filter和join，可以把相同的运算同时作用在许多数据单元上，这样的变换只会产生新的RDD而不改变旧的RDD。这种模型可以让Spark用<strong>Lineage</strong>很高效地容错（后面会有介绍）。</li>\n</ul>\n<h3 id=\"1-1-2-RDD定义\"><a href=\"#1-1-2-RDD定义\" class=\"headerlink\" title=\"1.1.2 RDD定义\"></a>1.1.2 <strong>RDD定义</strong></h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents（代表） an immutable(不变的),partitioned collection of elements that can be operated on in parallel</span><br></pre></td></tr></table></figure>\n<p>RDD是spark的核心，也是整个spark的架构基础，RDD是弹性分布式集合（<code>Resilient Distributed Datasets</code>）的简称。</p>\n<h3 id=\"1-1-3-RDD特点\"><a href=\"#1-1-3-RDD特点\" class=\"headerlink\" title=\"1.1.3 RDD特点\"></a>1.1.3 <strong>RDD特点</strong></h3><ul>\n<li>immutable：只读，任何操作都不会改变RDD本身，只会创造新的RDD</li>\n<li>fault-tolerant：容错，通过Lineage可以高效容错</li>\n<li>partitioned：分片，RDD以partition作为最小存储和计算单元，分布在cluster的不同nodes上，一个node可以有多个partitions，一个partition只能在一个node上</li>\n<li><p>in parallel：并行，一个Task对应一个partition，Tasks之间相互独立可以并行计算</p>\n</li>\n<li><p>persistence：持久化，用户可以把会被重复使用的RDDs保存到storage上（内存或者磁盘）</p>\n</li>\n<li>partitioning：分区，用户可以选择RDD元素被partitioned的方式来优化计算，比如两个需要被join的数据集可以用相同的方式做hash-partitioned，这样可以减少shuffle提高性能</li>\n</ul>\n<h3 id=\"1-1-4-RDD抽象概念\"><a href=\"#1-1-4-RDD抽象概念\" class=\"headerlink\" title=\"1.1.4 RDD抽象概念\"></a>1.1.4 <strong>RDD抽象概念</strong></h3><p>一个RDD定义了对数据的一个操作过程, 用户提交的计算任务可以由多个RDD构成。多个RDD可以是对单个/多个数据的多个操作过程。多个RDD之间的关系使用依赖来表达。操作过程就是用户自定义的函数。</p>\n<p>RDD(弹性分布式数据集)去掉形容词，主体为：数据集。如果认为RDD就是数据集，那就有点理解错了。个人认为：RDD是定义对partition数据项转变的高阶函数，应用到输入源数据，输出转变后的数据，即：<strong>RDD是一个数据集到另外一个数据集的映射，而不是数据本身。</strong> 这个概念类似数学里的函数<code>f(x) = ax^2 + bx + c</code>。这个映射函数可以被序列化，所要被处理的数据被分区后分布在不同的机器上，应用一下这个映射函数，得出结果，聚合结果。</p>\n<p>这些集合是弹性的，如果数据集一部分丢失，则可以对它们进行重建。具有自动容错、位置感知调度和可伸缩性，而容错性是最难实现的，大多数分布式数据集的容错性有两种方式：数据检查点和记录数据的更新。对于大规模数据分析系统，数据检查点操作成本高，主要原因是大规模数据在服务器之间的传输带来的各方面的问题，相比记录数据的更新，RDD也只支持粗粒度的转换共享状态而非细粒度的更新共享状态，也就是记录如何从其他RDD转换而来(即lineage)，以便恢复丢失的分区。 </p>\n<p>RDDs 非常适合将相同操作应用在整个数据集的所有的元素上的批处理应用. 在这些场景下, RDDs 可以利用血缘关系图来高效的记住每一个 transformations 的步骤, 并且不需要记录大量的数据就可以恢复丢失的分区数据. RDDs 不太适合用于需要异步且细粒度的更新共享状态的应用, 比如一个 web 应用或者数据递增的 web 爬虫应用的存储系统。</p>\n<h2 id=\"1-2-RDD特点\"><a href=\"#1-2-RDD特点\" class=\"headerlink\" title=\"1.2 RDD特点\"></a>1.2 RDD特点</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Internally, each RDD is characterized by five main properties:</span><br><span class=\"line\">- A list of partitions</span><br><span class=\"line\">- A function for computing each split</span><br><span class=\"line\">- A list of dependencies on other RDDs</span><br><span class=\"line\">- Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</span><br><span class=\"line\">- Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</span><br></pre></td></tr></table></figure>\n<p>每个特性都对应RDD.scala中的一个方法实现：</p>\n<ul>\n<li><p><code>a list of partition</code>  由多个机器里面的partition组成的</p>\n</li>\n<li><p><code>a function for computing each split</code>  并行计算 </p>\n</li>\n<li><p><code>a list of dependencies on other RDDS</code> rdd间存在依赖关系,记录数据转换间的依赖</p>\n</li>\n<li><p><code>a partitioner for key-vaue</code> RDDS 可进行重新分区（只有key value的partition有）</p>\n</li>\n<li><p><code>a list of preferred locations to compute each spilt on</code>  用最期望的位置进行计算</p>\n</li>\n</ul>\n<h2 id=\"1-3-RDD操作\"><a href=\"#1-3-RDD操作\" class=\"headerlink\" title=\"1.3 RDD操作\"></a>1.3 RDD操作</h2><h3 id=\"1-3-1-RDD创建\"><a href=\"#1-3-1-RDD创建\" class=\"headerlink\" title=\"1.3.1 RDD创建\"></a>1.3.1 RDD创建</h3><ol>\n<li><code>parallelize</code>:从普通Scala集合创建</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> data = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> distData = sc.parallelize(data)</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>从Hadoop文件系统或与Hadoop兼容的其他持久化存储系统创建，如Hive、HBase</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> distFile = sc.textFile(<span class=\"string\">\"data.txt\"</span>)</span><br><span class=\"line\">distFile: org.apache.spark.rdd.<span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = data.txt <span class=\"type\">MapPartitionsRDD</span>[<span class=\"number\">10</span>] at textFile at &lt;console&gt;:<span class=\"number\">26</span></span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>从父RDD转换得到新的RDD</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> fromParent=distFile.map(s=&gt;s.length)</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-2-操作方式\"><a href=\"#1-3-2-操作方式\" class=\"headerlink\" title=\"1.3.2 操作方式\"></a>1.3.2 操作方式</h3><p>RDD在宏观来看类似于java中对象的概念，我们在java中对对象上作用一系列操作（方法）得到最终结果。同样的我们在RDD上进行一系列操作（算子）将一个RDD转换为另一个RDD，最终得到我们所需要的RDD。RDD算子主要包括：</p>\n<ul>\n<li><p><code>Transformation</code>算子：Transformation操作是延迟计算的，即从一个RDD转换成另一个RDD的转换操作不是 马上执行，需要等到有Action操作时，才真正出发执行，如Map、Filter等操作</p>\n</li>\n<li><p><code>Action</code>算子：Action算子会出发Spark提交作业（Job），并将数据输出到Spark系统，如collect、count等</p>\n</li>\n</ul>\n<p>RDD操作特点 <strong>惰性求值</strong>：</p>\n<p>transformation算子作用在RDD时，并不是立即触发计算，只是记录需要操作的指令。等到有Action算子出现时才真正开始触发计算。</p>\n<p>textFile等读取数据操作和persist和cache缓存操作也是惰性的</p>\n<p>为什么要使用惰性求值呢：使用惰性求值可以把一些操作合并到一起来减少数据的计算步骤，提高计算效率。</p>\n<p>从惰性求值角度看RDD就是一组spark计算指令的列表</p>\n<h3 id=\"1-3-4-缓存策略\"><a href=\"#1-3-4-缓存策略\" class=\"headerlink\" title=\"1.3.4 缓存策略\"></a>1.3.4 缓存策略</h3><p>RDD的缓存策略在<code>StorageLevel</code>中实现，通过对是否序列化，是否存储多个副本等条件的组合形成了多种缓存方式。例如：<code>MEMORY_ONLY_SER</code>存储在内存中并进行序列化，当内存不足时，不进行本地化；<code>MEMORY_AND_DISK_2</code>优先存储内存中，内存中无空间时，存储在本地磁盘，并有两个副本。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StorageLevel</span> <span class=\"title\">private</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    // 缓存方式</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _useDisk: <span class=\"type\">Boolean</span>, \t\t// 是否使用磁盘</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _useMemory: <span class=\"type\">Boolean</span>, \t// 是否使用内存</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _useOffHeap: <span class=\"type\">Boolean</span>,\t// 是否使用堆外内存</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _deserialized: <span class=\"type\">Boolean</span>, // 是否序列化</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var _replication: <span class=\"type\">Int</span> = 1</span>)\t<span class=\"title\">//</span> <span class=\"title\">存储副本，默认一个</span></span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Externalizable</span> </span>&#123;</span><br><span class=\"line\">      </span><br><span class=\"line\">  <span class=\"comment\">// 条件组合结果</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">NONE</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">DISK_ONLY</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">DISK_ONLY_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY_SER</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_ONLY_SER_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">true</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK_SER</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">MEMORY_AND_DISK_SER_2</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">OFF_HEAP</span> = <span class=\"keyword\">new</span> <span class=\"type\">StorageLevel</span>(<span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">true</span>, <span class=\"literal\">false</span>, <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p><strong>策略选择顺序</strong>：</p>\n<ul>\n<li><p>默认选择<code>MEMORY_ONLY</code></p>\n</li>\n<li><p>如果内存不足，选择<code>MEMORY_ONLY_SER</code></p>\n</li>\n<li><p>如果需要做容错,选择<code>MEMORY_ONLY_SER_2</code></p>\n</li>\n<li><p>如果中间计算RDD的代价比较大时，选择<code>MEMORY_AND_DISK</code></p>\n</li>\n</ul>\n<p><strong>控制操作</strong>：</p>\n<ol>\n<li><code>persist</code>操作，可以将RDD持久化到不同层次的存储介质，以便后续操作重复使用。</li>\n</ol>\n<p>　　  1)cache:RDD[T]  默认使用<code>MEMORY_ONLY</code></p>\n<p>　　  2)persist:RDD[T] 默认使用<code>MEMORY_ONLY</code></p>\n<p>　　  3)Persist(level:StorageLevel):RDD[T] eg: <code>myRdd.persist(StorageLevels.MEMORY_ONLY_SER)</code></p>\n<ol start=\"2\">\n<li><code>checkpoint</code></li>\n</ol>\n<p>　　将RDD持久化到HDFS中，与persist操作不同的是checkpoint会切断此RDD之前的依赖关系，而persist依然保留RDD的依赖关系。</p>\n<h3 id=\"1-3-5-RDD回收\"><a href=\"#1-3-5-RDD回收\" class=\"headerlink\" title=\"1.3.5 RDD回收\"></a>1.3.5 RDD回收</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Spark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the `RDD.unpersist()` method.</span><br></pre></td></tr></table></figure>\n<p>spark有一个监控线程去检测内存使用情况，当内存不足时使用LRU进行淘汰old data,也可以通过<code>RDD.unpersist()</code>方法手动移除缓存。</p>\n<h3 id=\"1-3-6-RDD保存\"><a href=\"#1-3-6-RDD保存\" class=\"headerlink\" title=\"1.3.6 RDD保存\"></a>1.3.6 RDD保存</h3><ul>\n<li><code>saveAsTextFile()</code>将RDD中的元素保存在指定目录中，这个目录位于任何Hadoop支持的存储系统中</li>\n<li><code>saveAsObjectFile()</code>将原RDD中的元素序列化成Java对象，存储在指定目录中</li>\n<li><code>saveAsSequenceFile()</code> 将键值对型RDD以SequenceFile的格式保存。键值对型RDD也可以以文本形式保存</li>\n</ul>\n<p>需要注意的是，上面的方法都把一个目录名字作为入参，然后在这个目录为每个RDD分区创建一个文件夹。这种设计不仅可以高效而且可容错。因为每个分区被存成一个文件，所以Spark在保存RDD的时候可以启动多个任务，并行执行，将数据写入文件系统中，这样也保证了写入数据的过程中可容错，一旦有一个分区写入文件的任务失败了，Spark可以在重启一个任务，重写刚才失败任务创建的文件。</p>\n<h1 id=\"2-RDD详解\"><a href=\"#2-RDD详解\" class=\"headerlink\" title=\"2. RDD详解\"></a>2. RDD详解</h1><h2 id=\"2-1-RDD分区\"><a href=\"#2-1-RDD分区\" class=\"headerlink\" title=\"2.1 RDD分区\"></a>2.1 RDD分区</h2><p><strong>RDD 表示并行计算的计算单元是使用分区（Partition）</strong>。</p>\n<h3 id=\"2-1-1-分区实现\"><a href=\"#2-1-1-分区实现\" class=\"headerlink\" title=\"2.1.1 分区实现\"></a>2.1.1 分区实现</h3><p>RDD 内部的数据集合在逻辑上和物理上被划分成多个小子集合，这样的每一个子集合我们将其称为分区，分区的个数会决定并行计算的粒度，而每一个分区数值的计算都是在一个单独的任务中进行，因此并行任务的个数，也是由 RDD（实际上是一个阶段的末 RDD，调度章节会介绍）分区的个数决定的。</p>\n<p>RDD 只是数据集的抽象，分区内部并不会存储具体的数据。<code>Partition</code> 类内包含一个 <code>index</code> 成员，表示该分区在 RDD 内的编号，<strong>通过 RDD 编号 + 分区编号可以唯一确定该分区对应的块编号</strong>，利用底层数据存储层提供的接口，就能从存储介质（如：HDFS、Memory）中提取出分区对应的数据</p>\n<p>怎么切分是<code>Partitioner</code>定义的, <code>Partitioner</code>有两个接口: <code>numPartitions</code>分区数, <code>getPartition(key: Any): Int</code>根据传入的参数确定分区号。实现了Partitioner的有：</p>\n<ol>\n<li>HashPartitioner</li>\n<li>RangePartitioner</li>\n<li>GridPartitioner</li>\n<li>PythonPartitioner</li>\n</ol>\n<p>一个RDD有了Partitioner, 就可以对当前RDD持有的数据进行划分</p>\n<h3 id=\"2-1-2-分区个数\"><a href=\"#2-1-2-分区个数\" class=\"headerlink\" title=\"2.1.2 分区个数\"></a>2.1.2 分区个数</h3><p>RDD 分区的一个分配原则是：<strong>尽可能使得分区的个数等于集群的CPU核数</strong>。</p>\n<p>RDD 可以通过创建操作或者转换操作得到。转换操作中，分区的个数会根据转换操作对应多个 RDD 之间的依赖关系确定，窄依赖子 RDD 由父 RDD 分区个数决定，Shuffle 依赖由子 RDD 分区器决定。</p>\n<p>创建操作中，程序开发者可以手动指定分区的个数，例如 <code>sc.parallelize (Array(1, 2, 3, 4, 5), 2)</code> 表示创建得到的 RDD 分区个数为 2，在没有指定分区个数的情况下，Spark 会根据集群部署模式，来确定一个分区个数默认值。</p>\n<p>对于 <code>parallelize</code> 方法，默认情况下，分区的个数会受 Apache Spark 配置参数 <code>spark.default.parallelism</code> 的影响,无论是以本地模式、Standalone 模式、Yarn 模式或者是 Mesos 模式来运行 Apache Spark，分区的默认个数等于对 <code>spark.default.parallelism</code> 的指定值，若该值未设置，则 Apache Spark 会根据不同集群模式的特征，来确定这个值。</p>\n<p>本地模式，默认分区个数等于本地机器的 CPU 核心总数（或者是用户通过 <code>local[N]</code> 参数指定分配给 Apache Spark 的核心数目),集群模式（Standalone 或者 Yarn）默认分区个数等于集群中所有核心数目的总和，或者 2，取两者中的较大值(<code>conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))</code>)</p>\n<p>对于 <code>textFile</code> 方法，默认分区个数等于 <code>min(defaultParallelism, 2)</code></p>\n<h3 id=\"2-1-3-分区内部记录个数\"><a href=\"#2-1-3-分区内部记录个数\" class=\"headerlink\" title=\"2.1.3 分区内部记录个数\"></a>2.1.3 分区内部记录个数</h3><p>分区分配的另一个分配原则是：尽可能使同一 RDD 不同分区内的记录的数量一致。</p>\n<p>对于转换操作得到的 RDD，如果是窄依赖，则分区记录数量依赖于父 RDD 中相同编号分区是如何进行数据分配的，如果是 Shuffle 依赖，则分区记录数量依赖于选择的分区器，<strong>分区器有哈希分区和范围分区</strong>。哈希分区器无法保证数据被平均分配到各个分区，而范围分区器则能做到这一点</p>\n<p>对于<code>textFile</code> 方法分区内数据的大小则是由 Hadoop API 接口 <code>FileInputFormat.getSplits</code> 方法决定（见 <code>HadoopRDD</code> 类），<strong>得到的每一个分片即为 RDD 的一个分区</strong>，分片内数据的大小会受文件大小、文件是否可分割、HDFS 中块大小等因素的影响，但总体而言会是比较均衡的分配</p>\n<h2 id=\"2-2-RDD依赖\"><a href=\"#2-2-RDD依赖\" class=\"headerlink\" title=\"2.2 RDD依赖\"></a>2.2 RDD依赖</h2><h3 id=\"2-2-1-依赖与-RDD\"><a href=\"#2-2-1-依赖与-RDD\" class=\"headerlink\" title=\"2.2.1 依赖与 RDD\"></a>2.2.1 <strong>依赖与 RDD</strong></h3><p>RDD 的容错机制是通过记录更新来实现的，且记录的是粗粒度的转换操作。在外部，我们将记录的信息称为<strong>血统（Lineage）关系</strong>，而到了源码级别，Apache Spark 记录的则是 RDD 之间的<strong>依赖（Dependency）</strong>关系。在一次转换操作中，创建得到的新 RDD 称为子 RDD，提供数据的 RDD 称为父 RDD，父 RDD 可能会存在多个，我们把子 RDD 与父 RDD 之间的关系称为依赖关系，或者可以说是子 RDD 依赖于父 RDD。</p>\n<p>依赖只保存父 RDD 信息，转换操作的其他信息，如数据处理函数，会在创建 RDD 时候，保存在新的 RDD 内。依赖在 Apache Spark 源码中的对应实现是 <code>Dependency</code> 抽象类，每个 <code>Dependency</code> 子类内部都会存储一个 <code>RDD</code> 对象，对应一个父 RDD，如果一次转换转换操作有多个父 RDD，就会对应产生多个 <code>Dependency</code> 对象，所有的 <code>Dependency</code> 对象存储在子 RDD 内部，通过遍历 RDD 内部的 <code>Dependency</code> 对象，就能获取该 RDD 所有依赖的父 RDD。</p>\n<h3 id=\"2-2-2-依赖分类\"><a href=\"#2-2-2-依赖分类\" class=\"headerlink\" title=\"2.2.2 依赖分类\"></a>2.2.2 依赖分类</h3><p>Apache Spark 将依赖进一步分为两类，分别是<strong>窄依赖（Narrow Dependency）</strong>和 <strong>Shuffle 依赖（Shuffle Dependency，在部分文献中也被称为 Wide Dependency，即宽依赖）</strong>。</p>\n<p>窄依赖中，父 RDD 中的一个分区最多只会被子 RDD 中的一个分区使用，换句话说，父 RDD 中，一个分区内的数据是不能被分割的，必须整个交付给子 RDD 中的一个分区。</p>\n<p>窄依赖可进一步分类成一对一依赖和范围依赖，对应实现分别是 <code>OneToOneDependency</code> 类和<code>RangeDependency</code> 类。一对一依赖表示子 RDD 分区的编号与父 RDD 分区的编号完全一致的情况，若两个 RDD 之间存在着一对一依赖，则子 RDD 的分区个数、分区内记录的个数都将继承自父 RDD。范围依赖是依赖关系中的一个特例，只被用于表示 <code>UnionRDD</code> 与父 RDD 之间的依赖关系。相比一对一依赖，除了第一个父 RDD，其他父 RDD 和子 RDD 的分区编号不再一致，Apache Spark 统一将<code>unionRDD</code>与父 RDD 之间（包含第一个 RDD）的关系都叫做范围依赖。</p>\n<p>依赖类图：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">graph TD</span><br><span class=\"line\">A[Dependency&lt;br&gt;依赖关系基类]--- B[NarrowDependency&lt;br&gt;窄依赖]</span><br><span class=\"line\">A---C[ShuffleDenpendency&lt;br&gt;shuffle依赖]</span><br><span class=\"line\">B---D[OneToOneDependency&lt;br&gt;一对一依赖]</span><br><span class=\"line\">B---E[RangeDependency&lt;br&gt;范围依赖]</span><br></pre></td></tr></table></figure>\n<p>下图展示了几类常见的窄依赖及其对应的转换操作。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/narrowDependency.png\" alt=\"窄依赖\"></p>\n<p>Shuffle 依赖中，父 RDD 中的分区可能会被多个子 RDD 分区使用。因为父 RDD 中一个分区内的数据会被分割，发送给子 RDD 的所有分区，因此 Shuffle 依赖也意味着父 RDD 与子 RDD 之间存在着 Shuffle 过程。下图展示了几类常见的Shuffle依赖及其对应的转换操作。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/wideDependency.png\" alt=\"窄依赖\"></p>\n<p>Shuffle 依赖的对应实现为<code>ShuffleDependency</code> 类,其实现比较复杂，主要通过以下成员完成：</p>\n<ul>\n<li><code>rdd</code>：用于表示 Shuffle 依赖中，子 RDD 所依赖的父 RDD。</li>\n<li><code>shuffleId</code>：Shuffle 的 ID 编号，在一个 Spark 应用程序中，每个 Shuffle 的编号都是唯一的。</li>\n<li><code>shuffleHandle</code>：Shuffle 句柄，<code>ShuffleHandle</code> 内部一般包含 Shuffle ID、Mapper 的个数以及对应的 Shuffle 依赖，在执行 <code>ShuffleMapTask</code> 时候，任务可以通过 <code>ShuffleManager</code> 获取得到该句柄，并进一步得到 Shuffle 相关信息。</li>\n<li><code>partitioner</code>：分区器，用于决定 Shuffle 过程中 Reducer 的个数（实际上是子 RDD 的分区个数）以及 Map 端的一条数据记录应该分配给哪一个 Reducer，也可以被用在 <code>CoGroupedRDD</code> 中，确定父 RDD 与子 RDD 之间的依赖关系类型。</li>\n<li><code>serializer</code>：序列化器。用于 Shuffle 过程中 Map 端数据的序列化和 Reduce 端数据的反序列化。</li>\n<li><code>KeyOrdering</code>：键值排序策略，用于决定子 RDD 的一个分区内，如何根据键值对 类型数据记录进行排序。</li>\n<li><code>Aggregator</code>：聚合器，内部包含了多个聚合函数，比较重要的函数有 <code>createCombiner：V =&gt; C</code>，<code>mergeValue: (C, V) =&gt; C</code> 以及 <code>mergeCombiners: (C, C) =&gt; C</code>。例如，对于 <code>groupByKey</code> 操作，<code>createCombiner</code> 表示把第一个元素放入到集合中，<code>mergeValue</code> 表示一个元素添加到集合中，<code>mergeCombiners</code> 表示把两个集合进行合并。这些函数被用于 Shuffle 过程中数据的聚合。</li>\n<li><code>mapSideCombine</code>：用于指定 Shuffle 过程中是否需要在 map 端进行 combine 操作。如果指定该值为 <code>true</code>，由于 combine 操作需要用到聚合器中的相关聚合函数，因此 <code>Aggregator</code> 不能为空，否则 Apache Spark 会抛出异常。例如：<code>groupByKey</code> 转换操作对应的<code>ShuffleDependency</code>中，<code>mapSideCombine = false</code>，而 <code>reduceByKey</code> 转换操作中，<code>mapSideCombine = true</code>。</li>\n</ul>\n<p>依赖关系是两个 RDD 之间的依赖，因此若一次转换操作中父 RDD 有多个，则可能会同时包含窄依赖和 Shuffle 依赖，下图所示的 <code>Join</code> 操作，RDD a 和 RDD c 采用了相同的分区器，两个 RDD 之间是窄依赖，Rdd b 的分区器与 RDD c 不同，因此它们之间是 Shuffle 依赖，具体实现可参见 <code>CoGroupedRDD</code> 类的 <code>getDependencies</code> 方法。这里能够再次发现：<strong>一个依赖对应的是两个 RDD，而不是一次转换操作。</strong></p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/mixDependency.png\" alt=\"窄依赖\"></p>\n<h3 id=\"2-2-3-依赖与容错机制\"><a href=\"#2-2-3-依赖与容错机制\" class=\"headerlink\" title=\"2.2.3 依赖与容错机制\"></a>2.2.3 依赖与容错机制</h3><p>介绍完依赖的类别和实现之后，回过头来，从分区的角度继续探究 Apache Spark 是如何通过依赖关系来实现容错机制的。下图给出了一张依赖关系图，<code>fileRDD</code> 经历了 <code>map</code>、<code>reduce</code> 以及<code>filter</code> 三次转换操作，得到了最终的 RDD，其中，<code>map</code>、<code>filter</code> 操作对应的依赖为窄依赖，<code>reduce</code> 操作对应的是 Shuffle 依赖。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/fault-tolrarnt0.png\" alt=\"fault-tolrarnt0\"></p>\n<p>假设最终 RDD 第一块分区内的数据因为某些原因丢失了，由于 RDD 内的每一个分区都会记录其对应的父 RDD 分区的信息，因此沿着下图所示的依赖关系往回走，我们就能找到该分区数据最终来源于 <code>fileRDD</code> 的所有分区，再沿着依赖关系往后计算路径中的每一个分区数据，即可得到丢失的分区数据。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/fault-tolrarnt1.png\" alt=\"fault-tolrarnt0\"></p>\n<p>这个例子并不是特别严谨，按照我们的思维，只有执行了持久化，存储在存储介质中的 RDD 分区才会出现数据丢失的情况，但是上例中最终的 RDD 并没有执行持久化操作。事实上，Apache Spark 将没有被持久化数据重新被计算，以及持久化的数据第一次被计算，也等价视为数据“丢失”，在 1.7 节中我们会看到这一点。</p>\n<h3 id=\"2-2-4-依赖与并行计算\"><a href=\"#2-2-4-依赖与并行计算\" class=\"headerlink\" title=\"2.2.4 依赖与并行计算\"></a>2.2.4 依赖与并行计算</h3><p>在上一节中我们看到，在 RDD 中，可以通过<strong>计算链（Computing Chain）</strong>来计算某个 RDD 分区内的数据，我们也知道分区是并行计算的基本单位，这时候可能会有一种想法：能否把 RDD 每个分区内数据的计算当成一个并行任务，每个并行任务包含一个计算链，将一个计算链交付给一个 CPU 核心去执行，集群中的 CPU 核心一起把 RDD 内的所有分区计算出来。</p>\n<p>答案是可以，这得益于 RDD 内部分区的数据依赖相互之间并不会干扰，而 Apache Spark 也是这么做的，但在实现过程中，仍有很多实际问题需要去考虑。进一步观察窄依赖、Shuffle 依赖在做并行计算时候的异同点。</p>\n<p>先来看下方左侧的依赖图，依赖图中所有的依赖关系都是窄依赖（包括一对一依赖和范围依赖），可以看到，不仅计算链是独立不干扰的（所以可以并行计算），所有计算链内的每个分区单元的计算工作也不会发生重复，如右侧的图所示。这意味着除非执行了持久化操作，否则计算过程中产生的中间数据我们没有必要保留 —— 因为当前分区的数据只会给计算链中的下一个分区使用，而不用专门保留给其他计算链使用。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/paralle1.png\" alt=\"paralle1\"></p>\n<p>再来观察 Shuffle 依赖的计算链，如图下方左侧的图中，既有窄依赖，又有 Shuffle 依赖，由于 Shuffle 依赖中，子 RDD 一个分区的数据依赖于父 RDD 内所有分区的数据，当我们想计算末 RDD 中一个分区的数据时，Shuffle 依赖处需要把父 RDD 所有分区的数据计算出来，如右侧的图所示（紫色表示最后两个分区计算链共同经过的地方） —— 而这些数据，在计算末 RDD 另外一个分区的数据时候，同样会被用到。</p>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/spark/paralle2.png\" alt=\"paralle2\"></p>\n<p>如果我们做到计算链的并行计算的话，这就意味着，要么 Shuffle 依赖处父 RDD 的数据在每次需要使用的时候都重复计算一遍，要么想办法把父 RDD 数据保存起来，提供给其余分区的数据计算使用。</p>\n<p>Apache Spark 采用的是第二种办法，但保存数据的方法可能与想象中的会有所不同，<strong>Spark 把计算链从 Shuffle 依赖处断开</strong>，划分成不同的<strong>阶段（Stage）</strong>，阶段之间存在依赖关系（其实就是 Shuffle 依赖），从而可以构建一张不同阶段之间的<strong>有向无环图（DAG）</strong>。</p>\n<h2 id=\"2-3-RDD-lineage\"><a href=\"#2-3-RDD-lineage\" class=\"headerlink\" title=\"2.3 RDD lineage\"></a>2.3 RDD lineage</h2><p>RDD的逻辑执行计划和物理<a href=\"https://github.com/lw-lin/CoolplaySpark/blob/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/1.1%20DStream%2C%20DStreamGraph%20%E8%AF%A6%E8%A7%A3.md\" target=\"_blank\" rel=\"noopener\">执行计划详解…</a></p>\n<p><strong>RDD逻辑执行计划</strong></p>\n<p>RDD是通过一系列transformation操作进行计算的，而这些transformation操作形成的图就是DAG，也就是逻辑执行计划</p>\n<p><strong>RDD物理执行计划</strong></p>\n<p>根据RDD延迟计算特性，其真正在触发计算是在有output时发生的，outPutRdd上记录了其上级依赖的RDD，依次向前直到碰到inputRdd，这个通过依赖反向去获取RDD的过程形成的就是物理执行计划。</p>\n<p><strong>逻辑执行计划所包含的RDD和物理执行计划所包含的RDD不一定是对等的</strong></p>\n<p>可以通过<code>toDebugString</code>查看RDD的lineage</p>\n<h2 id=\"2-4-RDD-计算函数\"><a href=\"#2-4-RDD-计算函数\" class=\"headerlink\" title=\"2.4 RDD 计算函数\"></a>2.4 RDD 计算函数</h2><p><a href=\"https://github.com/sustcoder/spark-tutorial/blob/master/src/main/scala/com/liyz/scala/starter/CollectionFun.scala\" target=\"_blank\" rel=\"noopener\">前往查看详情…</a></p>\n<h2 id=\"2-5-RDD-分区器\"><a href=\"#2-5-RDD-分区器\" class=\"headerlink\" title=\"2.5 RDD 分区器\"></a>2.5 RDD 分区器</h2><p><a href=\"https://sustcoder.github.io/2018/12/10/sparkCore-sourceCodeAnalysis_partitioner/\">前往查看详情…</a></p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations</a></p>\n<p><a href=\"https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html\" target=\"_blank\" rel=\"noopener\">https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html</a></p>\n<p><a href=\"http://spark.apachecn.org/paper/zh/spark-rdd.html\" target=\"_blank\" rel=\"noopener\">http://spark.apachecn.org/paper/zh/spark-rdd.html</a></p>\n","categories":["spark"],"tags":["spark","RDD"]},{"title":"spark集群环境搭建","url":"https://sustcoder.github.io/2018/09/28/2018-09-28-spark on yarn/","content":"<h1 id=\"spark-on-yarn\"><a href=\"#spark-on-yarn\" class=\"headerlink\" title=\"spark on yarn\"></a>spark on yarn</h1><h1 id=\"软件安装\"><a href=\"#软件安装\" class=\"headerlink\" title=\"软件安装\"></a>软件安装</h1><h2 id=\"当前环境\"><a href=\"#当前环境\" class=\"headerlink\" title=\"当前环境\"></a>当前环境</h2><p>hadoop环境搭建参考：<a href=\"https://my.oschina.net/freelili/blog/1834706\" target=\"_blank\" rel=\"noopener\">hadoop集群安装</a></p>\n<ul>\n<li>hadoop2.6</li>\n<li>spark-2.2.0-bin-hadoop2.6.tgz</li>\n<li>scala-2.11.12</li>\n</ul>\n<h2 id=\"安装scala\"><a href=\"#安装scala\" class=\"headerlink\" title=\"安装scala\"></a>安装scala</h2><blockquote>\n<p>tar -zxvf scala-2.11.12.tgz</p>\n<p>vi /etc/profile</p>\n</blockquote>\n<p>添加以下内容</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">export SCALA_HOME=/home/hadoop/app/scala</span><br><span class=\"line\">export PATH=$PATH:$SCALA_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>使配置生效</p>\n<blockquote>\n<p>source /etc/profile</p>\n</blockquote>\n<p>查看scala版本号</p>\n<blockquote>\n<p>scala -version</p>\n</blockquote>\n<p>注意： <strong>用root账户修改完变量后，需要重新打开ssh链接，配置才能生效</strong></p>\n<h2 id=\"安装spark\"><a href=\"#安装spark\" class=\"headerlink\" title=\"安装spark\"></a>安装spark</h2><blockquote>\n<p>tar -zvf spark-2.2.0-bin-without-hadoop.tgz</p>\n<p>vi /etc/profile</p>\n</blockquote>\n<p>添加以下内容</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">export SPARK_HOME=/home/hadoop/app/spark2.2.0</span><br><span class=\"line\">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>修改spark环境变量</p>\n<blockquote>\n<p>cp spark-env.sh.template spark-env.sh</p>\n<p>vi conf/spark-evn.sh</p>\n</blockquote>\n<p>添加以下内容</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">SPARK_DRIVER_MEMORY=512m</span><br><span class=\"line\">SPARK_DIST_CLASSPATH=$(/home/hadoop/app/hadoop-2.6.0/bin/hadoop classpath)</span><br><span class=\"line\">SPARK_LOCAL_DIRS=/home/hadoop/app/spark2.2.0</span><br><span class=\"line\">export SPARK_MASTER_IP=192.168.10.125</span><br><span class=\"line\"></span><br><span class=\"line\">export JAVA_HOME=/home/app/jdk8</span><br><span class=\"line\">export SCALA_HOME=/home/hadoop/app/scala</span><br><span class=\"line\">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class=\"line\">export HADOOP_CONF_DIR=/home/hadoop/app/hadoop/etc/hadoop</span><br><span class=\"line\">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SCALA_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>使配置变量生效</p>\n<blockquote>\n<p>source /etc/profile</p>\n</blockquote>\n<p>配置slaves</p>\n<blockquote>\n<p>vi slaves</p>\n</blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">node3</span><br><span class=\"line\">node4</span><br></pre></td></tr></table></figure>\n<p>将配置文件下发到从节点</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">scp slaves hadoop@node3:/home/hadoop/app/spark2.2.0/conf</span><br><span class=\"line\">scp slaves hadoop@node4:/home/hadoop/app/spark2.2.0/conf</span><br><span class=\"line\">scp spark-env.sh hadoop@node3:/home/hadoop/app/spark2.2.0/conf</span><br><span class=\"line\">scp spark-env.sh hadoop@node4:/home/hadoop/app/spark2.2.0/conf</span><br></pre></td></tr></table></figure>\n<h2 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h2><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动zookeeper,可能存在选举延迟，可多执行几次./zkServer.sh status查看启动结果</span></span><br><span class=\"line\">./runRemoteCmd.sh \"/home/hadoop/app/zookeeper/bin/zkServer.sh start\" zookeeper</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2节点上执行,启动HDFS</span></span><br><span class=\"line\">sbin/start-dfs.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2节点上执行,启动YARN</span></span><br><span class=\"line\">sbin/start-yarn.sh </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node4节点上面执行,启动resourcemanager</span></span><br><span class=\"line\">sbin/yarn-daemon.sh start resourcemanager</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2上启动spark</span></span><br><span class=\"line\">sbin/start-all.sh</span><br></pre></td></tr></table></figure>\n<h2 id=\"关闭\"><a href=\"#关闭\" class=\"headerlink\" title=\"关闭\"></a>关闭</h2><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 关闭spark</span></span><br><span class=\"line\">sbin/stop-all.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node4上执行</span></span><br><span class=\"line\">sbin/yarn-daemon.sh stop resourcemanager</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2上执行</span></span><br><span class=\"line\">sbin/stop-yarn.sh </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在node2上执行</span></span><br><span class=\"line\">sbin/stop-dfs.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 关闭zookeeper</span></span><br><span class=\"line\">runRemoteCmd.sh \"/home/hadoop/app/zookeeper/bin/zkServer.sh stop\" zookeeper</span><br></pre></td></tr></table></figure>\n<p>查看启动情况</p>\n<blockquote>\n<p>jps</p>\n</blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hdfs进程</span></span><br><span class=\"line\">1661 NameNode</span><br><span class=\"line\">1934 SecondaryNameNode</span><br><span class=\"line\">1750 DataNode</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> yarn进程</span></span><br><span class=\"line\">8395 ResourceManager</span><br><span class=\"line\">7725 NameNode</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> namenode HA</span></span><br><span class=\"line\">8256 DFSZKFailoverController</span><br><span class=\"line\">7985 JournalNode</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> zookeeper进程</span></span><br><span class=\"line\">1286 QuorumPeerMain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> spark进程</span></span><br><span class=\"line\">2551 Master</span><br><span class=\"line\">2641 Worker</span><br></pre></td></tr></table></figure>\n<h2 id=\"管理界面\"><a href=\"#管理界面\" class=\"headerlink\" title=\"管理界面\"></a>管理界面</h2><figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hadoop</span>:  http://node2:8088/</span><br><span class=\"line\"><span class=\"attribute\">nameNode</span>: http://node2:50070/</span><br><span class=\"line\"><span class=\"attribute\">nodeManager</span>:  http://node2:8042/</span><br><span class=\"line\">spark master:  http://node2:8080/</span><br><span class=\"line\">spark worker:  http://node2:8081/</span><br><span class=\"line\">spark jobs:  http://node2:4040/</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行示例\"><a href=\"#运行示例\" class=\"headerlink\" title=\"运行示例\"></a>运行示例</h2><p><strong>Spark-shell</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">vi test.text# 在文件中添加 hello spark</span><br><span class=\"line\">hdfs dfs -mkdir /test # 创建文件夹</span><br><span class=\"line\">hdfs dfs -put test.txt /test # 上传文件到hdfs</span><br><span class=\"line\">hdfs dfs -ls /test # 查看是否上传成功</span><br><span class=\"line\">./bin/spark-shell</span><br><span class=\"line\">sc.textFile(\"hdfs://node2:9000/test/test.txt\") # 从hdfs上获取文件</span><br><span class=\"line\">sc.first() # 获取文件的第一行数据</span><br></pre></td></tr></table></figure>\n<p><strong>Run application locally(Local模式)</strong></p>\n<blockquote>\n<p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master local[4] /home/hadoop/app/spark2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar</p>\n</blockquote>\n<p><strong>Run on a Spark standalone cluster(Standalone模式,使用Spark自带的简单集群管理器)</strong></p>\n<blockquote>\n<p>./bin/spark-submit \\<br>–class org.apache.spark.examples.SparkPi \\<br>–master spark://node2:7077 \\<br>–executor-memory 512m \\<br>–total-executor-cores 4 \\<br>/home/hadoop/app/spark2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar 10</p>\n</blockquote>\n<p><strong>Run on yarn(YARN模式，使用YARN作为集群管理器)</strong></p>\n<blockquote>\n<p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn –deploy-mode client examples/jars/spark-examples*.jar 10</p>\n</blockquote>\n<h1 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h1><blockquote>\n<p>HADOOP_CONF_DIR=/home/hadoop/app/hadoop/etc/hadoop</p>\n</blockquote>\n<p>确保 <code>HADOOP_CONF_DIR</code> 或者 <code>YARN_CONF_DIR</code> 指向包含 Hadoop 集群的（客户端）配置文件的目录。这些配置被用于写入 HDFS 并连接到 YARN ResourceManager 。此目录中包含的配置将被分发到 YARN 集群，以便 application（应用程序）使用的所有的所有 containers（容器）都使用相同的配置。如果配置引用了 Java 系统属性或者未由 YARN 管理的环境变量，则还应在 Spark 应用程序的配置（driver（驱动程序），executors（执行器），和在客户端模式下运行时的 AM ）。</p>\n<blockquote>\n<p> SPARK_DIST_CLASSPATH=$(/home/hadoop/app/hadoop-2.6.0/bin/hadoop classpath)</p>\n</blockquote>\n<p>Pre-build with user-provided Hadoop: 属于“Hadoop free”版,不包含hadoop的jar等，这样，下载到的Spark，可应用到任意Hadoop 版本。但是需要在spark的spar-evn.sh中指定配置hadoop的安装路径。</p>\n<blockquote>\n<p>spark-submit</p>\n</blockquote>\n<p>如果用户的应用程序被打包好了，它可以使用 <code>bin/spark-submit</code> 脚本来启动。这个脚本负责设置 Spark 和它的依赖的 classpath，并且可以支持 Spark 所支持的不同的 Cluster Manager 以及 deploy mode（部署模式）:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/spark-submit \\</span><br><span class=\"line\">  --class &lt;main-class&gt; \\</span><br><span class=\"line\">  --master &lt;master-url&gt; \\</span><br><span class=\"line\">  --deploy-mode &lt;deploy-mode&gt; \\</span><br><span class=\"line\">  --conf &lt;key&gt;=&lt;value&gt; \\</span><br><span class=\"line\">  ... # other options</span><br><span class=\"line\">  &lt;application-jar&gt; \\</span><br><span class=\"line\">  [application-arguments]</span><br></pre></td></tr></table></figure>\n<p>一些常用的 options（选项）有 :</p>\n<ul>\n<li><code>--class</code>: 您的应用程序的入口点（例如。 <code>org.apache.spark.examples.SparkPi</code>)</li>\n<li><code>--master</code>: standalone模式下是集群的 master URL，on yarn模式下值是<code>yarn</code></li>\n<li><code>--deploy-mode</code>: 是在 worker 节点(<code>cluster</code>) 上还是在本地作为一个外部的客户端(<code>client</code>) 部署您的 driver(默认: <code>client</code>)</li>\n<li><code>--conf</code>: 按照 key=value 格式任意的 Spark 配置属性。对于包含空格的 value（值）使用引号包 “key=value” 起来。</li>\n<li><code>application-jar</code>: 包括您的应用以及所有依赖的一个打包的 Jar 的路径。该 URL 在您的集群上必须是全局可见的，例如，一个 <code>hdfs://</code> path 或者一个 <code>file://</code> 在所有节点是可见的。</li>\n<li><code>application-arguments</code>: 传递到您的 main class 的 main 方法的参数，如果有的话。</li>\n</ul>\n<h1 id=\"异常处理\"><a href=\"#异常处理\" class=\"headerlink\" title=\"异常处理\"></a>异常处理</h1><p><strong>执行脚本</strong></p>\n<blockquote>\n<p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn –deploy-mode client /home/hadoop/app/spark2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar</p>\n</blockquote>\n<p><strong>报错信息一</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">Application application_1537990303043_0001 failed 2 times due to AM Container for appattempt_1537990303043_0001_000002 exited with  exitCode: -103</span><br><span class=\"line\">Diagnostics: </span><br><span class=\"line\">Container [pid=2344,containerID=container_1537990303043_0001_02_000001] is running beyond virtual memory limits. </span><br><span class=\"line\">Current usage: 74.0 MB of 1 GB physical memory used; </span><br><span class=\"line\">2.2 GB of 2.1 GB virtual memory used. Killing container.</span><br></pre></td></tr></table></figure>\n<p><strong>问题原因</strong></p>\n<p>虚拟机物理内存设置的是1G，则对应虚拟内存最大为1*2.1=2.1GB,实际使用了2.2[此处疑问：为什么就使用了2.2，单个任务默认分配1024M，加上一个任务的Container默认1024M导致吗？]，所以需要扩大虚拟内存的比例，或者限制container和task的大小，或者关闭掉对虚拟内存的检测。</p>\n<p><strong>解决方法</strong></p>\n<p>修改<code>yarn-site.xml</code>文件，新增以下内容，详情原因请参考：<a href=\"https://sustcoder.github.io/2018/09/27/YARN%20%E5%86%85%E5%AD%98%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3/\">YARN 内存参数详解</a></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>虚拟内存和物理内存比率，默认为2.1<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>不检查虚拟内存，默认为true<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p><strong>报错二</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Exception in thread <span class=\"string\">\"main\"</span> org.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.</span><br><span class=\"line\">\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:<span class=\"number\">85</span>)</span><br><span class=\"line\">\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:<span class=\"number\">62</span>)</span><br><span class=\"line\">\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:<span class=\"number\">173</span>)</span><br><span class=\"line\">\tat org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:<span class=\"number\">509</span>)</span><br><span class=\"line\">\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:<span class=\"number\">2509</span>)</span><br><span class=\"line\">\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$<span class=\"number\">6</span>.apply(SparkSession.scala:<span class=\"number\">909</span>)</span><br><span class=\"line\">\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$<span class=\"number\">6</span>.apply(SparkSession.scala:<span class=\"number\">901</span>)</span><br><span class=\"line\">\tat scala.Option.getOrElse(Option.scala:<span class=\"number\">121</span>)</span><br><span class=\"line\">\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:<span class=\"number\">901</span>)</span><br><span class=\"line\">\tat org.apache.spark.examples.SparkPi$.main(SparkPi.scala:<span class=\"number\">31</span>)</span><br><span class=\"line\">\tat org.apache.spark.examples.SparkPi.main(SparkPi.scala)</span><br><span class=\"line\">\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class=\"line\">\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class=\"number\">62</span>)</span><br><span class=\"line\">\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class=\"number\">43</span>)</span><br><span class=\"line\">\tat java.lang.reflect.Method.invoke(Method.java:<span class=\"number\">498</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:<span class=\"number\">755</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$<span class=\"number\">1</span>(SparkSubmit.scala:<span class=\"number\">180</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:<span class=\"number\">205</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:<span class=\"number\">119</span>)</span><br><span class=\"line\">\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class=\"line\"><span class=\"number\">18</span>/<span class=\"number\">09</span>/<span class=\"number\">04</span> <span class=\"number\">17</span>:<span class=\"number\">01</span>:<span class=\"number\">43</span> INFO util.ShutdownHookManager: Shutdown hook called</span><br></pre></td></tr></table></figure>\n<p><strong>问题原因</strong></p>\n<p>以上报错是在伪集群上运行时报错信息，具体报错原因未知，在切换到真正的集群环境后无此报错</p>\n<h1 id=\"配置链接\"><a href=\"#配置链接\" class=\"headerlink\" title=\"配置链接\"></a>配置链接</h1><p><strong>hadoop</strong></p>\n<ul>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/core-site.xml\" target=\"_blank\" rel=\"noopener\">core-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/hdfs-site.xml\" target=\"_blank\" rel=\"noopener\">hdfs-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/mapred-site.xml\" target=\"_blank\" rel=\"noopener\">mapred-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/yarn-site.xml\" target=\"_blank\" rel=\"noopener\">yarn-site.xml</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/hadoop-env.sh\" target=\"_blank\" rel=\"noopener\">hadoop-env.sh</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/masters\" target=\"_blank\" rel=\"noopener\">masters</a></p>\n</li>\n<li><p><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/hadoop/slaves\" target=\"_blank\" rel=\"noopener\">slaves</a></p>\n</li>\n</ul>\n<p><strong>zookeeper</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/zookeeper/zoo.cfg\" target=\"_blank\" rel=\"noopener\">zoo.cfg</a></li>\n</ul>\n<p><strong>spark</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/spark/spark-env.sh\" target=\"_blank\" rel=\"noopener\">spark-env.sh</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018//hadoop/conf/spark/slaves\" target=\"_blank\" rel=\"noopener\">slaves</a></li>\n</ul>\n<p><strong>env</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/env/hosts\" target=\"_blank\" rel=\"noopener\">hosts</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/env/profile\" target=\"_blank\" rel=\"noopener\">profile</a></li>\n</ul>\n<p><strong>tools</strong></p>\n<ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/tools/deploy.conf\" target=\"_blank\" rel=\"noopener\">deploy.conf</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/tools/deploy.sh\" target=\"_blank\" rel=\"noopener\">deploy.sh</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/conf/tools/runRemoteCmd.sh\" target=\"_blank\" rel=\"noopener\">runRemoteCmd.sh</a></li>\n</ul>\n","categories":["spark"],"tags":["spark","环境"]},{"title":"YARN 内存参数详解","url":"https://sustcoder.github.io/2018/09/27/2018-09-27-YARN 内存参数详解/","content":"<h2 id=\"yarn组件依赖关系\"><a href=\"#yarn组件依赖关系\" class=\"headerlink\" title=\"yarn组件依赖关系\"></a>yarn组件依赖关系</h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/hadoop/structure/yarn_structure.png\" alt=\"yarn结果图\"></p>\n<p>yarn主要由两部分组成，ResourceManager和NodeManger。NodeManager里面包含多个Container，每个Container里可以运行多个task，比如MapTask和ReduceTask等。ApplicationMaster也是在Container中运行。</p>\n<p>在YARN中，资源管理由ResourceManager和NodeManager共同完成，其中，<strong>ResourceManager中的调度器负责资源的分配，而NodeManager则负责资源的供给和隔离</strong>。ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的“资源调度”）后，NodeManager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。</p>\n<p>关于yarn的详细介绍可参考：<a href=\"https://my.oschina.net/freelili/blog/1853714\" target=\"_blank\" rel=\"noopener\">yarn的架构及作业调度</a></p>\n<h2 id=\"内存相关参数\"><a href=\"#内存相关参数\" class=\"headerlink\" title=\"内存相关参数\"></a>内存相关参数</h2><p><a href=\"https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml\" target=\"_blank\" rel=\"noopener\">yarn-site.xml官网参数表及其解释</a></p>\n<table>\n<thead>\n<tr>\n<th>配置文件</th>\n<th>配置设置</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.resource.memory-mb</td>\n<td>-1</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.vmem-pmem-ratio</td>\n<td>2.1</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.vmem-check-enabled</td>\n<td>true</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.pmem-check-enabled</td>\n<td>true</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.minimum-allocation-mb</td>\n<td>1024MB</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.maximum-allocation-mb</td>\n<td>8192 MB</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.nodemanager.resource.cpu-vcores</td>\n<td>8</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.minimum-allocation-vcores</td>\n<td>1</td>\n</tr>\n<tr>\n<td>yarn-site.xml</td>\n<td>yarn.scheduler.maximum-allocation-vcores</td>\n<td>32</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>yarn.app.mapreduce.am.resource.mb</td>\n<td>1536 MB</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>yarn.app.mapreduce.am.command-opts</td>\n<td>-Xmx1024m</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.map.memory.mb</td>\n<td>1024 MB</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.reduce.memory.mb</td>\n<td>1024 MB</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.map.java.opts</td>\n<td>最新版已经去掉</td>\n</tr>\n<tr>\n<td>mapred-site.xml</td>\n<td>mapreduce.reduce.java.opts</td>\n<td>最新版已经去掉</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"参数解释\"><a href=\"#参数解释\" class=\"headerlink\" title=\"参数解释\"></a>参数解释</h2><h3 id=\"NodeManager\"><a href=\"#NodeManager\" class=\"headerlink\" title=\"NodeManager\"></a>NodeManager</h3><ul>\n<li><code>yarn.nodemanager.resource.memory-mb</code>：节点最大可用内存，如果值为-1且<code>yarn.nodemanager.resource.detect-hardware-capabilities</code>值为<code>true</code>，则根据系统内存自动计算，否则默认值为8192M</li>\n<li><code>yarn.nodemanager.vmem-pmem-ratio</code>：虚拟内存率，Container 的虚拟内存大小的限制，每使用1MB物理内存，最多可用的虚拟内存数</li>\n<li><code>yarn.nodemanager.pmem-check-enabled</code>：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true;</li>\n<li><code>yarn.nodemanager.vmem-check-enabled</code>：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true</li>\n</ul>\n<h3 id=\"ResourceManager\"><a href=\"#ResourceManager\" class=\"headerlink\" title=\"ResourceManager\"></a>ResourceManager</h3><ul>\n<li><code>yarn.scheduler.minimum-allocation-mb</code>：单个任务可申请的最少物理内存量，默认是1024（MB），如果一个任务申请的物理内存量少于该值，则该对应的值改为这个数</li>\n<li><code>yarn.scheduler.maximum-allocation-mb</code>：单个任务可申请的最多物理内存量，默认是8192（MB）。</li>\n</ul>\n<h3 id=\"ApplicationMaster\"><a href=\"#ApplicationMaster\" class=\"headerlink\" title=\"ApplicationMaster\"></a>ApplicationMaster</h3><ul>\n<li><code>mapreduce.map.memory.mb</code>：分配给 Map Container的内存大小，默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。</li>\n<li><code>mapreduce.reduce.memory.mb</code>：分配给 Reduce Container的内存大小，默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。</li>\n<li><code>mapreduce.map.java.opts</code>：运行 Map 任务的 jvm 参数，如 -Xmx，-Xms 等选项</li>\n<li><code>mapreduce.reduce.java.opts</code>：运行 Reduce 任务的 jvm 参数，如-Xmx，-Xms等选项</li>\n</ul>\n<h3 id=\"CPU-资源\"><a href=\"#CPU-资源\" class=\"headerlink\" title=\"CPU 资源\"></a>CPU 资源</h3><ul>\n<li><code>yarn.nodemanager.resource.cpu-vcores</code>：该节点上 YARN 可使用的虚拟 CPU 个数，如果值是-1且<code>yarn.nodemanager.resource.detect-hardware-capabilities</code>值为·true`，则其cpu数目根据系统而定，否则默认值为8</li>\n<li><code>yarn.scheduler.minimum-allocation-vcores</code>：单个任务可申请的最小虚拟CPU个数, 默认是1</li>\n<li><code>yarn.scheduler.maximum-allocation-vcores</code>：单个任务可申请的最多虚拟CPU个数，默认是4</li>\n</ul>\n<h2 id=\"Killing-Container\"><a href=\"#Killing-Container\" class=\"headerlink\" title=\"Killing Container\"></a>Killing Container</h2><p>在本地虚拟机上跑task时报错如下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">Application application_1537990303043_0001 failed 2 times due to AM Container for appattempt_1537990303043_0001_000002 exited with  exitCode: -103</span><br><span class=\"line\">Diagnostics: </span><br><span class=\"line\">Container [pid=2344,containerID=container_1537990303043_0001_02_000001] is running beyond virtual memory limits. </span><br><span class=\"line\">Current usage: 74.0 MB of 1 GB physical memory used; </span><br><span class=\"line\">2.2 GB of 2.1 GB virtual memory used. Killing container.</span><br></pre></td></tr></table></figure>\n<p>虚拟机物理内存设置的是1G，则对应虚拟内存最大为1*2.1=2.1GB,实际使用了2.2[此处疑问：为什么就使用了2.2，单个任务默认分配1024M，加上一个任务的Container默认1024M导致吗？]，所以需要扩大虚拟内存的比例，或者限制container和task的大小，或者关闭掉对虚拟内存的检测。</p>\n<h3 id=\"yarn-site-xml\"><a href=\"#yarn-site-xml\" class=\"headerlink\" title=\"yarn-site.xml\"></a>yarn-site.xml</h3><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">&lt;property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;value&gt;256&lt;/value&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;description&gt;每个container可申请最小内存&lt;/description&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;/property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;value&gt;512&lt;/value&gt;</span></span><br><span class=\"line\"><span class=\"comment\">    &lt;description&gt;每个container可申请最大内存&lt;/description&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;/property&gt;</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>虚拟内存和物理内存比率，默认为2.1<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>不检查虚拟内存，默认为true<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"混淆点\"><a href=\"#混淆点\" class=\"headerlink\" title=\"混淆点\"></a>混淆点</h2><p><code>yarn.scheduler.minimum-allocation-mb</code>和<code>yarn.scheduler.maximum-allocation-mb</code>这两个参数不能限制任务的真正大小？？？</p>\n<p>这两个参数是管理员用来设置用户能够设置的每个任务可申请的最小和最大内存资源。具体每个任务到底申请多少，由各个应用程序单独设置，如果是mapreduce程序，可以map task申请的资源可通过mapreduce.map.memory.mb指定，reduce task的资源可通过mapreduce.reduce.memory.mb指定，这两个参数最大不能超过yarn.scheduler.maximum-allocation-mb</p>\n","categories":["spark"],"tags":["spark","环境"]},{"title":"windows下JDK版本切换脚本","url":"https://sustcoder.github.io/2018/09/18/2018-09-18-windows下JDK版本切换脚本/","content":"<h1 id=\"初衷\"><a href=\"#初衷\" class=\"headerlink\" title=\"初衷\"></a>初衷</h1><p>前几天在一个技术交流群中看到技术人应该怎样去扩展自己的知识，去发现新的技术。其中有一条就是：<strong>当你对当前的工作感到厌倦的时候就应该去思考是否可以对其进行优化</strong>，比如我在重复的打开环境变量，修改JDK版本号的时候，就为每天都要进行此操作而感到厌倦，以至于内心开始拒绝去切换JDK版本，拒绝去做需要在另一个版本上的工作。</p>\n<h1 id=\"research过程\"><a href=\"#research过程\" class=\"headerlink\" title=\"research过程\"></a>research过程</h1><p>首先我搜索的关键字是<code>jdk版本切换</code>，其搜索结果都是怎样设置多JDK版本，怎样去修改环境变量。但是这些结果并不是我想要的，不过我确实是想要切换JDK版本啊，为什么没搜到结果呢。</p>\n<p><strong>当搜索不到结果的时候，首先考虑我们的搜索关键字是否准备</strong></p>\n<p>再次思考，我其实不是想切换JDK版本，而是想更方便的切换JDK版本，怎样会更方便呢，比如只点一个按钮即可。那其实可以通过脚本去实现这个功能，所以我的搜索条件变成了<code>windos切换JDK版本脚本</code>然后就搜索到了想要的结果。</p>\n<h1 id=\"脚本\"><a href=\"#脚本\" class=\"headerlink\" title=\"脚本\"></a>脚本</h1><h2 id=\"运行截图\"><a href=\"#运行截图\" class=\"headerlink\" title=\"运行截图\"></a>运行截图</h2><p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/jdkVSwitch.png\" alt=\"运行截图\"></p>\n<h2 id=\"脚本内容\"><a href=\"#脚本内容\" class=\"headerlink\" title=\"脚本内容\"></a>脚本内容</h2><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@<span class=\"built_in\">echo</span> off</span><br><span class=\"line\"></span><br><span class=\"line\">rem --- Base Config 配置JDK的安装目录 ---</span><br><span class=\"line\">:init </span><br><span class=\"line\"><span class=\"built_in\">set</span> JAVA_HOME_1_8=C:\\Program Files\\Java\\jdk8</span><br><span class=\"line\"><span class=\"built_in\">set</span> JRE_HOME_1_8=C:\\Program Files\\Java\\jre8</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">set</span> JAVA_HOME_1_7=C:\\Program Files\\Java\\jdk7</span><br><span class=\"line\"><span class=\"built_in\">set</span> JRE_HOME_1_7=C:\\Program Files\\Java\\jre7</span><br><span class=\"line\">:start </span><br><span class=\"line\"><span class=\"built_in\">echo</span> 当前使用的JDK 版本: </span><br><span class=\"line\">java -version </span><br><span class=\"line\"><span class=\"built_in\">echo</span>. </span><br><span class=\"line\"><span class=\"built_in\">echo</span> ============================================= </span><br><span class=\"line\"><span class=\"built_in\">echo</span> jdk版本列表: </span><br><span class=\"line\"><span class=\"built_in\">echo</span>  jdk1.8 </span><br><span class=\"line\"><span class=\"built_in\">echo</span>  jdk1.7</span><br><span class=\"line\"><span class=\"built_in\">echo</span> ============================================= </span><br><span class=\"line\"></span><br><span class=\"line\">:select</span><br><span class=\"line\"><span class=\"built_in\">set</span> /p opt=请输入JDK版本。[7代表jdk1.7],[8代表jdk1.8]： </span><br><span class=\"line\"><span class=\"keyword\">if</span> %opt%==8 (</span><br><span class=\"line\">    <span class=\"built_in\">set</span> TARGET_JAVA_HOME=%JAVA_HOME_1_8%</span><br><span class=\"line\">\t<span class=\"built_in\">set</span> TARGET_JRE_HOME=%JRE_HOME_1_8%</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">if</span> %opt%==7 (</span><br><span class=\"line\">    <span class=\"built_in\">set</span> TARGET_JAVA_HOME=%JAVA_HOME_1_7%</span><br><span class=\"line\">\t<span class=\"built_in\">set</span> TARGET_JRE_HOME=%JRE_HOME_1_7%</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">echo</span> 当前选择的Java路径:</span><br><span class=\"line\"><span class=\"built_in\">echo</span> JAVE_HOME:%TARGET_JAVA_HOME%</span><br><span class=\"line\"><span class=\"built_in\">echo</span> JRE_HOME:%TARGET_JRE_HOME%</span><br><span class=\"line\"></span><br><span class=\"line\">wmic ENVIRONMENT <span class=\"built_in\">where</span> <span class=\"string\">\"name='JAVA_HOME'\"</span> delete</span><br><span class=\"line\">wmic ENVIRONMENT create name=<span class=\"string\">\"JAVA_HOME\"</span>,username=<span class=\"string\">\"&lt;system&gt;\"</span>,VariableValue=<span class=\"string\">\"%TARGET_JAVA_HOME%\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">wmic ENVIRONMENT <span class=\"built_in\">where</span> <span class=\"string\">\"name='JRE_HOME'\"</span> delete</span><br><span class=\"line\">wmic ENVIRONMENT create name=<span class=\"string\">\"JRE_HOME\"</span>,username=<span class=\"string\">\"&lt;system&gt;\"</span>,VariableValue=<span class=\"string\">\"%TARGET_JRE_HOME%\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">rem -- refresh env ---</span><br><span class=\"line\">call RefreshEnv</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">echo</span> 请按任意键退出!   </span><br><span class=\"line\">pause&gt;nul</span><br><span class=\"line\"></span><br><span class=\"line\">@<span class=\"built_in\">echo</span> on</span><br></pre></td></tr></table></figure>\n<h2 id=\"脚本下载\"><a href=\"#脚本下载\" class=\"headerlink\" title=\"脚本下载\"></a>脚本下载</h2><ul>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/RefreshEnv.exe\" target=\"_blank\" rel=\"noopener\">RefreshEnv.exe</a></li>\n<li><a href=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/switchVersion.bat\" target=\"_blank\" rel=\"noopener\">switchVersion.bat</a></li>\n</ul>\n<h1 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h1><ul>\n<li>是否需要配置<code>JRE_HOME</code>和安装JDK的路径有关系，下图是我的安装路径</li>\n</ul>\n<p><img src=\"https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/tool/script/jdkPath.png\" alt=\"jdk安装路径\"> </p>\n<ul>\n<li>需要修改<code>JAVA_HOME</code>的值为你对应的JDK安装路径</li>\n<li>需要以管理员权限运行脚本</li>\n</ul>\n","categories":["tools"],"tags":["tools","jdk","思考"]},{"title":"sbt安装与仓库设置","url":"https://sustcoder.github.io/2018/09/13/2018-09-13-sbt入门/","content":"<h2 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h2><ul>\n<li><code>java 1.8</code></li>\n<li><code>scala 2.12.6</code></li>\n<li><code>sbt 1.2.1</code></li>\n<li><code>idea2.18.3</code><h2 id=\"软件\"><a href=\"#软件\" class=\"headerlink\" title=\"软件\"></a>软件</h2></li>\n<li><code>idea sbt</code>插件</li>\n<li><code>idea scala</code>插件</li>\n<li>sbt安装包 <code>https://sbt-downloads.cdnedge.bluemix.net/releases/v1.2.1/sbt-1.2.1.msi</code>,非必须，可直接使用idea的sbt插件做对应配置<h2 id=\"安装sbt\"><a href=\"#安装sbt\" class=\"headerlink\" title=\"安装sbt\"></a>安装sbt</h2></li>\n</ul>\n<ol>\n<li>新建sbt安装路径，注意：<strong>sbt安装路径中不能含有空格和中文</strong>，将<code>sbt-1.2.1.msi</code>安装到此路径。</li>\n<li>配置环境变量</li>\n</ol>\n<ul>\n<li>新建变量sbt<blockquote>\n<p>SBT_HOME  D:\\ProgramFile\\sbt</p>\n</blockquote>\n</li>\n<li>添加变量到path中<blockquote>\n<p> %SBT_HOME%bin;</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h2><p>修改文件<code>onf/sbtconfig.txt</code>，添加以下内容</p>\n<blockquote>\n<p>-Dfile.encoding=UTF8</p>\n</blockquote>\n<h2 id=\"sbt仓库设置\"><a href=\"#sbt仓库设置\" class=\"headerlink\" title=\"sbt仓库设置\"></a>sbt仓库设置</h2><h3 id=\"方案一\"><a href=\"#方案一\" class=\"headerlink\" title=\"方案一\"></a>方案一</h3><p>直接修改sbt的jar里面的配置文件。<code>windows</code>下可通过360压缩替换掉<code>jar</code>包里面的文件。</p>\n<ol>\n<li>找到sbt安装目录<code>D:\\ProgramFile\\sbt\\bin</code></li>\n<li>备份<code>sbt-launch.jar</code>为<code>sbt-launch.jar.bak</code></li>\n<li>解压<code>sbt-launch.jar.bak</code>,打开个<code>sbt.boot.properties</code>文件</li>\n<li><p>在<code>[repositories]</code>里面的<code>local</code>下面添加以下数据源</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">alirepo1:https://maven.aliyun.com/repository/central</span><br><span class=\"line\">alirepo2:https://maven.aliyun.com/repository/jcenter</span><br><span class=\"line\">alirepo3:https://maven.aliyun.com/repository/public</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>使用360压缩打开<code>sbt-launch.jar</code>,找到<code>sbt.boot.properties</code>文件并替换</p>\n</li>\n</ol>\n<h3 id=\"方案二\"><a href=\"#方案二\" class=\"headerlink\" title=\"方案二\"></a>方案二</h3><p>配置sbt的数据源，让其优先加载我们配置的数据源</p>\n<ol>\n<li>在<code>D:\\ProgramFile\\sbt\\conf</code>目录下，新建文件<code>repository.properties</code></li>\n<li><p>在<code>repository.properties</code>中添加以下内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">[repositories]</span><br><span class=\"line\">local</span><br><span class=\"line\">alirepo1:https://maven.aliyun.com/repository/central</span><br><span class=\"line\">alirepo2:https://maven.aliyun.com/repository/jcenter</span><br><span class=\"line\">alirepo3:https://maven.aliyun.com/repository/public</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在<code>conf/sbtconfig.txt</code>中添加<code>repository.properties</code>文件路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">-Dsbt.repository.config=D:/ProgramFile/sbt/conf/repository.properties</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"添加依赖build-sbt\"><a href=\"#添加依赖build-sbt\" class=\"headerlink\" title=\"添加依赖build.sbt\"></a>添加依赖build.sbt</h2><p>在项目中找到<code>build.sbt</code>文件，此类似于<code>maven</code>中的<code>pom</code>文件<br>添加<code>spark-core</code>和<code>spark-sql</code>等的依赖<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">name := &quot;sbt-test&quot; // 项目名称</span><br><span class=\"line\"></span><br><span class=\"line\">version := &quot;0.1&quot; // 项目版本号</span><br><span class=\"line\"></span><br><span class=\"line\">scalaVersion := &quot;2.11.12&quot; // scala版本号</span><br><span class=\"line\"></span><br><span class=\"line\">// 依赖</span><br><span class=\"line\">libraryDependencies ++= Seq(</span><br><span class=\"line\">  &quot;org.apache.spark&quot;  %%  &quot;spark-core&quot;    % &quot;2.2.0&quot;,</span><br><span class=\"line\">  &quot;org.apache.spark&quot;  %%  &quot;spark-sql&quot;     % &quot;2.2.0&quot;,</span><br><span class=\"line\">  &quot;com.typesafe.scala-logging&quot; % &quot;scala-logging-slf4j_2.11&quot; % &quot;latest.integration&quot;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"下载依赖\"><a href=\"#下载依赖\" class=\"headerlink\" title=\"下载依赖\"></a>下载依赖</h2><p>通过以下方式打开执行命令窗口</p>\n<ul>\n<li>在<code>build.sbt</code>同级目录下打开<code>cmd</code>窗口</li>\n<li>在<code>idea</code>中打开<code>Termail</code>窗口</li>\n</ul>\n<p>下载或更新jar</p>\n<blockquote>\n<p>sbt update</p>\n</blockquote>\n<p>编译文件</p>\n<blockquote>\n<p>sbt compile</p>\n</blockquote>\n<p>打包</p>\n<blockquote>\n<p>sbt package</p>\n</blockquote>\n<h1 id=\"问题处理\"><a href=\"#问题处理\" class=\"headerlink\" title=\"问题处理\"></a>问题处理</h1><h2 id=\"版本不兼容\"><a href=\"#版本不兼容\" class=\"headerlink\" title=\"版本不兼容\"></a>版本不兼容</h2><h3 id=\"jdk不兼容\"><a href=\"#jdk不兼容\" class=\"headerlink\" title=\"jdk不兼容\"></a>jdk不兼容</h3><ul>\n<li><code>idea2018</code>需要<code>jdk8</code>以上</li>\n<li><code>spark2.0</code>需要<code>jdk8</code>以上<h2 id=\"文件下载缓慢\"><a href=\"#文件下载缓慢\" class=\"headerlink\" title=\"文件下载缓慢\"></a>文件下载缓慢</h2><code>idea</code>控制台<code>build</code>界面一直在转圈,并提示<code>dump project structure from sbt</code></li>\n</ul>\n<p>这里需要注意，在<code>Intellij Idea</code>启动时，会执行<code>dump project structure from sbt</code>的操作，也就是把sbt所需要的项目结构从远程服务器拉取到本地，在本地会生成sbt所需要的项目结构。由于是从国外的远程服务器下载，所以，这个过程很慢，笔者电脑上运行了15分钟。这个过程没有结束之前，上图中的<code>File-&gt;New</code>弹出的子菜单是找不到<code>Scala Class</code>这个选项的。所以，一定要等<code>dump project structure from sbt</code>的操作全部执行结束以后，再去按照上图操作来新建<code>Scala Class</code>文件。</p>\n<h2 id=\"修改sbt数据源\"><a href=\"#修改sbt数据源\" class=\"headerlink\" title=\"修改sbt数据源\"></a>修改sbt数据源</h2><h3 id=\"不靠谱方案\"><a href=\"#不靠谱方案\" class=\"headerlink\" title=\"不靠谱方案\"></a>不靠谱方案</h3><ol>\n<li>将数据源改为<code>maven.oschina.com</code>。此数据源已经失效</li>\n<li>将<code>sbt.boot.properties</code>中的<code>https</code>改为<code>http</code>。未生效</li>\n<li>在<code>sbt</code>的<code>vm</code>中配置<code>-Dsbt.override.build.repos=true</code>。此方法效果和<code>-Dsbt.repository.config=D:/ProgramFile/sbt/conf/repository.properties</code>一致，前提是需要配置数据源</li>\n<li>最笨方案，下载jar包，放到本地仓库<code>C:\\Users\\sustcoder\\.ivy2\\cache</code></li>\n</ol>\n<h3 id=\"修改成阿里云数据源后依旧下载失败\"><a href=\"#修改成阿里云数据源后依旧下载失败\" class=\"headerlink\" title=\"修改成阿里云数据源后依旧下载失败\"></a>修改成阿里云数据源后依旧下载失败</h3><ul>\n<li>配置的<code>sbt</code>版本在阿里云的仓库中没有。排查办法：可以去<code>maven.aliyun.com</code>去查看对应版本pom文件是否存在</li>\n<li>在阿里云上找到了对应版本但依旧保持。注意查看日志信息中下载的jar包路径含有<code>_2.10</code>类似的字样，比如在<code>build.sbt</code>中配置的是<code>&quot;org.apache.spark&quot;  %%  &quot;spark-sql&quot;     % &quot;2.2.0&quot;</code>,但是日志里面是<code>[warn]  :: com.typesafe.scala-logging#scala-sql_2.10;2.1.2: not found</code>,这个是因为sbt里面的<code>%%</code>代表sbt默认会拼接上scala的版本号在pom文件上，下载最适合的jar包，可以将<code>%%</code>改为<code>%</code>，即改为<code>&quot;org.apache.spark&quot;  %  &quot;spark-sql&quot;     % &quot;2.2.0&quot;</code>,注意区别：仅仅是少了一个百分号。</li>\n<li>执行<code>sbt-shell</code>会走默认的仓库配置，需要在sbt的vm参数中配置<code>-Dsbt.override.build.repos=true</code> ????<h2 id=\"查看配置参数是否生效\"><a href=\"#查看配置参数是否生效\" class=\"headerlink\" title=\"查看配置参数是否生效\"></a>查看配置参数是否生效</h2>可在日志控制台查看第一行日志，查看配置参数是否生效，走的是自己安装的sbt还是idea的插件,如下日志，在<code>sbtconfig.txt</code>中配置信息会进行加载<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&quot;C:\\Program Files\\Java\\jdk8\\bin\\java.exe&quot; -agentlib:jdwp=transport=dt_socket,address=localhost:58502,suspend=n,server=y -Xdebug -server -Xmx1536M </span><br><span class=\"line\">-Dsbt.repository.config=D:/develop/sbt/conf/repository.properties -Didea.managed=true -Dfile.encoding=UTF-8 </span><br><span class=\"line\">-Didea.runid=2018.2 -jar D:\\ProgramFile\\sbt\\bin\\sbt-launch.jar idea-shell</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>参考链接</strong></p>\n<p><a href=\"https://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html\" target=\"_blank\" rel=\"noopener\">https://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html</a></p>\n<p><a href=\"https://segmentfault.com/a/1190000002484978\" target=\"_blank\" rel=\"noopener\">https://segmentfault.com/a/1190000002484978</a></p>\n","categories":["spark"],"tags":["sbt","spark"]},{"title":"Hello World","url":"https://sustcoder.github.io/2018/09/10/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","categories":[],"tags":[]},{"title":"about","url":"https://sustcoder.github.io/about/index.html","content":"","categories":[],"tags":[]},{"title":"category","url":"https://sustcoder.github.io/category/index.html","content":"","categories":[],"tags":[]},{"title":"","url":"https://sustcoder.github.io/life/index.html","content":"<html>\n\t<head>\n\t\t<title>胖斯世界</title>\n\t\t<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\n\t\t<style type=\"text/css\">\n\t\t\t\nbody {\n\tmargin: 0px 0px;\n\tpadding: 0px 0px;\n\toverflow: hidden;\n}\n\ndiv {\n\tcolor: #fff;\n}\n\n.space {\n\tbackground : #000;\n\twidth : 100%;\n\theight : 100%;\n}\n\n.space  div{\n\ttop:50%;\n\tleft:50%;\n\tposition:absolute;\n}\n\n.container {\n\n\tbackground : #888;\n\t\n    animation-play-state: running;\n    -webkit-animation-play-state: running;    \n}\n\n.container:hover {\n\n\tbackground : #eee;\n\t\n    animation-play-state: paused;\n    -webkit-animation-play-state: paused;\n\t\n\ttransition: 3s;\n}\n\n#all-container {\n\n\tbackground : #0f0f0f;\n\n    -moz-border-radius: 400px;\n    -webkit-border-radius: 400px;\n    border-radius: 400px;\n\t\n\twidth : 800px;\n\theight : 800px;\n\tmargin-left:-400px;\n\tmargin-top:-400px;\n\t\n\n    animation: spin 80s linear infinite;\n    -webkit-animation: spin 80s linear infinite;\n\t\n}\n\n#all-container:hover,\n#family-container:hover,\n#me-container:hover,\n#wife-container:hover,\n#child-container:hover {\n\n    animation-play-state: paused;\n    -webkit-animation-play-state: paused;\n\t\n}\n\n#family-container {\n\n    -moz-border-radius: 120px;\n    -webkit-border-radius: 120px;\n    border-radius: 120px;\n\t\n\twidth : 240px;\n\theight : 240px;\n\tmargin-left:-120px;\n\tmargin-top:-120px;\n\t\n    animation: spin 40s linear infinite;\n    -webkit-animation: spin 40s linear infinite;\n\t\n\tbackground-image:url(https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/life/myworld/wife.jpg);\n}\n\n#family {\n\n\twidth : 180px;\n\theight : 180px;\n\tmargin-left:-90px;\n\tmargin-top:-90px;\n\tfont-size: 50;\n\tfont-weight:300;\n\t\n}\n\n#family .words {\n\tdisplay:none;\n\tmargin-left:-90px;\n\tmargin-top:90px;\n}\n\n#family:hover .words {\n\tdisplay:block;\n}\n\n#wife-container {\n\n    -moz-border-radius: 40px;\n    -webkit-border-radius: 40px;\n    border-radius: 40px;\n\t\n\twidth : 70px;\n\theight : 70px;\n\tmargin-left:-330px;\n\tmargin-top:-30px;\n\t\n    animation: spin 10s linear infinite;\n    -webkit-animation: spin 15s linear infinite;\n\t\n\tbackground-image:url(https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/life/myworld/me.jpg);\n}\n\n#wife {\n\n\twidth : 40px;\n\theight : 40px;\n\tmargin-left:-30px;\n\tmargin-top:-30px;\n\tfont-size:20;\n\tfont-weight:300;\n}\n\n#wife .words {\n\tdisplay:none;\n\tmargin-left:-50px;\n\tmargin-top:50px;\n}\n\n#wife:hover .words {\n\tdisplay:block;\n}\n\n#child-container {\n\n    -moz-border-radius: 20px;\n    -webkit-border-radius: 20px;\n    border-radius: 20px;\n\t\n\twidth : 40px;\n\theight : 40px;\n\tmargin-left:-330px;\n\tmargin-top:15px;\n\t\n    animation: spin 5s linear infinite;\n    -webkit-animation: spin 5s linear infinite;\n\t\n\tbackground-image:url(https://sustblog.oss-cn-beijing.aliyuncs.com/blog/2018/life/myworld/child.jpg);\n}\n\n#child {\n\n\twidth : 30px;\n\theight : 30px;\n\tmargin-left:-15px;\n\tmargin-top:-15px;\n\tfont-size:15;\n\tfont-weight:300;\n}\n\n#child .words {\n\tdisplay:none;\n\tmargin-left:-30px;\n\tmargin-top:30px;\n}\n\n#child:hover .words {\n\tdisplay:block;\n}\n\n@keyframes spin {\n\tto {\n\t\t-webkit-transform: rotate(1turn);\n\t\t-ms-transform: rotate(1turn);\n\t}\n}\n\n@-webkit-keyframes spin {\n\tto {\n\t\t-webkit-transform: rotate(1turn);\n\t\t-ms-transform: rotate(1turn);\n\t}\n}\n\n\n\n\t  </style>\n\t</head>\n\t<body>\n\t\t<div class=\"space\">\n\t\t\t<div class=\"container\" id=\"all-container\">\n\t\t\t\t<div class=\"container\" id=\"family-container\">\n\t\t\t\t\t<div id=\"family\" alt=\"菲菲\">\n\t\t\t\t\t\t<div class=\"words\">菲菲</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"container\" id=\"wife-container\">\n\t\t\t\t\t<div id=\"wife\" alt=\"胖斯\">\n\t\t\t\t\t\t<div class=\"words\">胖斯</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"container\" id=\"child-container\">\n\t\t\t\t\t<div id=\"child\" alt=\"小可爱\">\n\t\t\t\t\t\t<div class=\"words\">小可爱</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</body>\n</html>\n\n","categories":[],"tags":[]},{"title":"link","url":"https://sustcoder.github.io/link/index.html","content":"","categories":[],"tags":[]},{"title":"project","url":"https://sustcoder.github.io/project/index.html","content":"","categories":[],"tags":[]},{"title":"search","url":"https://sustcoder.github.io/search/index.html","content":"","categories":[],"tags":[]},{"title":"tag","url":"https://sustcoder.github.io/tag/index.html","content":"","categories":[],"tags":[]}]